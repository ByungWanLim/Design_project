{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/MMI24limbyungwan/.conda/envs/byungwan_resn/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "#from torch.optim.lr_scheduler import _LRScheduler\n",
    "\n",
    "from tqdm import tqdm\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# GPU 사용이 가능할 경우, GPU를 사용할 수 있게 함.'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = torch.device(device)\n",
    "print(device)\n",
    "\n",
    "print(os.environ.get('CUDA_VISIBLE_DEVICES'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RLE 인코딩 함수\n",
    "def rle_encode(mask):\n",
    "    pixels = mask.flatten()\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)\n",
    "\n",
    "# 클래스별 IoU를 계산하기 위한 함수\n",
    "def calculate_iou_per_class(y_true, y_pred, class_id):\n",
    "    intersection = np.sum((y_true == class_id) & (y_pred == class_id))\n",
    "    union = np.sum((y_true == class_id) | (y_pred == class_id))\n",
    "    iou = intersection / union if union > 0 else 0\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "\n",
    "def apply_fisheye_distortion(images, masks, label):\n",
    "    # 이미지 크기 가져오기\n",
    "    batch, channel, height, width = images.shape\n",
    "\n",
    "    # 카메라 매트릭스 생성\n",
    "    focal_length = width / 4\n",
    "    center_x = width / 2\n",
    "    center_y = height / 2\n",
    "    camera_matrix = np.array([[focal_length, 0, center_x],\n",
    "                              [0, focal_length, center_y],\n",
    "                              [0, 0, 1]], dtype=np.float32)\n",
    "\n",
    "    # 왜곡 계수 생성\n",
    "    # dist_num = 0\n",
    "    # if label == 1:\n",
    "    #     dist_num = random.randint(1,3)\n",
    "    # elif label == 2.5:\n",
    "    #     dist_num = 2.5\n",
    "    dist_num = label\n",
    "    dist_coeffs = np.array([0, 0.03 * dist_num, 0, 0], dtype=np.float32)\n",
    "\n",
    "    # 왜곡 보정\n",
    "    undistorted_images = []\n",
    "    undistorted_masks = []\n",
    "\n",
    "    for i in range(batch):\n",
    "        image = images[i].permute(1, 2, 0).cpu().numpy()  # 텐서를 NumPy 배열로 변환\n",
    "        mask = masks[i].cpu().numpy()\n",
    "        undistorted_image = cv2.undistort(image, camera_matrix, dist_coeffs)\n",
    "        undistorted_mask = cv2.undistort(mask, camera_matrix, dist_coeffs)\n",
    "        undistorted_mask = np.round(undistorted_mask).astype(np.uint8)\n",
    "        undistorted_mask[undistorted_mask > 12] = 12\n",
    "\n",
    "        # 다시 텐서로 변환\n",
    "        undistorted_image = torch.from_numpy(undistorted_image).permute(2, 0, 1).float().to(device)\n",
    "        undistorted_mask = torch.from_numpy(undistorted_mask).long().to(device)\n",
    "\n",
    "        undistorted_images.append(undistorted_image)\n",
    "        undistorted_masks.append(undistorted_mask)\n",
    "\n",
    "    undistorted_images = torch.stack(undistorted_images, dim=0)\n",
    "    undistorted_masks = torch.stack(undistorted_masks, dim=0)\n",
    "\n",
    "    return undistorted_images, undistorted_masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# # 이미지 데이터 (batch, channel, height, width)\n",
    "# image_data = torch.randn(4, 3, 256, 256)\n",
    "\n",
    "# # 이미지 차원 변경 (batch, height, width, channel)\n",
    "# image_data_permuted = image_data.permute(0, 2, 3, 1)\n",
    "\n",
    "# # 이미지 크기 확인\n",
    "# print(image_data_permuted.shape)\n",
    "\n",
    "# # 4개의 이미지로 나누기\n",
    "# images = image_data_permuted.split(1, dim=0)\n",
    "# # 또는 images = torch.split(image_data_permuted, 1, dim=0)\n",
    "# undistorted_images = []\n",
    "# # 4개 이미지의 크기 확인\n",
    "# for i, image in enumerate(images):\n",
    "#     print(f\"Image {i + 1} shape: {image.shape}\")\n",
    "#     undistorted_images.append(images[i].squeeze())\n",
    "\n",
    "# undistorted_images2 = torch.stack(undistorted_images, dim=0)\n",
    "# undistorted_images3 = undistorted_images2.permute(0,3,1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None, infer=False):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "        self.infer = infer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #directory_path = \"/mnt/nas27/Dataset/Samsung_DM\"\n",
    "        directory_path = './data/224'\n",
    "        img_path = self.data.iloc[idx, 1]\n",
    "        img_path = os.path.join(directory_path, img_path)\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        #image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        if self.infer:\n",
    "            if self.transform:\n",
    "                image = self.transform(image=image)['image']\n",
    "            return image\n",
    "        \n",
    "        mask_path = self.data.iloc[idx, 2]\n",
    "        mask_path = os.path.join(directory_path, mask_path)\n",
    "        mask = cv2.imread(mask_path)\n",
    "        #mask = cv2.cvtColor(mask, cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)\n",
    "        mask = np.round(mask).astype(np.uint8)\n",
    "        mask[mask > 12] = 12 #배경을 픽셀값 12로 간주\n",
    "        mask += 1\n",
    "        mask[mask == 13] = 0\n",
    "\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "# class CustomDataset_target(Dataset):\n",
    "#     def __init__(self, csv_file, transform=None, infer=False):\n",
    "#         self.data = pd.read_csv(csv_file)\n",
    "#         self.transform = transform\n",
    "#         self.infer = infer\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         directory_path = \"/mnt/nas27/Dataset/Samsung_DM\"\n",
    "#         img_path = self.data.iloc[idx, 1]\n",
    "#         img_path = os.path.join(directory_path, img_path[2:])\n",
    "#         image = cv2.imread(img_path)\n",
    "#         image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "#         if self.infer:\n",
    "#             if self.transform:\n",
    "#                 image = self.transform(image=image)['image']\n",
    "#             return image\n",
    "\n",
    "\n",
    "#         if self.transform:\n",
    "#             augmented = self.transform(image=image)\n",
    "#             image = augmented['image']\n",
    "            \n",
    "\n",
    "#         return image\n",
    "     \n",
    "\n",
    "transform = A.Compose(\n",
    "    [   \n",
    "        #A.Resize(224, 224),\n",
    "        #A.Resize(128, 128),\n",
    "        A.Normalize(),\n",
    "        \n",
    "        # 변형\n",
    "        # A.VerticalFlip(p=0.5),\n",
    "        # A.RandomRotate90(p=0.5),\n",
    "        # A.HueSaturationValue(p=0.2),\n",
    "        \n",
    "        ToTensorV2()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Unet의 기본이 되는 conv블럭\n",
    "# class ConvBlock(nn.Module):\n",
    "#     def __init__(self, in_channels, out_channels):\n",
    "#         super(ConvBlock, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "#         self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "#         self.relu1 = nn.ReLU()\n",
    "        \n",
    "#         self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)  # 여기서 in_channels는 out_channels와 동일해야 합니다.\n",
    "#         self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "#         self.relu2 = nn.ReLU()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.conv1(x)\n",
    "#         x = self.bn1(x)\n",
    "#         x = self.relu1(x)\n",
    "        \n",
    "#         x = self.conv2(x)\n",
    "#         x = self.bn2(x)\n",
    "#         x = self.relu2(x)\n",
    "#         return x\n",
    "\n",
    "# #인코더 블럭\n",
    "# class EncoderBlock(nn.Module):\n",
    "#     def __init__(self, in_channels, out_channels):\n",
    "#         super(EncoderBlock,self).__init__()\n",
    "#         self.convblock1 = ConvBlock(in_channels, out_channels)  # 첫 번째 ConvBlock의 in_channels는 입력 이미지의 채널 수와 일치해야 합니다.\n",
    "#         #self.convblock2 = ConvBlock(out_channels, out_channels)  # 두 번째 ConvBlock의 in_channels는 out_channels와 일치해야 합니다.\n",
    "#         self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "#     def forward(self,x):\n",
    "#         x = self.convblock1(x)\n",
    "#         #x = self.convblock2(x)\n",
    "#         p = self.maxpool(x)\n",
    "#         return x , p\n",
    "# #디코더 블럭\n",
    "# #디코더는 업샘플링 이후 스킵연결과 붙어서 convblock을 통과해야함\n",
    "# #skip보다 작은 x x먼저 업샘플링 32 -> 64 , skip과 결합 6464 \n",
    "# class DecoderBlock(nn.Module):\n",
    "#     def __init__(self, channels):\n",
    "#         super(DecoderBlock,self).__init__()\n",
    "#         self.upsample = nn.ConvTranspose2d(channels*2, channels, kernel_size=4, stride=2, padding=1)#x 업샘플링\n",
    "#         self.convblock1 = ConvBlock(channels*2, channels)#차원감소\n",
    "#         #self.convblock2 = ConvBlock(channels, channels)\n",
    "#     def forward(self,x,skip):\n",
    "#         x = self.upsample(x)\n",
    "#         x = torch.cat([x, skip], dim=1)\n",
    "#         x = self.convblock1(x)\n",
    "#         #x = self.convblock2(x)\n",
    "#         return x\n",
    "\n",
    "# ###########################################\n",
    "# class GradReverse(torch.autograd.Function):\n",
    "#     @staticmethod\n",
    "#     def forward(self, x):\n",
    "#         return x.view_as(x)\n",
    "#     @staticmethod\n",
    "#     def backward(self, grad_output): # 역전파 시에 gradient에 음수를 취함\n",
    "#         return (grad_output * -1)\n",
    "\n",
    "# class domain_classifier(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(domain_classifier, self).__init__()\n",
    "#         self.fc1 = nn.Linear(224*224*64, 10)\n",
    "#         self.fc2 = nn.Linear(10, 4) # source = 0, target = 1 회귀 가정\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = x.view(-1, 224*224*64)\n",
    "#         x = GradReverse.apply(x) # gradient reverse\n",
    "#         x = F.leaky_relu(self.fc1(x))\n",
    "#         x = self.fc2(x)\n",
    "        \n",
    "#         return x\n",
    "\n",
    "# ###########################################\n",
    "\n",
    "\n",
    "# #Unet구조 middle의 xm값의 움직임에 주의\n",
    "# class Unet(nn.Module):\n",
    "#     def __init__(self,n_classes):\n",
    "#         super(Unet,self).__init__()\n",
    "#         self.encoder1 = EncoderBlock(3,64)\n",
    "#         self.encoder2 = EncoderBlock(64,128)\n",
    "#         self.encoder3 = EncoderBlock(128,256)\n",
    "#         self.encoder4 = EncoderBlock(256,512)\n",
    "        \n",
    "#         self.middleconv = ConvBlock(512,1024)\n",
    "        \n",
    "        \n",
    "#         self.decoder4 = DecoderBlock(512)\n",
    "#         self.decoder3 = DecoderBlock(256)\n",
    "#         self.decoder2 = DecoderBlock(128)\n",
    "#         self.decoder1 = DecoderBlock(64)\n",
    "#         self.segmap = nn.Conv2d(64,n_classes, kernel_size=1)\n",
    "        \n",
    "#         self.domain_classifier = domain_classifier()\n",
    "                                        \n",
    "\n",
    "#     def forward(self,x):\n",
    "#         x1,p = self.encoder1(x)#3->64   #P:256,256 x1 :512,512\n",
    "#         x2,p = self.encoder2(p)#64->128 #P:128,128 x2:256,256\n",
    "#         x3,p = self.encoder3(p)#128->256#p:64,64 x3:128,128\n",
    "#         x4,p = self.encoder4(p)#256->512#p:32,32 x4:64,64\n",
    "        \n",
    "#         xm = self.middleconv(p)#512->1024#32,32\n",
    "        \n",
    "#         x = self.decoder4(xm,x4)#뉴런:1024->512->512 #출력tensor:64,64\n",
    "#         x = self.decoder3(x,x3)#뉴런:512->256->256 #출력tensor:128,128\n",
    "#         x = self.decoder2(x,x2)#뉴런:256->128->128 #출력tensor:256,256\n",
    "#         x = self.decoder1(x,x1)#뉴런:128->64->64 #출력tensor:512,512\n",
    "\n",
    "#         x_c = self.segmap(x)\n",
    "#         x_d = self.domain_classifier(x)\n",
    "#         # print(\"x_c\", x_c.shape)\n",
    "#         # print(\"x_d\", x_d.shape)\n",
    "#         return x_c, x_d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class GradReverse(torch.autograd.Function):\n",
    "#     @staticmethod\n",
    "#     def forward(self, x):\n",
    "#         return x.view_as(x)\n",
    "#     @staticmethod\n",
    "#     def backward(self, grad_output): # 역전파 시에 gradient에 음수를 취함\n",
    "#         return grad_output * (-1)\n",
    "\n",
    "# class domain_classifier(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(domain_classifier, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(128, 1, kernel_size=1)\n",
    "#         self.fc1 = nn.Linear(224*224*1, 10)\n",
    "#         self.fc2 = nn.Linear(10, 4) # source = 0, target = 1 회귀 가정\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.conv1(x)\n",
    "#         x = x.view(-1, 224*224*1)\n",
    "#         #print(x.shape)\n",
    "#         x = GradReverse.apply(x) # gradient reverse\n",
    "#         x = F.leaky_relu(self.fc1(x))\n",
    "#         x = self.fc2(x)\n",
    "#         #print(x.shape)\n",
    "#         #return torch.sigmoid(x)\n",
    "#         return x\n",
    "\n",
    "# #Unet의 기본이 되는 conv블럭\n",
    "# class ConvBlock(nn.Module):\n",
    "#     def __init__(self, in_channels, out_channels,kernel_size = 3):\n",
    "#         super(ConvBlock, self).__init__()\n",
    "#         self.kernel_size = kernel_size\n",
    "#         self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=1)\n",
    "#         self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "#         self.relu1 = nn.ReLU()\n",
    "        \n",
    "#         self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size, padding=1)  # 여기서 in_channels는 out_channels와 동일해야 합니다.\n",
    "#         self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "#         self.relu2 = nn.ReLU()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.conv1(x)\n",
    "#         x = self.bn1(x)\n",
    "#         x = self.relu1(x)\n",
    "        \n",
    "#         x = self.conv2(x)\n",
    "#         x = self.bn2(x)\n",
    "#         x = self.relu2(x)\n",
    "#         return x\n",
    "# class IdentityBlock(nn.Module):\n",
    "#     def __init__(self, in_channels, mid_channels, out_channels, stride=1):\n",
    "#         super(IdentityBlock, self).__init__()\n",
    "        \n",
    "#         # 1x1 convolution\n",
    "#         self.conv1 = nn.Conv2d(in_channels, mid_channels, kernel_size=1, stride=stride, bias=False)\n",
    "#         self.bn1 = nn.BatchNorm2d(mid_channels)\n",
    "#         self.relu1 = nn.ReLU()\n",
    "\n",
    "#         # 3x3 convolution\n",
    "#         self.conv2 = nn.Conv2d(mid_channels, mid_channels, kernel_size=3, padding=1, bias=False)\n",
    "#         self.bn2 = nn.BatchNorm2d(mid_channels)\n",
    "#         self.relu2 = nn.ReLU()\n",
    "\n",
    "#         # 1x1 convolution\n",
    "#         self.conv3 = nn.Conv2d(mid_channels, out_channels, kernel_size=1, bias=False)\n",
    "#         self.bn3 = nn.BatchNorm2d(out_channels)\n",
    "#         self.relu3 = nn.ReLU()\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         out = self.conv1(x)\n",
    "#         out = self.bn1(out)\n",
    "#         out = self.relu1(out)\n",
    "        \n",
    "#         out = self.conv2(out)\n",
    "#         out = self.bn2(out)\n",
    "#         out = self.relu2(out)\n",
    "         \n",
    "#         out = self.conv3(out)\n",
    "#         out = self.bn3(out)\n",
    "#         out = self.relu3(out)\n",
    "        \n",
    "#         return out\n",
    "# class HeadBlock(IdentityBlock):\n",
    "#     def __init__(self, in_channels, mid_channels, out_channels, stride=1):\n",
    "#         super(HeadBlock, self).__init__(in_channels, mid_channels, out_channels, stride)\n",
    "        \n",
    "#         self.shortcut = nn.Sequential(\n",
    "#             nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "#             nn.BatchNorm2d(out_channels)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         identity = x\n",
    "#         out = super().forward(x)\n",
    "        \n",
    "#         if identity.size() != out.size():\n",
    "#             identity = F.interpolate(identity, size=out.size()[2:])\n",
    "#         identity = self.shortcut(identity)\n",
    "        \n",
    "#         out += identity\n",
    "#         out = self.relu3(out)\n",
    "        \n",
    "#         return out\n",
    "# #인코더 블럭\n",
    "# class Conv2(nn.Module):\n",
    "#     def __init__(self,in_channels, mid_channels, out_channels):\n",
    "#         super(Conv2,self).__init__() \n",
    "#         self.headblock = HeadBlock(in_channels,mid_channels,out_channels)\n",
    "#         self.identityblock1 = IdentityBlock(out_channels,mid_channels,out_channels)\n",
    "#         self.identityblock2 = IdentityBlock(out_channels,mid_channels,out_channels)\n",
    "#         self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "#     def forward(self,x):\n",
    "#         x = self.headblock(x)\n",
    "#         x = self.identityblock1(x)\n",
    "#         x = self.identityblock2(x)\n",
    "#         p = self.maxpool(x)\n",
    "#         return x , p\n",
    "# class Conv3(nn.Module):\n",
    "#     def __init__(self,in_channels, mid_channels, out_channels):\n",
    "#         super(Conv3,self).__init__() \n",
    "#         self.headblock = HeadBlock(in_channels,mid_channels,out_channels)\n",
    "#         self.identityblock1 = IdentityBlock(out_channels,mid_channels,out_channels)\n",
    "#         self.identityblock2 = IdentityBlock(out_channels,mid_channels,out_channels)\n",
    "#         self.identityblock3 = IdentityBlock(out_channels,mid_channels,out_channels)\n",
    "#         self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "#     def forward(self,x):\n",
    "#         x = self.headblock(x)\n",
    "#         x = self.identityblock1(x)\n",
    "#         x = self.identityblock2(x)\n",
    "#         x = self.identityblock3(x)\n",
    "#         p = self.maxpool(x)\n",
    "#         return x , p\n",
    "# class Conv4(nn.Module):\n",
    "#     def __init__(self,in_channels, mid_channels, out_channels):\n",
    "#         super(Conv4,self).__init__() \n",
    "#         self.headblock = HeadBlock(in_channels,mid_channels,out_channels)\n",
    "#         self.identityblock1 = IdentityBlock(out_channels,mid_channels,out_channels)\n",
    "#         self.identityblock2 = IdentityBlock(out_channels,mid_channels,out_channels)\n",
    "#         self.identityblock3 = IdentityBlock(out_channels,mid_channels,out_channels)\n",
    "#         self.identityblock4 = IdentityBlock(out_channels,mid_channels,out_channels)\n",
    "#         self.identityblock5 = IdentityBlock(out_channels,mid_channels,out_channels)\n",
    "#         self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "#     def forward(self,x):\n",
    "#         x = self.headblock(x)\n",
    "#         x = self.identityblock1(x)\n",
    "#         x = self.identityblock2(x)\n",
    "#         x = self.identityblock3(x)\n",
    "#         x = self.identityblock4(x)\n",
    "#         x = self.identityblock5(x)\n",
    "#         p = self.maxpool(x)\n",
    "#         return x , p\n",
    "# class Conv5(nn.Module):\n",
    "#     def __init__(self,in_channels, mid_channels, out_channels):\n",
    "#         super(Conv5,self).__init__() \n",
    "#         self.headblock = HeadBlock(in_channels,mid_channels,out_channels)\n",
    "#         self.identityblock1 = IdentityBlock(out_channels,mid_channels,out_channels)\n",
    "#         self.identityblock2 = IdentityBlock(out_channels,mid_channels,out_channels)\n",
    "#         self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "#     def forward(self,x):\n",
    "#         x = self.headblock(x)\n",
    "#         x = self.identityblock1(x)\n",
    "#         x = self.identityblock2(x)\n",
    "#         p = self.maxpool(x)\n",
    "#         return x , p\n",
    "# #디코더 블럭\n",
    "# class DecoderBlock(nn.Module):\n",
    "#     def __init__(self, channels):\n",
    "#         super(DecoderBlock, self).__init__()\n",
    "#         self.upsample = nn.ConvTranspose2d(channels*2, channels, kernel_size=4, stride=2, padding=1, output_padding=1) # output_padding 추가\n",
    "#         self.convblock1 = ConvBlock(channels*2, channels)\n",
    "\n",
    "#     def forward(self, x, skip):\n",
    "#         x = self.upsample(x)\n",
    "#         if x.size(2) != skip.size(2) or x.size(3) != skip.size(3):\n",
    "#             x = F.interpolate(x, size=(skip.size(2), skip.size(3)))\n",
    "#         x = torch.cat([x, skip], dim=1)\n",
    "#         x = self.convblock1(x)\n",
    "#         return x\n",
    "\n",
    "# #Unet구조 middle의 xm값의 움직임에 주의\n",
    "# class Resnet50_Unet(nn.Module):\n",
    "#     def __init__(self,n_classes):\n",
    "#         super(Resnet50_Unet,self).__init__()\n",
    "#         self.fconv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2)\n",
    "#         self.fbn1 = nn.BatchNorm2d(64)\n",
    "#         self.frelu1 = nn.ReLU()\n",
    "#         self.fconv2 = nn.Conv2d(64, 128, kernel_size=1, stride=1)\n",
    "#         self.fbn2 = nn.BatchNorm2d(128)\n",
    "#         self.frelu2 = nn.ReLU()\n",
    "#         self.fmaxpooling = nn.MaxPool2d(kernel_size=3,stride=2)\n",
    "        \n",
    "#         self.conv2 = Conv2(128,64,256)\n",
    "#         self.conv3 = Conv3(256,128,512)\n",
    "#         self.conv4 = Conv4(512,256,1024)\n",
    "#         self.conv5 = Conv5(1024,512,2048)\n",
    "        \n",
    "#         self.middleconv = ConvBlock(2048,4096)\n",
    "#         self.dropout = nn.Dropout2d(0.4) #\n",
    "           \n",
    "#         self.decoder5 = DecoderBlock(2048)\n",
    "#         self.decoder4 = DecoderBlock(1024)\n",
    "#         self.decoder3 = DecoderBlock(512)\n",
    "#         self.decoder2 = DecoderBlock(256)\n",
    "#         self.decoder1 = DecoderBlock(128)\n",
    "        \n",
    "#         self.segmap = nn.Conv2d(128,n_classes, kernel_size=1)\n",
    "#         self.domain_classifier = domain_classifier()\n",
    "        \n",
    "#     def forward(self,x):\n",
    "#         x = self.fconv1(x)#3->64\n",
    "#         x = self.fbn1(x)\n",
    "#         x = self.frelu1(x)\n",
    "#         x = self.fconv2(x)\n",
    "#         x = self.fbn2(x)\n",
    "#         x1 = self.frelu2(x)\n",
    "#         p = self.fmaxpooling(x)#첫 conv: x0([8, 64, 109, 109]) p([8, 64, 54, 54])\n",
    "#         x2,p = self.conv2(p)#conv2:  x1:([8, 256, 54, 54]) p([8, 256, 26, 26])\n",
    "#         x3,p = self.conv3(p)#conv3:  x2([8, 512, 26, 26]) p([8, 512, 12, 12])\n",
    "#         x4,p = self.conv4(p)#conv4:  x3([8, 1024, 12, 12]) p([8, 1024, 5, 5])\n",
    "#         x5,p = self.conv5(p)#conv5:  x4([8, 2048, 5, 5]) p([8, 2048, 2, 2])\n",
    "        \n",
    "#         xm = self.middleconv(p)#xm([8, 4096, 2, 2])\n",
    "#         xm = self.dropout(xm)\n",
    "        \n",
    "#         x = self.decoder5(xm,x5)#뉴런:2048*2->2048 1\n",
    "#         x = self.decoder4(x,x4)#뉴런:1024*2->1024 \n",
    "#         x = self.decoder3(x,x3) #14\n",
    "#         x = self.decoder2(x,x2)#28\n",
    "#         x = self.decoder1(x,x1)#55\n",
    "        \n",
    "#         x = F.interpolate(x, size=(224, 224))\n",
    "#         x_c = self.segmap(x)\n",
    "#         x_d = self.domain_classifier(x)\n",
    "#         #print(x.shape)\n",
    "#         return x_c,x_d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resnet34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradReverse(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(self, x):\n",
    "        return x.view_as(x)\n",
    "    @staticmethod\n",
    "    def backward(self, grad_output): # 역전파 시에 gradient에 음수를 취함\n",
    "        return grad_output * (-1)\n",
    "\n",
    "class domain_classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(domain_classifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(224*224*64, 10)\n",
    "        self.fc2 = nn.Linear(10, 4) # source = 0, target = 1 회귀 가정\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 224*224*64)\n",
    "        x = GradReverse.apply(x) # gradient reverse\n",
    "        x = F.leaky_relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        #return torch.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "class domain_linear(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(domain_linear, self).__init__()\n",
    "        self.fc1 = nn.Linear(224*224*64, 10)\n",
    "        self.fc2 = nn.Linear(10, 1) # source = 0, target = 1 회귀 가정\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 224*224*64)\n",
    "        x = GradReverse.apply(x) # gradient reverse\n",
    "        x = F.leaky_relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return torch.sigmoid(x)\n",
    "        #return x\n",
    "class IdentityBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(IdentityBlock, self).__init__()\n",
    "        \n",
    "        # 3x3 convolution\n",
    "        self.conv1 = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1, stride=stride, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(in_channels)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        # 3x3 convolution\n",
    "        self.conv2 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        # Skip connection\n",
    "        self.skip = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.skip = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        \n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu1(out)\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        \n",
    "        # Adding the skip connection\n",
    "        out += self.skip(identity)\n",
    "        out = self.relu2(out)\n",
    "        \n",
    "        return out\n",
    "#인코더 블럭\n",
    "class Conv2(nn.Module):\n",
    "    def __init__(self,in_channels, out_channels):\n",
    "        super(Conv2,self).__init__() \n",
    "        self.identityblock1 = IdentityBlock(in_channels,in_channels)\n",
    "        self.identityblock2 = IdentityBlock(in_channels,in_channels)\n",
    "        self.identityblock3 = IdentityBlock(in_channels,out_channels)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2,padding=1)\n",
    "    def forward(self,x):\n",
    "        x = self.identityblock1(x)\n",
    "        x = self.identityblock2(x)\n",
    "        x = self.identityblock3(x)\n",
    "        p = self.maxpool(x)\n",
    "        \n",
    "        return x , p\n",
    "class Conv3(nn.Module):\n",
    "    def __init__(self,in_channels, out_channels):\n",
    "        super(Conv3,self).__init__()         \n",
    "        self.identityblock1 = IdentityBlock(in_channels,in_channels)\n",
    "        self.identityblock2 = IdentityBlock(in_channels,in_channels)\n",
    "        self.identityblock3 = IdentityBlock(in_channels,in_channels)\n",
    "        self.identityblock4 = IdentityBlock(in_channels,out_channels)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2,padding=1)\n",
    "    def forward(self,x):\n",
    "        x = self.identityblock1(x)\n",
    "        x = self.identityblock2(x)\n",
    "        x = self.identityblock3(x)\n",
    "        x = self.identityblock4(x)\n",
    "        p = self.maxpool(x)\n",
    "        \n",
    "        return x , p\n",
    "class Conv4(nn.Module):\n",
    "    def __init__(self,in_channels, out_channels):\n",
    "        super(Conv4,self).__init__()         \n",
    "        self.identityblock1 = IdentityBlock(in_channels,in_channels)\n",
    "        self.identityblock2 = IdentityBlock(in_channels,in_channels)\n",
    "        self.identityblock3 = IdentityBlock(in_channels,in_channels)\n",
    "        self.identityblock4 = IdentityBlock(in_channels,in_channels)\n",
    "        self.identityblock5 = IdentityBlock(in_channels,in_channels)\n",
    "        self.identityblock6 = IdentityBlock(in_channels,out_channels)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2,padding=1)\n",
    "    def forward(self,x):\n",
    "        x = self.identityblock1(x)\n",
    "        x = self.identityblock2(x)\n",
    "        x = self.identityblock3(x)\n",
    "        x = self.identityblock4(x)\n",
    "        x = self.identityblock5(x)\n",
    "        x = self.identityblock6(x)\n",
    "        p = self.maxpool(x)\n",
    "        \n",
    "        return x , p\n",
    "class Conv5(nn.Module):\n",
    "    def __init__(self,in_channels, out_channels):\n",
    "        super(Conv5,self).__init__() \n",
    "        self.identityblock1 = IdentityBlock(in_channels,in_channels)\n",
    "        self.identityblock2 = IdentityBlock(in_channels,in_channels)\n",
    "        self.identityblock3 = IdentityBlock(in_channels,out_channels)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2,padding=1)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.identityblock1(x)\n",
    "        x = self.identityblock2(x)\n",
    "        x = self.identityblock3(x)\n",
    "        p = self.maxpool(x)\n",
    "        \n",
    "        return x , p\n",
    "#디코더 블럭\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.upsample = nn.ConvTranspose2d(channels*2, channels, kernel_size=4, stride=2, padding=1) # output_padding 추가\n",
    "        self.convblock1 = IdentityBlock(channels*2, channels)\n",
    "\n",
    "    def forward(self, x, skip):\n",
    "        x = self.upsample(x)\n",
    "        if x.size(2) != skip.size(2) or x.size(3) != skip.size(3):\n",
    "            x = F.interpolate(x, size=(skip.size(2), skip.size(3)))\n",
    "        x = torch.cat([x, skip], dim=1)\n",
    "        x = self.convblock1(x)\n",
    "        #print(\"x\",x.shape,\"skip: \",skip.shape)\n",
    "        return x\n",
    "\n",
    "#Unet구조 middle의 xm값의 움직임에 주의\n",
    "class Resnet34_Unet(nn.Module):\n",
    "    def __init__(self,n_classes):\n",
    "        super(Resnet34_Unet,self).__init__()\n",
    "        self.fconv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3)\n",
    "        self.fbn1 = nn.BatchNorm2d(64)\n",
    "        self.frelu1 = nn.ReLU()\n",
    "        self.fmaxpooling = nn.MaxPool2d(kernel_size=3,stride=2,padding=1)\n",
    "        \n",
    "        self.conv2 = Conv2(64,128)\n",
    "        self.conv3 = Conv3(128,256)\n",
    "        self.conv4 = Conv4(256,512)\n",
    "        self.conv5 = Conv5(512,1024)\n",
    "        \n",
    "        self.middleconv = IdentityBlock(1024,2048)\n",
    "        self.dropout = nn.Dropout2d(0.1) #\n",
    "           \n",
    "        self.decoder5 = DecoderBlock(1024)\n",
    "        self.decoder4 = DecoderBlock(512)\n",
    "        self.decoder3 = DecoderBlock(256)\n",
    "        self.decoder2 = DecoderBlock(128)\n",
    "        self.decoder1 = DecoderBlock(64)\n",
    "        self.transpose = nn.ConvTranspose2d(64, 64, kernel_size=4, stride=2, padding=1) # output_padding 추가\n",
    "        \n",
    "        self.segmap = nn.Conv2d(64,n_classes, kernel_size=1)\n",
    "        self.domain_classifier = domain_classifier()\n",
    "        self.domain_linear = domain_linear()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.fconv1(x)#3->64\n",
    "        x0 = self.fbn1(x)\n",
    "        x1 = self.frelu1(x)\n",
    "        p = self.fmaxpooling(x1)#첫 conv: x0([8, 64, 109, 109]) p([8, 64, 54, 54])\n",
    "        #print(\"conv1: \",x1.shape, \"maxpooling: \",p.shape)\n",
    "        x2,p = self.conv2(p)\n",
    "        #print(\"conv2: \",x2.shape, \"maxpooling: \",p.shape)\n",
    "        x3,p = self.conv3(p)\n",
    "        #print(\"conv3: \",x3.shape, \"maxpooling: \",p.shape)\n",
    "        x4,p = self.conv4(p)\n",
    "        #print(\"conv4: \",x4.shape, \"maxpooling: \",p.shape)\n",
    "        x5,p = self.conv5(p)\n",
    "        #print(\"conv5: \",x5.shape, \"maxpooling: \",p.shape)\n",
    "        \n",
    "        xm = self.middleconv(p)#xm([8, 4096, 2, 2])\n",
    "        #print(\"xm: \",xm.shape, \"maxpooling: \",p.shape)\n",
    "        xm = self.dropout(xm)\n",
    "        \n",
    "        x = self.decoder5(xm,x5)#뉴런:2048*2->2048 1\n",
    "        x = self.decoder4(x,x4)#뉴런:1024*2->1024 \n",
    "        x = self.decoder3(x,x3) #14\n",
    "        x = self.decoder2(x,x2)#28\n",
    "        x = self.decoder1(x,x1)#55\n",
    "        x = self.transpose(x)\n",
    "        \n",
    "        #print(x.shape)\n",
    "        #x = F.interpolate(x, size=(224, 224))\n",
    "        x_c = self.segmap(x)\n",
    "        #x_d = self.domain_linear(x)\n",
    "        x_d = self.domain_classifier(x)\n",
    "        \n",
    "        \n",
    "        return x_c,x_d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resnet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class GradReverse(torch.autograd.Function):\n",
    "#     @staticmethod\n",
    "#     def forward(self, x):\n",
    "#         return x.view_as(x)\n",
    "#     @staticmethod\n",
    "#     def backward(self, grad_output): # 역전파 시에 gradient에 음수를 취함\n",
    "#         return grad_output * (-1)\n",
    "\n",
    "# class domain_classifier(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(domain_classifier, self).__init__()\n",
    "#         self.fc1 = nn.Linear(224*224*64, 10)\n",
    "#         self.fc2 = nn.Linear(10, 4) # source = 0, target = 1 회귀 가정\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = x.view(-1, 224*224*64)\n",
    "#         x = GradReverse.apply(x) # gradient reverse\n",
    "#         x = F.leaky_relu(self.fc1(x))\n",
    "#         x = self.fc2(x)\n",
    "        \n",
    "#         #return torch.sigmoid(x)\n",
    "#         return x\n",
    "\n",
    "# class IdentityBlock(nn.Module):\n",
    "#     def __init__(self, in_channels, out_channels, stride=1):\n",
    "#         super(IdentityBlock, self).__init__()\n",
    "        \n",
    "#         # 3x3 convolution\n",
    "#         self.conv1 = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1, stride=stride, bias=False)\n",
    "#         self.bn1 = nn.BatchNorm2d(in_channels)\n",
    "#         self.relu1 = nn.ReLU()\n",
    "        \n",
    "#         # 3x3 convolution\n",
    "#         self.conv2 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False)\n",
    "#         self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "#         self.relu2 = nn.ReLU()\n",
    "        \n",
    "#         # Skip connection\n",
    "#         self.skip = nn.Sequential()\n",
    "#         if stride != 1 or in_channels != out_channels:\n",
    "#             self.skip = nn.Sequential(\n",
    "#                 nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "#                 nn.BatchNorm2d(out_channels)\n",
    "#             )\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         identity = x\n",
    "        \n",
    "#         out = self.conv1(x)\n",
    "#         out = self.bn1(out)\n",
    "#         out = self.relu1(out)\n",
    "        \n",
    "#         out = self.conv2(out)\n",
    "#         out = self.bn2(out)\n",
    "        \n",
    "#         # Adding the skip connection\n",
    "#         out += self.skip(identity)\n",
    "#         out = self.relu2(out)\n",
    "        \n",
    "#         return out\n",
    "\n",
    "\n",
    "# #인코더 블럭\n",
    "# class Conv2(nn.Module):\n",
    "#     def __init__(self,in_channels, out_channels):\n",
    "#         super(Conv2,self).__init__() \n",
    "#         self.identityblock1 = IdentityBlock(in_channels,in_channels)\n",
    "#         self.identityblock2 = IdentityBlock(in_channels,out_channels)\n",
    "#         self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2,padding=1)\n",
    "#     def forward(self,x):\n",
    "#         x = self.identityblock1(x)\n",
    "#         x = self.identityblock2(x)\n",
    "#         p = self.maxpool(x)\n",
    "        \n",
    "#         return x , p\n",
    "# class Conv3(nn.Module):\n",
    "#     def __init__(self,in_channels, out_channels):\n",
    "#         super(Conv3,self).__init__()         \n",
    "#         self.identityblock1 = IdentityBlock(in_channels,in_channels)\n",
    "#         self.identityblock2 = IdentityBlock(in_channels,out_channels)\n",
    "#         self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2,padding=1)\n",
    "#     def forward(self,x):\n",
    "#         x = self.identityblock1(x)\n",
    "#         x = self.identityblock2(x)\n",
    "#         p = self.maxpool(x)\n",
    "        \n",
    "#         return x , p\n",
    "# class Conv4(nn.Module):\n",
    "#     def __init__(self,in_channels, out_channels):\n",
    "#         super(Conv4,self).__init__()         \n",
    "#         self.identityblock1 = IdentityBlock(in_channels,in_channels)\n",
    "#         self.identityblock2 = IdentityBlock(in_channels,out_channels)\n",
    "#         self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2,padding=1)\n",
    "#     def forward(self,x):\n",
    "#         x = self.identityblock1(x)\n",
    "#         x = self.identityblock2(x)\n",
    "#         p = self.maxpool(x)\n",
    "        \n",
    "#         return x , p\n",
    "# class Conv5(nn.Module):\n",
    "#     def __init__(self,in_channels, out_channels):\n",
    "#         super(Conv5,self).__init__() \n",
    "#         self.identityblock1 = IdentityBlock(in_channels,in_channels)\n",
    "#         self.identityblock2 = IdentityBlock(in_channels,out_channels)\n",
    "#         self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2,padding=1)\n",
    "        \n",
    "#     def forward(self,x):\n",
    "#         x = self.identityblock1(x)\n",
    "#         x = self.identityblock2(x)\n",
    "#         p = self.maxpool(x)\n",
    "        \n",
    "#         return x , p\n",
    "# #디코더 블럭\n",
    "# class DecoderBlock(nn.Module):\n",
    "#     def __init__(self, channels):\n",
    "#         super(DecoderBlock, self).__init__()\n",
    "#         self.upsample = nn.ConvTranspose2d(channels*2, channels, kernel_size=4, stride=2, padding=1) # output_padding 추가\n",
    "#         self.convblock1 = IdentityBlock(channels*2, channels)\n",
    "\n",
    "#     def forward(self, x, skip):\n",
    "#         x = self.upsample(x)\n",
    "#         if x.size(2) != skip.size(2) or x.size(3) != skip.size(3):\n",
    "#             x = F.interpolate(x, size=(skip.size(2), skip.size(3)))\n",
    "#         x = torch.cat([x, skip], dim=1)\n",
    "#         x = self.convblock1(x)\n",
    "#         #print(\"x\",x.shape,\"skip: \",skip.shape)\n",
    "#         return x\n",
    "\n",
    "# #Unet구조 middle의 xm값의 움직임에 주의\n",
    "# class Resnet18_Unet(nn.Module):\n",
    "#     def __init__(self,n_classes):\n",
    "#         super(Resnet18_Unet,self).__init__()\n",
    "#         self.fconv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3)\n",
    "#         self.fbn1 = nn.BatchNorm2d(64)\n",
    "#         self.frelu1 = nn.ReLU()\n",
    "#         self.fmaxpooling = nn.MaxPool2d(kernel_size=3,stride=2,padding=1)\n",
    "        \n",
    "#         self.conv2 = Conv2(64,128)\n",
    "#         self.conv3 = Conv3(128,256)\n",
    "#         self.conv4 = Conv4(256,512)\n",
    "#         self.conv5 = Conv5(512,1024)\n",
    "        \n",
    "#         self.middleconv = IdentityBlock(1024,2048)\n",
    "#         self.dropout = nn.Dropout2d(0.1) #\n",
    "           \n",
    "#         self.decoder5 = DecoderBlock(1024)\n",
    "#         self.decoder4 = DecoderBlock(512)\n",
    "#         self.decoder3 = DecoderBlock(256)\n",
    "#         self.decoder2 = DecoderBlock(128)\n",
    "#         self.decoder1 = DecoderBlock(64)\n",
    "#         self.transpose = nn.ConvTranspose2d(64, 64, kernel_size=4, stride=2, padding=1) # output_padding 추가\n",
    "        \n",
    "#         self.segmap = nn.Conv2d(64,n_classes, kernel_size=1)\n",
    "#         self.domain_classifier = domain_classifier()\n",
    "        \n",
    "#     def forward(self,x):\n",
    "#         x = self.fconv1(x)#3->64\n",
    "#         x0 = self.fbn1(x)\n",
    "#         x1 = self.frelu1(x)\n",
    "#         p = self.fmaxpooling(x1)#첫 conv: x0([8, 64, 109, 109]) p([8, 64, 54, 54])\n",
    "#         #print(\"conv1: \",x1.shape, \"maxpooling: \",p.shape)\n",
    "#         x2,p = self.conv2(p)\n",
    "#         #print(\"conv2: \",x2.shape, \"maxpooling: \",p.shape)\n",
    "#         x3,p = self.conv3(p)\n",
    "#         #print(\"conv3: \",x3.shape, \"maxpooling: \",p.shape)\n",
    "#         x4,p = self.conv4(p)\n",
    "#         #print(\"conv4: \",x4.shape, \"maxpooling: \",p.shape)\n",
    "#         x5,p = self.conv5(p)\n",
    "#         #print(\"conv5: \",x5.shape, \"maxpooling: \",p.shape)\n",
    "        \n",
    "#         xm = self.middleconv(p)#xm([8, 4096, 2, 2])\n",
    "#         #print(\"xm: \",xm.shape, \"maxpooling: \",p.shape)\n",
    "#         xm = self.dropout(xm)\n",
    "        \n",
    "#         x = self.decoder5(xm,x5)#뉴런:2048*2->2048 1\n",
    "#         x = self.decoder4(x,x4)#뉴런:1024*2->1024 \n",
    "#         x = self.decoder3(x,x3) #14\n",
    "#         x = self.decoder2(x,x2)#28\n",
    "#         x = self.decoder1(x,x1)#55\n",
    "#         x = self.transpose(x)\n",
    "        \n",
    "#         #print(x.shape)\n",
    "#         #x = F.interpolate(x, size=(224, 224))\n",
    "#         x_c = self.segmap(x)\n",
    "#         x_d = self.domain_classifier(x)\n",
    "        \n",
    "#         return x_c,x_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# # 저장된 class_weights를 불러옵니다.\n",
    "# class_weights_path = 'CLASS_WEIGHTS.pkl'\n",
    "\n",
    "# with open(class_weights_path, 'rb') as file:\n",
    "#     CLASS_WEIGHTS = pickle.load(file)\n",
    "\n",
    "# print(CLASS_WEIGHTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function과 optimizer 정의\n",
    "\n",
    "class DANN_Loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DANN_Loss, self).__init__()\n",
    "\n",
    "        #self.CE = nn.CrossEntropyLoss(weight=CLASS_WEIGHTS) # 0~9 class 분류용\n",
    "        self.CE = nn.CrossEntropyLoss()\n",
    "        #self.BCE = nn.BCELoss() # 도메인 분류용\n",
    "        \n",
    "    # result : DANN_CNN에서 반환된 값\n",
    "    # label : 숫자 0 ~ 9에 대한 라벨\n",
    "    # domain_num : 0(source) or 1(target)\n",
    "    def forward(self, result, label, domain_num, alpha = 1):\n",
    "        label_logits, domain_logits = result # DANN_CNN의 결과\n",
    "\n",
    "        batch_size = domain_logits.shape[0]\n",
    "\n",
    "        # print(\"segment_mask : \", label.shape)\n",
    "        # print(\"domain_answer : \", domain_target.shape)\n",
    "        segment_loss = self.CE(label_logits, label) # class 분류 loss\n",
    "\n",
    "        # domain_target = torch.FloatTensor([domain_num] * batch_size).unsqueeze(1).to(device)\n",
    "        # domain_loss = self.BCE(domain_logits, domain_target)\n",
    "\n",
    "        domain_target = torch.LongTensor([domain_num] * batch_size).to(device)\n",
    "        domain_loss = self.CE(domain_logits, domain_target) # domain 분류 loss\n",
    "        \n",
    "        loss = segment_loss + alpha * domain_loss\n",
    "\n",
    "        return loss, segment_loss, domain_loss\n",
    "    \n",
    "\n",
    "loss_fn = DANN_Loss().to(device)\n",
    "\n",
    "#criterion =nn.CrossEntropyLoss()\n",
    "#domain_criterion = nn.BCELoss()\n",
    "#criterion = nn.CrossEntropyLoss(weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/MMI24limbyungwan/Design_Project/Design_project/Domain_Adaptation_LBW_4d.ipynb 셀 18\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B163.239.13.23/home/MMI24limbyungwan/Design_Project/Design_project/Domain_Adaptation_LBW_4d.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m BATCH_SIZE \u001b[39m=\u001b[39m \u001b[39m16\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B163.239.13.23/home/MMI24limbyungwan/Design_Project/Design_project/Domain_Adaptation_LBW_4d.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m ACCMULATION_STEP \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B163.239.13.23/home/MMI24limbyungwan/Design_Project/Design_project/Domain_Adaptation_LBW_4d.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m N_CLASSES \u001b[39m=\u001b[39m \u001b[39m13\u001b[39m \u001b[39m#IoU 점수측정하기 위한 클래스의 개수\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B163.239.13.23/home/MMI24limbyungwan/Design_Project/Design_project/Domain_Adaptation_LBW_4d.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m ALPHA \u001b[39m=\u001b[39m \u001b[39m0.0001\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B163.239.13.23/home/MMI24limbyungwan/Design_Project/Design_project/Domain_Adaptation_LBW_4d.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m N_LABELS \u001b[39m=\u001b[39m \u001b[39m4\u001b[39m\n",
      "\u001b[1;32m/home/MMI24limbyungwan/Design_Project/Design_project/Domain_Adaptation_LBW_4d.ipynb 셀 18\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B163.239.13.23/home/MMI24limbyungwan/Design_Project/Design_project/Domain_Adaptation_LBW_4d.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m BATCH_SIZE \u001b[39m=\u001b[39m \u001b[39m16\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B163.239.13.23/home/MMI24limbyungwan/Design_Project/Design_project/Domain_Adaptation_LBW_4d.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m ACCMULATION_STEP \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B163.239.13.23/home/MMI24limbyungwan/Design_Project/Design_project/Domain_Adaptation_LBW_4d.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m N_CLASSES \u001b[39m=\u001b[39m \u001b[39m13\u001b[39m \u001b[39m#IoU 점수측정하기 위한 클래스의 개수\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B163.239.13.23/home/MMI24limbyungwan/Design_Project/Design_project/Domain_Adaptation_LBW_4d.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m ALPHA \u001b[39m=\u001b[39m \u001b[39m0.0001\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B163.239.13.23/home/MMI24limbyungwan/Design_Project/Design_project/Domain_Adaptation_LBW_4d.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m N_LABELS \u001b[39m=\u001b[39m \u001b[39m4\u001b[39m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1363\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:662\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1288\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1250\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:297\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.conda/envs/byungwan_resn/lib/python3.8/site-packages/debugpy/_vendored/pydevd/pydevd.py:1976\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[1;32m   1973\u001b[0m             from_this_thread\u001b[39m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[1;32m   1975\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_threads_suspended_single_notification\u001b[39m.\u001b[39mnotify_thread_suspended(thread_id, stop_reason):\n\u001b[0;32m-> 1976\u001b[0m         keep_suspended \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_wait_suspend(thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\n\u001b[1;32m   1978\u001b[0m frames_list \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1980\u001b[0m \u001b[39mif\u001b[39;00m keep_suspended:\n\u001b[1;32m   1981\u001b[0m     \u001b[39m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/byungwan_resn/lib/python3.8/site-packages/debugpy/_vendored/pydevd/pydevd.py:2011\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_mpl_hook()\n\u001b[1;32m   2010\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocess_internal_commands()\n\u001b[0;32m-> 2011\u001b[0m     time\u001b[39m.\u001b[39;49msleep(\u001b[39m0.01\u001b[39;49m)\n\u001b[1;32m   2013\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[39mstr\u001b[39m(\u001b[39mid\u001b[39m(frame)))\n\u001b[1;32m   2015\u001b[0m \u001b[39m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "LR = 0.001\n",
    "EP = 10\n",
    "BATCH_SIZE = 16\n",
    "ACCMULATION_STEP = 1 \n",
    "N_CLASSES = 13 #IoU 점수측정하기 위한 클래스의 개수\n",
    "ALPHA = 0.0001\n",
    "N_LABELS = 4\n",
    "Label = [0,1,2,3]\n",
    "# model 초기화\n",
    "#model = Resnet18_Unet(n_classes = N_CLASSES).to(device)\n",
    "model = Resnet34_Unet(n_classes = N_CLASSES).to(device)\n",
    "#model = Resnet50_Unet(n_classes = N_CLASSES).to(device)\n",
    "#model = Unet(n_classes = N_CLASSES).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "optimizer.zero_grad() \n",
    "\n",
    "source_dataset = CustomDataset(csv_file='./data/896_csv/train_source.csv', transform=transform)\n",
    "source_dataloader = DataLoader(source_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_target_dataset = CustomDataset(csv_file='./data/896_csv/val_source.csv', transform=transform)\n",
    "val_target_dataloader = DataLoader(val_target_dataset, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import os\n",
    "# import cv2\n",
    "# import matplotlib.pyplot as plt\n",
    "# a1 = pd.read_csv(\"./data/6_fish_source.csv\")\n",
    "# p = \"./data/224/\"\n",
    "# a2 = os.path.join(p, a1.iloc[3,2])\n",
    "# a3 = cv2.imread(a2)\n",
    "# a4 = cv2.cvtColor(a3, cv2.COLOR_BGR2GRAY)\n",
    "# a4 = np.round(a4).astype(np.uint8)\n",
    "# a5 = a4*20\n",
    "\n",
    "# plt.imshow(a5, cmap='gray')\n",
    "# plt.axis('off')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "# #torch.cuda.empty_cache()\n",
    "# # import wandb\n",
    "\n",
    "\n",
    "# # wandb.init(\n",
    "# #     # set the wandb project where this run will be logged\n",
    "# #     project=\"practice_10_27_4d_res18\",\n",
    "    \n",
    "# #     # track hyperparameters and run metadata\n",
    "# #     config={\n",
    "# #     \"learning_rate\": LR,\n",
    "# #     \"architecture\": \"CNN\",\n",
    "# #     \"dataset\": \"Samsung\",\n",
    "# #     \"epochs\": EP,\n",
    "# #     }\n",
    "# # )\n",
    "\n",
    "# for epoch in range(EP):\n",
    "#     # 클래스별 IoU를 누적할 리스트 초기화\n",
    "#     train_class_ious = []\n",
    "#     fish_train_class_ious = []\n",
    "#     # 학습\n",
    "#     model.train()\n",
    "#     epoch_loss = 0\n",
    "#     seg_loss = 0\n",
    "#     dom_loss = 0\n",
    "\n",
    "#     for source_images, source_masks in tqdm(source_dataloader):\n",
    "#         label = random.randint(0,3)\n",
    "#         source_images, source_masks = apply_fisheye_distortion(source_images, source_masks, label)\n",
    "#         source_images = source_images.float().to(device)\n",
    "#         source_masks = source_masks.long().to(device)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         source_outputs = model(source_images)\n",
    "\n",
    "#         source_loss, segment_loss, domain_loss = loss_fn(source_outputs, source_masks, label, alpha = ALPHA)\n",
    "\n",
    "#         loss = source_loss\n",
    "#         epoch_loss += loss.item()\n",
    "#         seg_loss += segment_loss.item()\n",
    "#         dom_loss += domain_loss.item()\n",
    "\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         #scheduler.step()\n",
    "#                 # train 클래스별 IoU 계산\n",
    "#         source_outputs = torch.softmax(source_outputs[0], dim=1).cpu()\n",
    "#         source_outputs = torch.argmax(source_outputs, dim=1).numpy()\n",
    "\n",
    "#         for class_id in range(N_CLASSES):\n",
    "#             iou = calculate_iou_per_class(np.array(source_masks.cpu()), np.array(source_outputs), class_id)\n",
    "#             train_class_ious.append(iou)\n",
    "\n",
    "#     train_class_ious = np.array(train_class_ious).reshape(N_CLASSES, -1)\n",
    "#     train_class_ious = np.mean(train_class_ious, axis=1)\n",
    "#     print(\"--IoU Scores Train--\")\n",
    "#     for class_id, iou in enumerate(train_class_ious):\n",
    "#         print(f'Class{class_id}: {iou:.4f}', end=\" \")\n",
    "#         if (class_id+1) % 7 == 0:\n",
    "#             print()\n",
    "\n",
    "#     # mIoU 계산\n",
    "#     train_mIoU = np.mean(train_class_ious)\n",
    "\n",
    "#     # 에폭마다 결과 출력 \n",
    "#     print(f\"\\nEpoch{epoch+1}\")\n",
    "#     print(f\"Train seg Loss: {(seg_loss/len(source_dataloader))}\")\n",
    "#     print(f\"Train dom Loss: {(dom_loss/len(source_dataloader))}\")\n",
    "#     print(f\"Train Loss: {(epoch_loss/len(source_dataloader))}\")\n",
    "#     print(f\"Train mIoU: {train_mIoU}\" )\n",
    "#     print(\"___________________________________________________________________________________________\\n\")\n",
    "\n",
    "\n",
    "#     ################################################################\n",
    "#     # 클래스별 IoU를 누적할 리스트 초기화\n",
    "#     val_class_ious = []\n",
    "#     fish_val_class_ious = []\n",
    "#     val_epoch_loss = 0\n",
    "#     val_seg_loss = 0\n",
    "#     val_dom_loss = 0\n",
    "#     # 학습\n",
    "#     with torch.no_grad():\n",
    "#         model.eval()\n",
    "\n",
    "#         for target_images, target_masks in tqdm(val_target_dataloader):\n",
    "#             label = 2.5\n",
    "#             target_images, target_masks = apply_fisheye_distortion(target_images, target_masks, label)\n",
    "#             target_images = target_images.float().to(device)\n",
    "#             target_masks = target_masks.long().to(device)\n",
    "\n",
    "#             target_outputs = model(target_images)\n",
    "\n",
    "#             target_loss, val_segment_loss, val_domain_loss = loss_fn(target_outputs, target_masks, 0, alpha = ALPHA)\n",
    "\n",
    "#             loss = target_loss\n",
    "\n",
    "#             val_epoch_loss += loss.item()\n",
    "#             val_seg_loss += val_segment_loss.item()\n",
    "#             val_dom_loss += val_domain_loss.item()\n",
    "\n",
    "#             # train 클래스별 IoU 계산\n",
    "#             target_outputs = torch.softmax(target_outputs[0], dim=1).cpu()\n",
    "#             target_outputs = torch.argmax(target_outputs, dim=1).numpy()\n",
    "\n",
    "#             for class_id in range(N_CLASSES):\n",
    "#                 iou = calculate_iou_per_class(np.array(target_masks.cpu()), np.array(target_outputs), class_id)\n",
    "#                 fish_val_class_ious.append(iou)\n",
    "\n",
    "#     fish_val_class_ious = np.array(fish_val_class_ious).reshape(N_CLASSES, -1)\n",
    "#     fish_val_class_ious = np.mean(fish_val_class_ious, axis=1)\n",
    "#     print()\n",
    "#     print(\"--IoU Scores Fish val--\")\n",
    "#     for class_id, iou in enumerate(fish_val_class_ious):\n",
    "#         print(f'Class{class_id}: {iou:.4f}', end=\" \")\n",
    "#         if (class_id+1) % 7 == 0:\n",
    "#             print()\n",
    "\n",
    "#     # mIoU 계산\n",
    "#     fish_val_mIoU = np.mean(fish_val_class_ious)\n",
    "\n",
    "#     # 에폭마다 결과 출력 \n",
    "#     print(f\"\\nEpoch{epoch+1}\")\n",
    "#     print(f\"Valid seg Loss: {(val_seg_loss/len(val_target_dataloader))}\")\n",
    "#     print(f\"Valid dom Loss: {(val_dom_loss/len(val_target_dataloader))}\")\n",
    "#     print(f\"Valid Loss: {(val_epoch_loss/len(val_target_dataloader))}\")\n",
    "#     print(f\"Valid mIoU: {fish_val_mIoU}\" )\n",
    "#     print(\"___________________________________________________________________________________________\\n\")\n",
    "\n",
    "# #     # log metrics to wandb\n",
    "# #     wandb.log({\"train score\": train_mIoU})\n",
    "# #     wandb.log({\"val score\": fish_val_mIoU})\n",
    "# #     wandb.log({\"train loss\": (epoch_loss/len(source_dataloader))})\n",
    "# #     wandb.log({\"val loss\": (val_epoch_loss/len(val_target_dataloader))})\n",
    "    \n",
    "    \n",
    "# # # [optional] finish the wandb run, necessary in notebooks\n",
    "# # wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1:   0%|          | 0/138 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1: 100%|██████████| 138/138 [06:24<00:00,  2.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label_0: IoU Scores Train\n",
      "Class00: 0.8694 Class01: 0.7132 Class02: 0.0104 Class03: 0.5695 Class04: 0.0725 Class05: 0.0000 Class06: 0.0000 \n",
      "Class07: 0.1147 Class08: 0.6219 Class09: 0.9037 Class10: 0.0000 Class11: 0.0000 Class12: 0.4654 \n",
      "Label_1: IoU Scores Train\n",
      "Class00: 0.2820 Class01: 0.5019 Class02: 0.0144 Class03: 0.3811 Class04: 0.0519 Class05: 0.0000 Class06: 0.0000 \n",
      "Class07: 0.0471 Class08: 0.4002 Class09: 0.5190 Class10: 0.0000 Class11: 0.0000 Class12: 0.3760 \n",
      "Label_2: IoU Scores Train\n",
      "Class00: 0.2414 Class01: 0.4249 Class02: 0.0106 Class03: 0.3300 Class04: 0.0437 Class05: 0.0000 Class06: 0.0000 \n",
      "Class07: 0.0351 Class08: 0.3448 Class09: 0.4285 Class10: 0.0000 Class11: 0.0000 Class12: 0.3343 \n",
      "Label_3: IoU Scores Train\n",
      "Class00: 0.2244 Class01: 0.3830 Class02: 0.0086 Class03: 0.2959 Class04: 0.0379 Class05: 0.0000 Class06: 0.0000 \n",
      "Class07: 0.0319 Class08: 0.3143 Class09: 0.3830 Class10: 0.0000 Class11: 0.0000 Class12: 0.3033 \n",
      "Train seg Loss: 0.44953753513054573 Train dom Loss: 2.488720782048229\n",
      "Train Loss: 0.4497864074681116\n",
      "Train mIoU: 0.21326279517329547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:14<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--IoU Scores Fish val--\n",
      "Class00: 0.9854 Class01: 0.7594 Class02: 0.0591 Class03: 0.5123 Class04: 0.0714 Class05: 0.0000 Class06: 0.0000 \n",
      "Class07: 0.1494 Class08: 0.5949 Class09: 0.9290 Class10: 0.0000 Class11: 0.0000 Class12: 0.5248 \n",
      "Epoch1\n",
      "Valid Seg Loss: 0.31790237526098886 Valid dom Loss: 70.18172938028971\n",
      "Valid Loss: 0.324920546511809\n",
      "Valid mIoU: 0.3527526033346507\n",
      "___________________________________________________________________________________________\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 2: 100%|██████████| 138/138 [06:23<00:00,  2.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label_0: IoU Scores Train\n",
      "Class00: 0.9478 Class01: 0.8077 Class02: 0.1563 Class03: 0.6839 Class04: 0.4073 Class05: 0.0000 Class06: 0.0880 \n",
      "Class07: 0.2598 Class08: 0.7187 Class09: 0.9315 Class10: 0.0000 Class11: 0.0000 Class12: 0.6917 \n",
      "Label_1: IoU Scores Train\n",
      "Class00: 0.2870 Class01: 0.5394 Class02: 0.1079 Class03: 0.4365 Class04: 0.2481 Class05: 0.0000 Class06: 0.0291 \n",
      "Class07: 0.1100 Class08: 0.4555 Class09: 0.5284 Class10: 0.0000 Class11: 0.0000 Class12: 0.4962 \n",
      "Label_2: IoU Scores Train\n",
      "Class00: 0.2435 Class01: 0.4488 Class02: 0.0823 Class03: 0.3743 Class04: 0.2010 Class05: 0.0000 Class06: 0.0177 \n",
      "Class07: 0.0821 Class08: 0.3881 Class09: 0.4368 Class10: 0.0000 Class11: 0.0000 Class12: 0.4310 \n",
      "Label_3: IoU Scores Train\n",
      "Class00: 0.2247 Class01: 0.3971 Class02: 0.0621 Class03: 0.3373 Class04: 0.1739 Class05: 0.0000 Class06: 0.0133 \n",
      "Class07: 0.0708 Class08: 0.3533 Class09: 0.3881 Class10: 0.0000 Class11: 0.0000 Class12: 0.3902 \n",
      "Train seg Loss: 0.29789743021778436 Train dom Loss: 2.1666906090782813e-05\n",
      "Train Loss: 0.297897432377373\n",
      "Train mIoU: 0.27014910643918605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:13<00:00,  2.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--IoU Scores Fish val--\n",
      "Class00: 0.9893 Class01: 0.7881 Class02: 0.1397 Class03: 0.2863 Class04: 0.1078 Class05: 0.0000 Class06: 0.0079 \n",
      "Class07: 0.0436 Class08: 0.6295 Class09: 0.1819 Class10: 0.0000 Class11: 0.0000 Class12: 0.5440 \n",
      "Epoch2\n",
      "Valid Seg Loss: 0.7142555276552837 Valid dom Loss: 68.04130655924479\n",
      "Valid Loss: 0.7210596601168314\n",
      "Valid mIoU: 0.28600397489689866\n",
      "___________________________________________________________________________________________\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 3: 100%|██████████| 138/138 [06:28<00:00,  2.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label_0: IoU Scores Train\n",
      "Class00: 0.9521 Class01: 0.8371 Class02: 0.2909 Class03: 0.7206 Class04: 0.4970 Class05: 0.0000 Class06: 0.1687 \n",
      "Class07: 0.3179 Class08: 0.7400 Class09: 0.9401 Class10: 0.0000 Class11: 0.0000 Class12: 0.7621 \n",
      "Label_1: IoU Scores Train\n",
      "Class00: 0.2872 Class01: 0.5475 Class02: 0.1683 Class03: 0.4531 Class04: 0.2899 Class05: 0.0000 Class06: 0.0591 \n",
      "Class07: 0.1332 Class08: 0.4654 Class09: 0.5295 Class10: 0.0000 Class11: 0.0000 Class12: 0.5249 \n",
      "Label_2: IoU Scores Train\n",
      "Class00: 0.2437 Class01: 0.4530 Class02: 0.1291 Class03: 0.3853 Class04: 0.2263 Class05: 0.0000 Class06: 0.0397 \n",
      "Class07: 0.0988 Class08: 0.3975 Class09: 0.4378 Class10: 0.0000 Class11: 0.0000 Class12: 0.4469 \n",
      "Label_3: IoU Scores Train\n",
      "Class00: 0.2247 Class01: 0.4003 Class02: 0.1035 Class03: 0.3486 Class04: 0.1951 Class05: 0.0000 Class06: 0.0307 \n",
      "Class07: 0.0846 Class08: 0.3607 Class09: 0.3884 Class10: 0.0000 Class11: 0.0000 Class12: 0.4062 \n",
      "Train seg Loss: 0.26034966837344825 Train dom Loss: 5.960688146339934e-06\n",
      "Train Loss: 0.2603496689403403\n",
      "Train mIoU: 0.29010341479425816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:14<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--IoU Scores Fish val--\n",
      "Class00: 0.9903 Class01: 0.7323 Class02: 0.0948 Class03: 0.5607 Class04: 0.0847 Class05: 0.0000 Class06: 0.0167 \n",
      "Class07: 0.0300 Class08: 0.6503 Class09: 0.9252 Class10: 0.0000 Class11: 0.0000 Class12: 0.5557 \n",
      "Epoch3\n",
      "Valid Seg Loss: 0.3191495423515638 Valid dom Loss: 79.54526952107747\n",
      "Valid Loss: 0.3271040678024292\n",
      "Valid mIoU: 0.35699790078500676\n",
      "___________________________________________________________________________________________\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 4: 100%|██████████| 138/138 [06:32<00:00,  2.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label_0: IoU Scores Train\n",
      "Class00: 0.9538 Class01: 0.8443 Class02: 0.3565 Class03: 0.7305 Class04: 0.5548 Class05: 0.0077 Class06: 0.2049 \n",
      "Class07: 0.3599 Class08: 0.7486 Class09: 0.9437 Class10: 0.0000 Class11: 0.0000 Class12: 0.7639 \n",
      "Label_1: IoU Scores Train\n",
      "Class00: 0.2874 Class01: 0.5493 Class02: 0.2030 Class03: 0.4583 Class04: 0.3147 Class05: 0.0009 Class06: 0.0650 \n",
      "Class07: 0.1518 Class08: 0.4711 Class09: 0.5299 Class10: 0.0000 Class11: 0.0000 Class12: 0.5253 \n",
      "Label_2: IoU Scores Train\n",
      "Class00: 0.2438 Class01: 0.4552 Class02: 0.1497 Class03: 0.3919 Class04: 0.2489 Class05: 0.0006 Class06: 0.0456 \n",
      "Class07: 0.1128 Class08: 0.4014 Class09: 0.4378 Class10: 0.0000 Class11: 0.0000 Class12: 0.4496 \n",
      "Label_3: IoU Scores Train\n",
      "Class00: 0.2247 Class01: 0.4016 Class02: 0.1217 Class03: 0.3521 Class04: 0.2129 Class05: 0.0006 Class06: 0.0363 \n",
      "Class07: 0.0984 Class08: 0.3637 Class09: 0.3884 Class10: 0.0000 Class11: 0.0000 Class12: 0.4072 \n",
      "Train seg Loss: 0.2472648694301429 Train dom Loss: 7.656406200397802e-06\n",
      "Train Loss: 0.2472648701859989\n",
      "Train mIoU: 0.2994262120101733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:13<00:00,  2.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--IoU Scores Fish val--\n",
      "Class00: 0.9928 Class01: 0.5755 Class02: 0.0759 Class03: 0.3148 Class04: 0.1288 Class05: 0.0016 Class06: 0.0195 \n",
      "Class07: 0.1182 Class08: 0.6744 Class09: 0.2318 Class10: 0.0000 Class11: 0.0000 Class12: 0.5982 \n",
      "Epoch4\n",
      "Valid Seg Loss: 0.8143147865931193 Valid dom Loss: 68.48375701904297\n",
      "Valid Loss: 0.8211631655693055\n",
      "Valid mIoU: 0.2870443860093491\n",
      "___________________________________________________________________________________________\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 5: 100%|██████████| 138/138 [06:26<00:00,  2.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label_0: IoU Scores Train\n",
      "Class00: 0.9567 Class01: 0.8636 Class02: 0.3964 Class03: 0.7635 Class04: 0.5865 Class05: 0.0513 Class06: 0.2607 \n",
      "Class07: 0.4100 Class08: 0.7795 Class09: 0.9491 Class10: 0.0004 Class11: 0.0000 Class12: 0.8188 \n",
      "Label_1: IoU Scores Train\n",
      "Class00: 0.2876 Class01: 0.5549 Class02: 0.2242 Class03: 0.4719 Class04: 0.3297 Class05: 0.0044 Class06: 0.0733 \n",
      "Class07: 0.1759 Class08: 0.4841 Class09: 0.5310 Class10: 0.0003 Class11: 0.0001 Class12: 0.5462 \n",
      "Label_2: IoU Scores Train\n",
      "Class00: 0.2439 Class01: 0.4578 Class02: 0.1674 Class03: 0.4005 Class04: 0.2588 Class05: 0.0029 Class06: 0.0554 \n",
      "Class07: 0.1306 Class08: 0.4111 Class09: 0.4384 Class10: 0.0001 Class11: 0.0002 Class12: 0.4639 \n",
      "Label_3: IoU Scores Train\n",
      "Class00: 0.2246 Class01: 0.4027 Class02: 0.1399 Class03: 0.3606 Class04: 0.2223 Class05: 0.0027 Class06: 0.0468 \n",
      "Class07: 0.1126 Class08: 0.3729 Class09: 0.3892 Class10: 0.0001 Class11: 0.0001 Class12: 0.4190 \n",
      "Train seg Loss: 0.21908264866341715 Train dom Loss: 2.413718849775822e-07\n",
      "Train Loss: 0.219082648690412\n",
      "Train mIoU: 0.31240154437813783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:13<00:00,  2.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--IoU Scores Fish val--\n",
      "Class00: 0.9924 Class01: 0.7939 Class02: 0.1567 Class03: 0.5044 Class04: 0.0809 Class05: 0.0037 Class06: 0.0322 \n",
      "Class07: 0.1748 Class08: 0.7156 Class09: 0.7664 Class10: 0.0000 Class11: 0.0000 Class12: 0.6614 \n",
      "Epoch5\n",
      "Valid Seg Loss: 0.3413064077496529 Valid dom Loss: 82.95020929972331\n",
      "Valid Loss: 0.34960142920414605\n",
      "Valid mIoU: 0.37556216239124995\n",
      "___________________________________________________________________________________________\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 6: 100%|██████████| 138/138 [06:04<00:00,  2.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label_0: IoU Scores Train\n",
      "Class00: 0.9416 Class01: 0.8545 Class02: 0.3944 Class03: 0.7523 Class04: 0.5519 Class05: 0.0647 Class06: 0.2610 \n",
      "Class07: 0.3915 Class08: 0.7662 Class09: 0.9463 Class10: 0.0021 Class11: 0.0000 Class12: 0.7918 \n",
      "Label_1: IoU Scores Train\n",
      "Class00: 0.2869 Class01: 0.5528 Class02: 0.2180 Class03: 0.4666 Class04: 0.3045 Class05: 0.0055 Class06: 0.0738 \n",
      "Class07: 0.1628 Class08: 0.4775 Class09: 0.5295 Class10: 0.0005 Class11: 0.0002 Class12: 0.5377 \n",
      "Label_2: IoU Scores Train\n",
      "Class00: 0.2437 Class01: 0.4572 Class02: 0.1653 Class03: 0.3984 Class04: 0.2407 Class05: 0.0035 Class06: 0.0554 \n",
      "Class07: 0.1205 Class08: 0.4073 Class09: 0.4379 Class10: 0.0003 Class11: 0.0001 Class12: 0.4595 \n",
      "Label_3: IoU Scores Train\n",
      "Class00: 0.2249 Class01: 0.4030 Class02: 0.1359 Class03: 0.3588 Class04: 0.2076 Class05: 0.0036 Class06: 0.0458 \n",
      "Class07: 0.1063 Class08: 0.3693 Class09: 0.3889 Class10: 0.0003 Class11: 0.0002 Class12: 0.4163 \n",
      "Train seg Loss: 0.238675635739945 Train dom Loss: 4.573095874949059\n",
      "Train Loss: 0.23913294506137786\n",
      "Train mIoU: 0.30740709325365595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:12<00:00,  2.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--IoU Scores Fish val--\n",
      "Class00: 0.9894 Class01: 0.7487 Class02: 0.0463 Class03: 0.4858 Class04: 0.0476 Class05: 0.0034 Class06: 0.0262 \n",
      "Class07: 0.1866 Class08: 0.6923 Class09: 0.8874 Class10: 0.0000 Class11: 0.0000 Class12: 0.5280 \n",
      "Epoch6\n",
      "Valid Seg Loss: 0.37500882347424824 Valid dom Loss: 488.4124776204427\n",
      "Valid Loss: 0.4238500709335009\n",
      "Valid mIoU: 0.3570656291563825\n",
      "___________________________________________________________________________________________\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 7: 100%|██████████| 138/138 [05:50<00:00,  2.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label_0: IoU Scores Train\n",
      "Class00: 0.9578 Class01: 0.8754 Class02: 0.4584 Class03: 0.7838 Class04: 0.6170 Class05: 0.1286 Class06: 0.3172 \n",
      "Class07: 0.4623 Class08: 0.7906 Class09: 0.9537 Class10: 0.0048 Class11: 0.0023 Class12: 0.8317 \n",
      "Label_1: IoU Scores Train\n",
      "Class00: 0.2876 Class01: 0.5581 Class02: 0.2490 Class03: 0.4791 Class04: 0.3409 Class05: 0.0141 Class06: 0.0821 \n",
      "Class07: 0.1900 Class08: 0.4870 Class09: 0.5312 Class10: 0.0010 Class11: 0.0081 Class12: 0.5524 \n",
      "Label_2: IoU Scores Train\n",
      "Class00: 0.2439 Class01: 0.4602 Class02: 0.1874 Class03: 0.4073 Class04: 0.2677 Class05: 0.0097 Class06: 0.0664 \n",
      "Class07: 0.1404 Class08: 0.4140 Class09: 0.4389 Class10: 0.0006 Class11: 0.0074 Class12: 0.4694 \n",
      "Label_3: IoU Scores Train\n",
      "Class00: 0.2246 Class01: 0.4044 Class02: 0.1544 Class03: 0.3651 Class04: 0.2296 Class05: 0.0086 Class06: 0.0575 \n",
      "Class07: 0.1212 Class08: 0.3750 Class09: 0.3897 Class10: 0.0008 Class11: 0.0054 Class12: 0.4243 \n",
      "Train seg Loss: 0.2040417910809966 Train dom Loss: 0.0021730203559433173\n",
      "Train Loss: 0.20404200838959735\n",
      "Train mIoU: 0.3238057872251018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:12<00:00,  2.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--IoU Scores Fish val--\n",
      "Class00: 0.9923 Class01: 0.8240 Class02: 0.1502 Class03: 0.5898 Class04: 0.2135 Class05: 0.0160 Class06: 0.0400 \n",
      "Class07: 0.1500 Class08: 0.7057 Class09: 0.8830 Class10: 0.0000 Class11: 0.0001 Class12: 0.7093 \n",
      "Epoch7\n",
      "Valid Seg Loss: 0.2543222347895304 Valid dom Loss: 531.2226440429688\n",
      "Valid Loss: 0.30744449943304064\n",
      "Valid mIoU: 0.4057010178834121\n",
      "___________________________________________________________________________________________\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 8: 100%|██████████| 138/138 [05:50<00:00,  2.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label_0: IoU Scores Train\n",
      "Class00: 0.9589 Class01: 0.8841 Class02: 0.4956 Class03: 0.8080 Class04: 0.6253 Class05: 0.1959 Class06: 0.3453 \n",
      "Class07: 0.5002 Class08: 0.8043 Class09: 0.9562 Class10: 0.0118 Class11: 0.0336 Class12: 0.8510 \n",
      "Label_1: IoU Scores Train\n",
      "Class00: 0.2879 Class01: 0.5602 Class02: 0.2593 Class03: 0.4861 Class04: 0.3386 Class05: 0.0279 Class06: 0.0901 \n",
      "Class07: 0.1987 Class08: 0.4915 Class09: 0.5310 Class10: 0.0037 Class11: 0.0638 Class12: 0.5563 \n",
      "Label_2: IoU Scores Train\n",
      "Class00: 0.2439 Class01: 0.4615 Class02: 0.1957 Class03: 0.4107 Class04: 0.2641 Class05: 0.0196 Class06: 0.0729 \n",
      "Class07: 0.1473 Class08: 0.4171 Class09: 0.4387 Class10: 0.0021 Class11: 0.0575 Class12: 0.4708 \n",
      "Label_3: IoU Scores Train\n",
      "Class00: 0.2246 Class01: 0.4057 Class02: 0.1626 Class03: 0.3687 Class04: 0.2245 Class05: 0.0191 Class06: 0.0627 \n",
      "Class07: 0.1281 Class08: 0.3776 Class09: 0.3898 Class10: 0.0021 Class11: 0.0544 Class12: 0.4245 \n",
      "Train seg Loss: 0.18977987930934498 Train dom Loss: 4.4541436883421744e-10\n",
      "Train Loss: 0.18977987930934498\n",
      "Train mIoU: 0.3348440338030093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:12<00:00,  2.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--IoU Scores Fish val--\n",
      "Class00: 0.9938 Class01: 0.8133 Class02: 0.1359 Class03: 0.6162 Class04: 0.1704 Class05: 0.0237 Class06: 0.0473 \n",
      "Class07: 0.1838 Class08: 0.7298 Class09: 0.9208 Class10: 0.0001 Class11: 0.0007 Class12: 0.7000 \n",
      "Epoch8\n",
      "Valid Seg Loss: 0.25927166591087975 Valid dom Loss: 531.083383178711\n",
      "Valid Loss: 0.3123799999554952\n",
      "Valid mIoU: 0.41044341775442317\n",
      "___________________________________________________________________________________________\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 9: 100%|██████████| 138/138 [05:53<00:00,  2.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label_0: IoU Scores Train\n",
      "Class00: 0.9603 Class01: 0.8937 Class02: 0.5240 Class03: 0.8192 Class04: 0.6701 Class05: 0.2353 Class06: 0.3705 \n",
      "Class07: 0.5051 Class08: 0.8119 Class09: 0.9569 Class10: 0.0263 Class11: 0.0809 Class12: 0.8691 \n",
      "Label_1: IoU Scores Train\n",
      "Class00: 0.2880 Class01: 0.5615 Class02: 0.2682 Class03: 0.4904 Class04: 0.3548 Class05: 0.0366 Class06: 0.0951 \n",
      "Class07: 0.1994 Class08: 0.4948 Class09: 0.5310 Class10: 0.0092 Class11: 0.1068 Class12: 0.5615 \n",
      "Label_2: IoU Scores Train\n",
      "Class00: 0.2440 Class01: 0.4627 Class02: 0.2046 Class03: 0.4152 Class04: 0.2795 Class05: 0.0255 Class06: 0.0773 \n",
      "Class07: 0.1508 Class08: 0.4204 Class09: 0.4386 Class10: 0.0068 Class11: 0.0999 Class12: 0.4754 \n",
      "Label_3: IoU Scores Train\n",
      "Class00: 0.2247 Class01: 0.4062 Class02: 0.1692 Class03: 0.3725 Class04: 0.2387 Class05: 0.0257 Class06: 0.0659 \n",
      "Class07: 0.1334 Class08: 0.3808 Class09: 0.3896 Class10: 0.0059 Class11: 0.0934 Class12: 0.4282 \n",
      "Train seg Loss: 0.17828117066697366 Train dom Loss: 0.0049352748319432494\n",
      "Train Loss: 0.1782816642139485\n",
      "Train mIoU: 0.3453022501930885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:12<00:00,  2.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--IoU Scores Fish val--\n",
      "Class00: 0.9915 Class01: 0.7083 Class02: 0.0822 Class03: 0.5794 Class04: 0.1286 Class05: 0.0541 Class06: 0.0255 \n",
      "Class07: 0.1624 Class08: 0.5842 Class09: 0.8857 Class10: 0.0017 Class11: 0.0000 Class12: 0.6073 \n",
      "Epoch9\n",
      "Valid Seg Loss: 0.3279515201846758 Valid dom Loss: 536.3352478027343\n",
      "Valid Loss: 0.38158504366874696\n",
      "Valid mIoU: 0.3700765979186359\n",
      "___________________________________________________________________________________________\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 10: 100%|██████████| 138/138 [05:41<00:00,  2.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label_0: IoU Scores Train\n",
      "Class00: 0.9581 Class01: 0.8841 Class02: 0.4995 Class03: 0.8031 Class04: 0.6451 Class05: 0.2390 Class06: 0.3625 \n",
      "Class07: 0.5073 Class08: 0.7997 Class09: 0.9560 Class10: 0.0233 Class11: 0.0766 Class12: 0.8399 \n",
      "Label_1: IoU Scores Train\n",
      "Class00: 0.2880 Class01: 0.5615 Class02: 0.2629 Class03: 0.4864 Class04: 0.3448 Class05: 0.0396 Class06: 0.0998 \n",
      "Class07: 0.1993 Class08: 0.4903 Class09: 0.5309 Class10: 0.0108 Class11: 0.1130 Class12: 0.5599 \n",
      "Label_2: IoU Scores Train\n",
      "Class00: 0.2441 Class01: 0.4627 Class02: 0.1971 Class03: 0.4108 Class04: 0.2678 Class05: 0.0286 Class06: 0.0821 \n",
      "Class07: 0.1497 Class08: 0.4157 Class09: 0.4385 Class10: 0.0083 Class11: 0.1117 Class12: 0.4754 \n",
      "Label_3: IoU Scores Train\n",
      "Class00: 0.2248 Class01: 0.4066 Class02: 0.1660 Class03: 0.3687 Class04: 0.2269 Class05: 0.0272 Class06: 0.0715 \n",
      "Class07: 0.1313 Class08: 0.3768 Class09: 0.3893 Class10: 0.0074 Class11: 0.1001 Class12: 0.4288 \n",
      "Train seg Loss: 0.18660489661430102 Train dom Loss: 3.3338392272262486e-09\n",
      "Train Loss: 0.18660489661430102\n",
      "Train mIoU: 0.3422932915359793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:11<00:00,  2.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--IoU Scores Fish val--\n",
      "Class00: 0.9929 Class01: 0.8183 Class02: 0.1546 Class03: 0.6108 Class04: 0.2022 Class05: 0.0318 Class06: 0.0478 \n",
      "Class07: 0.1929 Class08: 0.7033 Class09: 0.8847 Class10: 0.0009 Class11: 0.0010 Class12: 0.7172 \n",
      "Epoch10\n",
      "Valid Seg Loss: 0.2591992984215418 Valid dom Loss: 529.7867970784505\n",
      "Valid Loss: 0.3121779774626096\n",
      "Valid mIoU: 0.4121857075415901\n",
      "___________________________________________________________________________________________\n",
      "\n",
      "Hyperparamerters\n",
      "LR = 0.001 | EP = 10, BATCH_SIZE = 16, N_CLASSES = 13, init_alpha = 0.0001, N_LABELS = 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "#torch.cuda.empty_cache()\n",
    "import wandb\n",
    "\n",
    "\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"11_08_4d_res34_0.0001\",\n",
    "    \n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": LR,\n",
    "    \"architecture\": \"CNN\",\n",
    "    \"dataset\": \"Samsung\",\n",
    "    \"epochs\": EP,\n",
    "    }\n",
    ")\n",
    "\n",
    "for epoch in range(EP):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    seg_losses = 0\n",
    "    domain_losses = 0\n",
    "    if epoch < 10:  # 전체 EP 40%\n",
    "        alpha = 0.0001\n",
    "    elif epoch <25: # 전체 EP 30%\n",
    "        alpha = 0.0001\n",
    "    else:          # 전체 EP 30%\n",
    "        alpha = 0.0001\n",
    "    train_class_ious = [[],[],[],[]]\n",
    "    for source_images, source_masks in tqdm(source_dataloader,desc=f\"Epoch: {epoch+1}\"):\n",
    "        random.shuffle(Label)\n",
    "        for l in range(N_LABELS):\n",
    "            label = Label[l]\n",
    "            source_image, source_mask = apply_fisheye_distortion(source_images, source_masks, label)\n",
    "            source_image = source_image.float().to(device)\n",
    "            source_mask = source_mask.long().to(device)\n",
    "            source_outputs = model(source_image)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            target_loss, seg_loss, domain_loss = loss_fn(source_outputs, source_mask, label, alpha = alpha)\n",
    "            epoch_loss += target_loss.item()\n",
    "            seg_losses +=  seg_loss.item()\n",
    "            domain_losses += domain_loss.item()\n",
    "            target_loss.backward()\n",
    "            optimizer.step()\n",
    "            # miou측정\n",
    "            source_outputs = torch.softmax(source_outputs[0], dim=1).cpu()\n",
    "            source_outputs = torch.argmax(source_outputs, dim=1).numpy()\n",
    "            for class_id in range(N_CLASSES):\n",
    "                iou = calculate_iou_per_class(np.array(source_masks.cpu()), np.array(source_outputs), class_id)\n",
    "                train_class_ious[label].append(iou)\n",
    "            #print(train_class_ious[0])\n",
    "    \n",
    "    for i in range(N_LABELS):\n",
    "        buff = np.array(train_class_ious[i]).reshape(-1, N_CLASSES)\n",
    "        buff =np.mean(buff, axis=0)\n",
    "        print(f\"\\nLabel_{i}: IoU Scores Train\") \n",
    "        for class_id, iou in enumerate(buff):\n",
    "            print(f'Class{class_id:02d}: {iou:.4f}', end=\" \")\n",
    "            if (class_id+1) % 7 == 0:\n",
    "                print()   \n",
    "    print()    \n",
    "    print(f\"Train seg Loss: {(seg_losses/(N_LABELS*len(source_dataloader)))}\", f\"Train dom Loss: {(domain_losses/(N_LABELS*len(source_dataloader)))}\")\n",
    "    print(f\"Train Loss: {(epoch_loss/(N_LABELS*len(source_dataloader)))}\")\n",
    "    print(f\"Train mIoU: {np.mean(train_class_ious)}\" )\n",
    "    ################################################################\n",
    "    # 클래스별 IoU를 누적할 리스트 초기화\n",
    "    val_class_ious = []\n",
    "    fish_val_class_ious = []\n",
    "    val_epoch_loss = 0\n",
    "    val_seg_loss = 0\n",
    "    val_domain_loss = 0\n",
    "    # valid\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "\n",
    "        for target_images, target_masks in tqdm(val_target_dataloader):\n",
    "            label = 2.5\n",
    "            target_images, target_masks = apply_fisheye_distortion(target_images, target_masks, label)\n",
    "            target_images = target_images.float().to(device)\n",
    "            target_masks = target_masks.long().to(device)\n",
    "\n",
    "            target_outputs = model(target_images)\n",
    "\n",
    "            target_loss, target_seg_loss, target_domain_loss = loss_fn(target_outputs, target_masks, 0, alpha = alpha)\n",
    "\n",
    "            val_seg_loss +=  target_seg_loss.item()\n",
    "            val_domain_loss += target_domain_loss.item()\n",
    "            \n",
    "            loss = target_loss\n",
    "\n",
    "            val_epoch_loss += loss.item()\n",
    "\n",
    "            # train 클래스별 IoU 계산\n",
    "            target_outputs = torch.softmax(target_outputs[0], dim=1).cpu()\n",
    "            target_outputs = torch.argmax(target_outputs, dim=1).numpy()\n",
    "\n",
    "            for class_id in range(N_CLASSES):\n",
    "                iou = calculate_iou_per_class(np.array(target_masks.cpu()), np.array(target_outputs), class_id)\n",
    "                fish_val_class_ious.append(iou)\n",
    "\n",
    "    fish_val_class_ious = np.array(fish_val_class_ious).reshape(-1, N_CLASSES)\n",
    "    fish_val_class_ious = np.mean(fish_val_class_ious, axis=0)\n",
    "    print()\n",
    "    print(\"--IoU Scores Fish val--\")\n",
    "    for class_id, iou in enumerate(fish_val_class_ious):\n",
    "        print(f'Class{class_id:02d}: {iou:.4f}', end=\" \")\n",
    "        if (class_id+1) % 7 == 0:\n",
    "            print()\n",
    "\n",
    "    # mIoU 계산\n",
    "    fish_val_mIoU = np.mean(fish_val_class_ious)\n",
    "\n",
    "    # 에폭마다 결과 출력 \n",
    "    print(f\"\\nEpoch{epoch+1}\")\n",
    "    print(f\"Valid Seg Loss: {(val_seg_loss/len(val_target_dataloader))}\",f\"Valid dom Loss: {(val_domain_loss/len(val_target_dataloader))}\")\n",
    "    print(f\"Valid Loss: {(val_epoch_loss/len(val_target_dataloader))}\")\n",
    "    print(f\"Valid mIoU: {fish_val_mIoU}\" )\n",
    "    print(\"___________________________________________________________________________________________\\n\")\n",
    "\n",
    "\n",
    "    # log metrics to wandb\n",
    "    wandb.log({\"train score\": np.mean(train_class_ious)})\n",
    "    wandb.log({\"val score\": fish_val_mIoU})\n",
    "    wandb.log({\"train loss\": (epoch_loss/(N_LABELS*len(source_dataloader)))})\n",
    "    wandb.log({\"val loss\": (val_epoch_loss/len(val_target_dataloader))})\n",
    "    \n",
    "    \n",
    "# [optional] finish the wandb run, necessary in notebooks\n",
    "wandb.finish()\n",
    "\n",
    "print(\"Hyperparamerters\")\n",
    "print(f\"LR = {LR} | EP = {EP}, BATCH_SIZE = {BATCH_SIZE}, N_CLASSES = {N_CLASSES}, init_alpha = {ALPHA}, N_LABELS = {N_LABELS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './data/resnet34_1107_d003_10ep_rd.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "byungwan_resn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
