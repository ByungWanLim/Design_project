{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/MMI24limbyungwan/.conda/envs/byungwan_resn/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "#from torch.optim.lr_scheduler import _LRScheduler\n",
    "\n",
    "from tqdm import tqdm\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# GPU 사용이 가능할 경우, GPU를 사용할 수 있게 함.'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = torch.device(device)\n",
    "print(device)\n",
    "\n",
    "print(os.environ.get('CUDA_VISIBLE_DEVICES'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RLE 인코딩 함수\n",
    "def rle_encode(mask):\n",
    "    pixels = mask.flatten()\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)\n",
    "\n",
    "# 클래스별 IoU를 계산하기 위한 함수\n",
    "def calculate_iou_per_class(y_true, y_pred, class_id):\n",
    "    intersection = np.sum((y_true == class_id) & (y_pred == class_id))\n",
    "    union = np.sum((y_true == class_id) | (y_pred == class_id))\n",
    "    iou = intersection / union if union > 0 else 0\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "\n",
    "def apply_fisheye_distortion(images, masks, label):\n",
    "    # 이미지 크기 가져오기\n",
    "    batch, channel, height, width = images.shape\n",
    "\n",
    "    # 카메라 매트릭스 생성\n",
    "    focal_length = width / 4\n",
    "    center_x = width / 2\n",
    "    center_y = height / 2\n",
    "    camera_matrix = np.array([[focal_length, 0, center_x],\n",
    "                              [0, focal_length, center_y],\n",
    "                              [0, 0, 1]], dtype=np.float32)\n",
    "\n",
    "    # 왜곡 계수 생성\n",
    "    # dist_num = 0\n",
    "    # if label == 1:\n",
    "    #     dist_num = random.randint(1,3)\n",
    "    # elif label == 2.5:\n",
    "    #     dist_num = 2.5\n",
    "    dist_num = label+1\n",
    "    dist_coeffs = np.array([0, 0.03 * dist_num, 0, 0], dtype=np.float32)\n",
    "\n",
    "    # 왜곡 보정\n",
    "    undistorted_images = []\n",
    "    undistorted_masks = []\n",
    "\n",
    "    for i in range(batch):\n",
    "        image = images[i].permute(1, 2, 0).cpu().numpy()  # 텐서를 NumPy 배열로 변환\n",
    "        mask = masks[i].cpu().numpy()\n",
    "        undistorted_image = cv2.undistort(image, camera_matrix, dist_coeffs)\n",
    "        undistorted_mask = cv2.undistort(mask, camera_matrix, dist_coeffs)\n",
    "        undistorted_mask = np.round(undistorted_mask).astype(np.uint8)\n",
    "        undistorted_mask[undistorted_mask > 12] = 12\n",
    "\n",
    "        # 다시 텐서로 변환\n",
    "        undistorted_image = torch.from_numpy(undistorted_image).permute(2, 0, 1).float().to(device)\n",
    "        undistorted_mask = torch.from_numpy(undistorted_mask).long().to(device)\n",
    "\n",
    "        undistorted_images.append(undistorted_image)\n",
    "        undistorted_masks.append(undistorted_mask)\n",
    "\n",
    "    undistorted_images = torch.stack(undistorted_images, dim=0)\n",
    "    undistorted_masks = torch.stack(undistorted_masks, dim=0)\n",
    "\n",
    "    return undistorted_images, undistorted_masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# # 이미지 데이터 (batch, channel, height, width)\n",
    "# image_data = torch.randn(4, 3, 256, 256)\n",
    "\n",
    "# # 이미지 차원 변경 (batch, height, width, channel)\n",
    "# image_data_permuted = image_data.permute(0, 2, 3, 1)\n",
    "\n",
    "# # 이미지 크기 확인\n",
    "# print(image_data_permuted.shape)\n",
    "\n",
    "# # 4개의 이미지로 나누기\n",
    "# images = image_data_permuted.split(1, dim=0)\n",
    "# # 또는 images = torch.split(image_data_permuted, 1, dim=0)\n",
    "# undistorted_images = []\n",
    "# # 4개 이미지의 크기 확인\n",
    "# for i, image in enumerate(images):\n",
    "#     print(f\"Image {i + 1} shape: {image.shape}\")\n",
    "#     undistorted_images.append(images[i].squeeze())\n",
    "\n",
    "# undistorted_images2 = torch.stack(undistorted_images, dim=0)\n",
    "# undistorted_images3 = undistorted_images2.permute(0,3,1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None, infer=False):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "        self.infer = infer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #directory_path = \"/mnt/nas27/Dataset/Samsung_DM\"\n",
    "        directory_path = './data/224'\n",
    "        img_path = self.data.iloc[idx, 1]\n",
    "        img_path = os.path.join(directory_path, img_path)\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        #image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        if self.infer:\n",
    "            if self.transform:\n",
    "                image = self.transform(image=image)['image']\n",
    "            return image\n",
    "        \n",
    "        mask_path = self.data.iloc[idx, 2]\n",
    "        mask_path = os.path.join(directory_path, mask_path)\n",
    "        mask = cv2.imread(mask_path)\n",
    "        #mask = cv2.cvtColor(mask, cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)\n",
    "        mask = np.round(mask).astype(np.uint8)\n",
    "        mask[mask > 12] = 12 #배경을 픽셀값 12로 간주\n",
    "        mask += 1\n",
    "        mask[mask == 13] = 0\n",
    "\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "# class CustomDataset_target(Dataset):\n",
    "#     def __init__(self, csv_file, transform=None, infer=False):\n",
    "#         self.data = pd.read_csv(csv_file)\n",
    "#         self.transform = transform\n",
    "#         self.infer = infer\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         directory_path = \"/mnt/nas27/Dataset/Samsung_DM\"\n",
    "#         img_path = self.data.iloc[idx, 1]\n",
    "#         img_path = os.path.join(directory_path, img_path[2:])\n",
    "#         image = cv2.imread(img_path)\n",
    "#         image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "#         if self.infer:\n",
    "#             if self.transform:\n",
    "#                 image = self.transform(image=image)['image']\n",
    "#             return image\n",
    "\n",
    "\n",
    "#         if self.transform:\n",
    "#             augmented = self.transform(image=image)\n",
    "#             image = augmented['image']\n",
    "            \n",
    "\n",
    "#         return image\n",
    "     \n",
    "\n",
    "transform = A.Compose(\n",
    "    [   \n",
    "        #A.Resize(224, 224),\n",
    "        #A.Resize(128, 128),\n",
    "        A.Normalize(),\n",
    "        \n",
    "        # 변형\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        # A.RandomRotate90(p=0.5),\n",
    "        A.HueSaturationValue(p=0.2),\n",
    "        \n",
    "        ToTensorV2()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 기본 U-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Unet의 기본이 되는 conv블럭\n",
    "# class ConvBlock(nn.Module):\n",
    "#     def __init__(self, in_channels, out_channels):\n",
    "#         super(ConvBlock, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "#         self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "#         self.relu1 = nn.ReLU()\n",
    "        \n",
    "#         self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)  # 여기서 in_channels는 out_channels와 동일해야 합니다.\n",
    "#         self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "#         self.relu2 = nn.ReLU()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.conv1(x)\n",
    "#         x = self.bn1(x)\n",
    "#         x = self.relu1(x)\n",
    "        \n",
    "#         x = self.conv2(x)\n",
    "#         x = self.bn2(x)\n",
    "#         x = self.relu2(x)\n",
    "#         return x\n",
    "\n",
    "# #인코더 블럭\n",
    "# class EncoderBlock(nn.Module):\n",
    "#     def __init__(self, in_channels, out_channels):\n",
    "#         super(EncoderBlock,self).__init__()\n",
    "#         self.convblock1 = ConvBlock(in_channels, out_channels)  # 첫 번째 ConvBlock의 in_channels는 입력 이미지의 채널 수와 일치해야 합니다.\n",
    "#         #self.convblock2 = ConvBlock(out_channels, out_channels)  # 두 번째 ConvBlock의 in_channels는 out_channels와 일치해야 합니다.\n",
    "#         self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "#     def forward(self,x):\n",
    "#         x = self.convblock1(x)\n",
    "#         #x = self.convblock2(x)\n",
    "#         p = self.maxpool(x)\n",
    "#         return x , p\n",
    "# #디코더 블럭\n",
    "# #디코더는 업샘플링 이후 스킵연결과 붙어서 convblock을 통과해야함\n",
    "# #skip보다 작은 x x먼저 업샘플링 32 -> 64 , skip과 결합 6464 \n",
    "# class DecoderBlock(nn.Module):\n",
    "#     def __init__(self, channels):\n",
    "#         super(DecoderBlock,self).__init__()\n",
    "#         self.upsample = nn.ConvTranspose2d(channels*2, channels, kernel_size=4, stride=2, padding=1)#x 업샘플링\n",
    "#         self.convblock1 = ConvBlock(channels*2, channels)#차원감소\n",
    "#         #self.convblock2 = ConvBlock(channels, channels)\n",
    "#     def forward(self,x,skip):\n",
    "#         x = self.upsample(x)\n",
    "#         x = torch.cat([x, skip], dim=1)\n",
    "#         x = self.convblock1(x)\n",
    "#         #x = self.convblock2(x)\n",
    "#         return x\n",
    "\n",
    "# ###########################################\n",
    "# class GradReverse(torch.autograd.Function):\n",
    "#     @staticmethod\n",
    "#     def forward(self, x):\n",
    "#         return x.view_as(x)\n",
    "#     @staticmethod\n",
    "#     def backward(self, grad_output): # 역전파 시에 gradient에 음수를 취함\n",
    "#         return (grad_output * -1)\n",
    "\n",
    "# class domain_classifier(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(domain_classifier, self).__init__()\n",
    "#         self.fc1 = nn.Linear(224*224*64, 10)\n",
    "#         self.fc2 = nn.Linear(10, 4) # source = 0, target = 1 회귀 가정\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = x.view(-1, 224*224*64)\n",
    "#         x = GradReverse.apply(x) # gradient reverse\n",
    "#         x = F.leaky_relu(self.fc1(x))\n",
    "#         x = self.fc2(x)\n",
    "        \n",
    "#         return x\n",
    "\n",
    "# ###########################################\n",
    "\n",
    "\n",
    "# #Unet구조 middle의 xm값의 움직임에 주의\n",
    "# class Unet(nn.Module):\n",
    "#     def __init__(self,n_classes):\n",
    "#         super(Unet,self).__init__()\n",
    "#         self.encoder1 = EncoderBlock(3,64)\n",
    "#         self.encoder2 = EncoderBlock(64,128)\n",
    "#         self.encoder3 = EncoderBlock(128,256)\n",
    "#         self.encoder4 = EncoderBlock(256,512)\n",
    "        \n",
    "#         self.middleconv = ConvBlock(512,1024)\n",
    "        \n",
    "        \n",
    "#         self.decoder4 = DecoderBlock(512)\n",
    "#         self.decoder3 = DecoderBlock(256)\n",
    "#         self.decoder2 = DecoderBlock(128)\n",
    "#         self.decoder1 = DecoderBlock(64)\n",
    "#         self.segmap = nn.Conv2d(64,n_classes, kernel_size=1)\n",
    "        \n",
    "#         self.domain_classifier = domain_classifier()\n",
    "                                        \n",
    "\n",
    "#     def forward(self,x):\n",
    "#         x1,p = self.encoder1(x)#3->64   #P:256,256 x1 :512,512\n",
    "#         x2,p = self.encoder2(p)#64->128 #P:128,128 x2:256,256\n",
    "#         x3,p = self.encoder3(p)#128->256#p:64,64 x3:128,128\n",
    "#         x4,p = self.encoder4(p)#256->512#p:32,32 x4:64,64\n",
    "        \n",
    "#         xm = self.middleconv(p)#512->1024#32,32\n",
    "        \n",
    "#         x = self.decoder4(xm,x4)#뉴런:1024->512->512 #출력tensor:64,64\n",
    "#         x = self.decoder3(x,x3)#뉴런:512->256->256 #출력tensor:128,128\n",
    "#         x = self.decoder2(x,x2)#뉴런:256->128->128 #출력tensor:256,256\n",
    "#         x = self.decoder1(x,x1)#뉴런:128->64->64 #출력tensor:512,512\n",
    "\n",
    "#         x_c = self.segmap(x)\n",
    "#         x_d = self.domain_classifier(x)\n",
    "#         # print(\"x_c\", x_c.shape)\n",
    "#         # print(\"x_d\", x_d.shape)\n",
    "#         return x_c, x_d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class GradReverse(torch.autograd.Function):\n",
    "#     @staticmethod\n",
    "#     def forward(self, x):\n",
    "#         return x.view_as(x)\n",
    "#     @staticmethod\n",
    "#     def backward(self, grad_output): # 역전파 시에 gradient에 음수를 취함\n",
    "#         return grad_output * (-1)\n",
    "\n",
    "# class domain_classifier(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(domain_classifier, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(128, 1, kernel_size=1)\n",
    "#         self.fc1 = nn.Linear(224*224*1, 10)\n",
    "#         self.fc2 = nn.Linear(10, 4) # source = 0, target = 1 회귀 가정\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.conv1(x)\n",
    "#         x = x.view(-1, 224*224*1)\n",
    "#         #print(x.shape)\n",
    "#         x = GradReverse.apply(x) # gradient reverse\n",
    "#         x = F.leaky_relu(self.fc1(x))\n",
    "#         x = self.fc2(x)\n",
    "#         #print(x.shape)\n",
    "#         #return torch.sigmoid(x)\n",
    "#         return x\n",
    "\n",
    "# #Unet의 기본이 되는 conv블럭\n",
    "# class ConvBlock(nn.Module):\n",
    "#     def __init__(self, in_channels, out_channels,kernel_size = 3):\n",
    "#         super(ConvBlock, self).__init__()\n",
    "#         self.kernel_size = kernel_size\n",
    "#         self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=1)\n",
    "#         self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "#         self.relu1 = nn.ReLU()\n",
    "        \n",
    "#         self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size, padding=1)  # 여기서 in_channels는 out_channels와 동일해야 합니다.\n",
    "#         self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "#         self.relu2 = nn.ReLU()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.conv1(x)\n",
    "#         x = self.bn1(x)\n",
    "#         x = self.relu1(x)\n",
    "        \n",
    "#         x = self.conv2(x)\n",
    "#         x = self.bn2(x)\n",
    "#         x = self.relu2(x)\n",
    "#         return x\n",
    "# class IdentityBlock(nn.Module):\n",
    "#     def __init__(self, in_channels, mid_channels, out_channels, stride=1):\n",
    "#         super(IdentityBlock, self).__init__()\n",
    "        \n",
    "#         # 1x1 convolution\n",
    "#         self.conv1 = nn.Conv2d(in_channels, mid_channels, kernel_size=1, stride=stride, bias=False)\n",
    "#         self.bn1 = nn.BatchNorm2d(mid_channels)\n",
    "#         self.relu1 = nn.ReLU()\n",
    "\n",
    "#         # 3x3 convolution\n",
    "#         self.conv2 = nn.Conv2d(mid_channels, mid_channels, kernel_size=3, padding=1, bias=False)\n",
    "#         self.bn2 = nn.BatchNorm2d(mid_channels)\n",
    "#         self.relu2 = nn.ReLU()\n",
    "\n",
    "#         # 1x1 convolution\n",
    "#         self.conv3 = nn.Conv2d(mid_channels, out_channels, kernel_size=1, bias=False)\n",
    "#         self.bn3 = nn.BatchNorm2d(out_channels)\n",
    "#         self.relu3 = nn.ReLU()\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         out = self.conv1(x)\n",
    "#         out = self.bn1(out)\n",
    "#         out = self.relu1(out)\n",
    "        \n",
    "#         out = self.conv2(out)\n",
    "#         out = self.bn2(out)\n",
    "#         out = self.relu2(out)\n",
    "         \n",
    "#         out = self.conv3(out)\n",
    "#         out = self.bn3(out)\n",
    "#         out = self.relu3(out)\n",
    "        \n",
    "#         return out\n",
    "# class HeadBlock(IdentityBlock):\n",
    "#     def __init__(self, in_channels, mid_channels, out_channels, stride=1):\n",
    "#         super(HeadBlock, self).__init__(in_channels, mid_channels, out_channels, stride)\n",
    "        \n",
    "#         self.shortcut = nn.Sequential(\n",
    "#             nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "#             nn.BatchNorm2d(out_channels)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         identity = x\n",
    "#         out = super().forward(x)\n",
    "        \n",
    "#         if identity.size() != out.size():\n",
    "#             identity = F.interpolate(identity, size=out.size()[2:])\n",
    "#         identity = self.shortcut(identity)\n",
    "        \n",
    "#         out += identity\n",
    "#         out = self.relu3(out)\n",
    "        \n",
    "#         return out\n",
    "# #인코더 블럭\n",
    "# class Conv2(nn.Module):\n",
    "#     def __init__(self,in_channels, mid_channels, out_channels):\n",
    "#         super(Conv2,self).__init__() \n",
    "#         self.headblock = HeadBlock(in_channels,mid_channels,out_channels)\n",
    "#         self.identityblock1 = IdentityBlock(out_channels,mid_channels,out_channels)\n",
    "#         self.identityblock2 = IdentityBlock(out_channels,mid_channels,out_channels)\n",
    "#         self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "#     def forward(self,x):\n",
    "#         x = self.headblock(x)\n",
    "#         x = self.identityblock1(x)\n",
    "#         x = self.identityblock2(x)\n",
    "#         p = self.maxpool(x)\n",
    "#         return x , p\n",
    "# class Conv3(nn.Module):\n",
    "#     def __init__(self,in_channels, mid_channels, out_channels):\n",
    "#         super(Conv3,self).__init__() \n",
    "#         self.headblock = HeadBlock(in_channels,mid_channels,out_channels)\n",
    "#         self.identityblock1 = IdentityBlock(out_channels,mid_channels,out_channels)\n",
    "#         self.identityblock2 = IdentityBlock(out_channels,mid_channels,out_channels)\n",
    "#         self.identityblock3 = IdentityBlock(out_channels,mid_channels,out_channels)\n",
    "#         self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "#     def forward(self,x):\n",
    "#         x = self.headblock(x)\n",
    "#         x = self.identityblock1(x)\n",
    "#         x = self.identityblock2(x)\n",
    "#         x = self.identityblock3(x)\n",
    "#         p = self.maxpool(x)\n",
    "#         return x , p\n",
    "# class Conv4(nn.Module):\n",
    "#     def __init__(self,in_channels, mid_channels, out_channels):\n",
    "#         super(Conv4,self).__init__() \n",
    "#         self.headblock = HeadBlock(in_channels,mid_channels,out_channels)\n",
    "#         self.identityblock1 = IdentityBlock(out_channels,mid_channels,out_channels)\n",
    "#         self.identityblock2 = IdentityBlock(out_channels,mid_channels,out_channels)\n",
    "#         self.identityblock3 = IdentityBlock(out_channels,mid_channels,out_channels)\n",
    "#         self.identityblock4 = IdentityBlock(out_channels,mid_channels,out_channels)\n",
    "#         self.identityblock5 = IdentityBlock(out_channels,mid_channels,out_channels)\n",
    "#         self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "#     def forward(self,x):\n",
    "#         x = self.headblock(x)\n",
    "#         x = self.identityblock1(x)\n",
    "#         x = self.identityblock2(x)\n",
    "#         x = self.identityblock3(x)\n",
    "#         x = self.identityblock4(x)\n",
    "#         x = self.identityblock5(x)\n",
    "#         p = self.maxpool(x)\n",
    "#         return x , p\n",
    "# class Conv5(nn.Module):\n",
    "#     def __init__(self,in_channels, mid_channels, out_channels):\n",
    "#         super(Conv5,self).__init__() \n",
    "#         self.headblock = HeadBlock(in_channels,mid_channels,out_channels)\n",
    "#         self.identityblock1 = IdentityBlock(out_channels,mid_channels,out_channels)\n",
    "#         self.identityblock2 = IdentityBlock(out_channels,mid_channels,out_channels)\n",
    "#         self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "#     def forward(self,x):\n",
    "#         x = self.headblock(x)\n",
    "#         x = self.identityblock1(x)\n",
    "#         x = self.identityblock2(x)\n",
    "#         p = self.maxpool(x)\n",
    "#         return x , p\n",
    "# #디코더 블럭\n",
    "# class DecoderBlock(nn.Module):\n",
    "#     def __init__(self, channels):\n",
    "#         super(DecoderBlock, self).__init__()\n",
    "#         self.upsample = nn.ConvTranspose2d(channels*2, channels, kernel_size=4, stride=2, padding=1, output_padding=1) # output_padding 추가\n",
    "#         self.convblock1 = ConvBlock(channels*2, channels)\n",
    "\n",
    "#     def forward(self, x, skip):\n",
    "#         x = self.upsample(x)\n",
    "#         if x.size(2) != skip.size(2) or x.size(3) != skip.size(3):\n",
    "#             x = F.interpolate(x, size=(skip.size(2), skip.size(3)))\n",
    "#         x = torch.cat([x, skip], dim=1)\n",
    "#         x = self.convblock1(x)\n",
    "#         return x\n",
    "\n",
    "# #Unet구조 middle의 xm값의 움직임에 주의\n",
    "# class Resnet50_Unet(nn.Module):\n",
    "#     def __init__(self,n_classes):\n",
    "#         super(Resnet50_Unet,self).__init__()\n",
    "#         self.fconv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2)\n",
    "#         self.fbn1 = nn.BatchNorm2d(64)\n",
    "#         self.frelu1 = nn.ReLU()\n",
    "#         self.fconv2 = nn.Conv2d(64, 128, kernel_size=1, stride=1)\n",
    "#         self.fbn2 = nn.BatchNorm2d(128)\n",
    "#         self.frelu2 = nn.ReLU()\n",
    "#         self.fmaxpooling = nn.MaxPool2d(kernel_size=3,stride=2)\n",
    "        \n",
    "#         self.conv2 = Conv2(128,64,256)\n",
    "#         self.conv3 = Conv3(256,128,512)\n",
    "#         self.conv4 = Conv4(512,256,1024)\n",
    "#         self.conv5 = Conv5(1024,512,2048)\n",
    "        \n",
    "#         self.middleconv = ConvBlock(2048,4096)\n",
    "#         self.dropout = nn.Dropout2d(0.4) #\n",
    "           \n",
    "#         self.decoder5 = DecoderBlock(2048)\n",
    "#         self.decoder4 = DecoderBlock(1024)\n",
    "#         self.decoder3 = DecoderBlock(512)\n",
    "#         self.decoder2 = DecoderBlock(256)\n",
    "#         self.decoder1 = DecoderBlock(128)\n",
    "        \n",
    "#         self.segmap = nn.Conv2d(128,n_classes, kernel_size=1)\n",
    "#         self.domain_classifier = domain_classifier()\n",
    "        \n",
    "#     def forward(self,x):\n",
    "#         x = self.fconv1(x)#3->64\n",
    "#         x = self.fbn1(x)\n",
    "#         x = self.frelu1(x)\n",
    "#         x = self.fconv2(x)\n",
    "#         x = self.fbn2(x)\n",
    "#         x1 = self.frelu2(x)\n",
    "#         p = self.fmaxpooling(x)#첫 conv: x0([8, 64, 109, 109]) p([8, 64, 54, 54])\n",
    "#         x2,p = self.conv2(p)#conv2:  x1:([8, 256, 54, 54]) p([8, 256, 26, 26])\n",
    "#         x3,p = self.conv3(p)#conv3:  x2([8, 512, 26, 26]) p([8, 512, 12, 12])\n",
    "#         x4,p = self.conv4(p)#conv4:  x3([8, 1024, 12, 12]) p([8, 1024, 5, 5])\n",
    "#         x5,p = self.conv5(p)#conv5:  x4([8, 2048, 5, 5]) p([8, 2048, 2, 2])\n",
    "        \n",
    "#         xm = self.middleconv(p)#xm([8, 4096, 2, 2])\n",
    "#         xm = self.dropout(xm)\n",
    "        \n",
    "#         x = self.decoder5(xm,x5)#뉴런:2048*2->2048 1\n",
    "#         x = self.decoder4(x,x4)#뉴런:1024*2->1024 \n",
    "#         x = self.decoder3(x,x3) #14\n",
    "#         x = self.decoder2(x,x2)#28\n",
    "#         x = self.decoder1(x,x1)#55\n",
    "        \n",
    "#         x = F.interpolate(x, size=(224, 224))\n",
    "#         x_c = self.segmap(x)\n",
    "#         x_d = self.domain_classifier(x)\n",
    "#         #print(x.shape)\n",
    "#         return x_c,x_d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resnet34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradReverse(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(self, x):\n",
    "        return x.view_as(x)\n",
    "    @staticmethod\n",
    "    def backward(self, grad_output): # 역전파 시에 gradient에 음수를 취함\n",
    "        return grad_output * (-1)\n",
    "\n",
    "class domain_classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(domain_classifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(224*224*64, 50)\n",
    "        self.fc2 = nn.Linear(50, 3) # source = 0, target = 1 회귀 가정\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 224*224*64)\n",
    "        x = GradReverse.apply(x) # gradient reverse\n",
    "        x = F.leaky_relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        #return torch.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "class domain_linear(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(domain_linear, self).__init__()\n",
    "        self.fc1 = nn.Linear(224*224*64, 10)\n",
    "        self.fc2 = nn.Linear(10, 1) # source = 0, target = 1 회귀 가정\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 224*224*64)\n",
    "        x = GradReverse.apply(x) # gradient reverse\n",
    "        x = F.leaky_relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return torch.sigmoid(x)\n",
    "        #return x\n",
    "class IdentityBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(IdentityBlock, self).__init__()\n",
    "        \n",
    "        # 3x3 convolution\n",
    "        self.conv1 = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1, stride=stride, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(in_channels)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        # 3x3 convolution\n",
    "        self.conv2 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        # Skip connection\n",
    "        self.skip = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.skip = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        \n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu1(out)\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        \n",
    "        # Adding the skip connection\n",
    "        out += self.skip(identity)\n",
    "        out = self.relu2(out)\n",
    "        \n",
    "        return out\n",
    "#인코더 블럭\n",
    "class Conv2(nn.Module):\n",
    "    def __init__(self,in_channels, out_channels):\n",
    "        super(Conv2,self).__init__() \n",
    "        self.identityblock1 = IdentityBlock(in_channels,in_channels)\n",
    "        self.identityblock2 = IdentityBlock(in_channels,in_channels)\n",
    "        self.identityblock3 = IdentityBlock(in_channels,out_channels)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2,padding=1)\n",
    "    def forward(self,x):\n",
    "        x = self.identityblock1(x)\n",
    "        x = self.identityblock2(x)\n",
    "        x = self.identityblock3(x)\n",
    "        p = self.maxpool(x)\n",
    "        \n",
    "        return x , p\n",
    "class Conv3(nn.Module):\n",
    "    def __init__(self,in_channels, out_channels):\n",
    "        super(Conv3,self).__init__()         \n",
    "        self.identityblock1 = IdentityBlock(in_channels,in_channels)\n",
    "        self.identityblock2 = IdentityBlock(in_channels,in_channels)\n",
    "        self.identityblock3 = IdentityBlock(in_channels,in_channels)\n",
    "        self.identityblock4 = IdentityBlock(in_channels,out_channels)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2,padding=1)\n",
    "    def forward(self,x):\n",
    "        x = self.identityblock1(x)\n",
    "        x = self.identityblock2(x)\n",
    "        x = self.identityblock3(x)\n",
    "        x = self.identityblock4(x)\n",
    "        p = self.maxpool(x)\n",
    "        \n",
    "        return x , p\n",
    "class Conv4(nn.Module):\n",
    "    def __init__(self,in_channels, out_channels):\n",
    "        super(Conv4,self).__init__()         \n",
    "        self.identityblock1 = IdentityBlock(in_channels,in_channels)\n",
    "        self.identityblock2 = IdentityBlock(in_channels,in_channels)\n",
    "        self.identityblock3 = IdentityBlock(in_channels,in_channels)\n",
    "        self.identityblock4 = IdentityBlock(in_channels,in_channels)\n",
    "        self.identityblock5 = IdentityBlock(in_channels,in_channels)\n",
    "        self.identityblock6 = IdentityBlock(in_channels,out_channels)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2,padding=1)\n",
    "    def forward(self,x):\n",
    "        x = self.identityblock1(x)\n",
    "        x = self.identityblock2(x)\n",
    "        x = self.identityblock3(x)\n",
    "        x = self.identityblock4(x)\n",
    "        x = self.identityblock5(x)\n",
    "        x = self.identityblock6(x)\n",
    "        p = self.maxpool(x)\n",
    "        \n",
    "        return x , p\n",
    "class Conv5(nn.Module):\n",
    "    def __init__(self,in_channels, out_channels):\n",
    "        super(Conv5,self).__init__() \n",
    "        self.identityblock1 = IdentityBlock(in_channels,in_channels)\n",
    "        self.identityblock2 = IdentityBlock(in_channels,in_channels)\n",
    "        self.identityblock3 = IdentityBlock(in_channels,out_channels)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2,padding=1)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.identityblock1(x)\n",
    "        x = self.identityblock2(x)\n",
    "        x = self.identityblock3(x)\n",
    "        p = self.maxpool(x)\n",
    "        \n",
    "        return x , p\n",
    "#디코더 블럭\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.upsample = nn.ConvTranspose2d(channels*2, channels, kernel_size=4, stride=2, padding=1) # output_padding 추가\n",
    "        self.convblock1 = IdentityBlock(channels*2, channels)\n",
    "\n",
    "    def forward(self, x, skip):\n",
    "        x = self.upsample(x)\n",
    "        if x.size(2) != skip.size(2) or x.size(3) != skip.size(3):\n",
    "            x = F.interpolate(x, size=(skip.size(2), skip.size(3)))\n",
    "        x = torch.cat([x, skip], dim=1)\n",
    "        x = self.convblock1(x)\n",
    "        #print(\"x\",x.shape,\"skip: \",skip.shape)\n",
    "        return x\n",
    "\n",
    "#Unet구조 middle의 xm값의 움직임에 주의\n",
    "class Resnet34_Unet(nn.Module):\n",
    "    def __init__(self,n_classes):\n",
    "        super(Resnet34_Unet,self).__init__()\n",
    "        self.fconv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3)\n",
    "        self.fbn1 = nn.BatchNorm2d(64)\n",
    "        self.frelu1 = nn.ReLU()\n",
    "        self.fmaxpooling = nn.MaxPool2d(kernel_size=3,stride=2,padding=1)\n",
    "        \n",
    "        self.conv2 = Conv2(64,128)\n",
    "        self.conv3 = Conv3(128,256)\n",
    "        self.conv4 = Conv4(256,512)\n",
    "        self.conv5 = Conv5(512,1024)\n",
    "        \n",
    "        self.middleconv = IdentityBlock(1024,2048)\n",
    "        self.dropout = nn.Dropout2d(0.1) #\n",
    "           \n",
    "        self.decoder5 = DecoderBlock(1024)\n",
    "        self.decoder4 = DecoderBlock(512)\n",
    "        self.decoder3 = DecoderBlock(256)\n",
    "        self.decoder2 = DecoderBlock(128)\n",
    "        self.decoder1 = DecoderBlock(64)\n",
    "        self.transpose = nn.ConvTranspose2d(64, 64, kernel_size=4, stride=2, padding=1) # output_padding 추가\n",
    "        \n",
    "        self.segmap = nn.Conv2d(64,n_classes, kernel_size=1)\n",
    "        self.domain_classifier = domain_classifier()\n",
    "        self.domain_linear = domain_linear()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.fconv1(x)#3->64\n",
    "        x0 = self.fbn1(x)\n",
    "        x1 = self.frelu1(x)\n",
    "        p = self.fmaxpooling(x1)#첫 conv: x0([8, 64, 109, 109]) p([8, 64, 54, 54])\n",
    "        #print(\"conv1: \",x1.shape, \"maxpooling: \",p.shape)\n",
    "        x2,p = self.conv2(p)\n",
    "        #print(\"conv2: \",x2.shape, \"maxpooling: \",p.shape)\n",
    "        x3,p = self.conv3(p)\n",
    "        #print(\"conv3: \",x3.shape, \"maxpooling: \",p.shape)\n",
    "        x4,p = self.conv4(p)\n",
    "        #print(\"conv4: \",x4.shape, \"maxpooling: \",p.shape)\n",
    "        x5,p = self.conv5(p)\n",
    "        #print(\"conv5: \",x5.shape, \"maxpooling: \",p.shape)\n",
    "        \n",
    "        xm = self.middleconv(p)#xm([8, 4096, 2, 2])\n",
    "        #print(\"xm: \",xm.shape, \"maxpooling: \",p.shape)\n",
    "        xm = self.dropout(xm)\n",
    "        \n",
    "        x = self.decoder5(xm,x5)#뉴런:2048*2->2048 1\n",
    "        x = self.decoder4(x,x4)#뉴런:1024*2->1024 \n",
    "        x = self.decoder3(x,x3) #14\n",
    "        x = self.decoder2(x,x2)#28\n",
    "        x = self.decoder1(x,x1)#55\n",
    "        x = self.transpose(x)\n",
    "        \n",
    "        #print(x.shape)\n",
    "        #x = F.interpolate(x, size=(224, 224))\n",
    "        x_c = self.segmap(x)\n",
    "        #x_d = self.domain_linear(x)\n",
    "        x_d = self.domain_classifier(x)\n",
    "        \n",
    "        \n",
    "        return x_c,x_d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resnet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class GradReverse(torch.autograd.Function):\n",
    "#     @staticmethod\n",
    "#     def forward(self, x):\n",
    "#         return x.view_as(x)\n",
    "#     @staticmethod\n",
    "#     def backward(self, grad_output): # 역전파 시에 gradient에 음수를 취함\n",
    "#         return grad_output * (-1)\n",
    "\n",
    "# class domain_classifier(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(domain_classifier, self).__init__()\n",
    "#         self.fc1 = nn.Linear(224*224*64, 10)\n",
    "#         self.fc2 = nn.Linear(10, 4) # source = 0, target = 1 회귀 가정\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = x.view(-1, 224*224*64)\n",
    "#         x = GradReverse.apply(x) # gradient reverse\n",
    "#         x = F.leaky_relu(self.fc1(x))\n",
    "#         x = self.fc2(x)\n",
    "        \n",
    "#         #return torch.sigmoid(x)\n",
    "#         return x\n",
    "\n",
    "# class IdentityBlock(nn.Module):\n",
    "#     def __init__(self, in_channels, out_channels, stride=1):\n",
    "#         super(IdentityBlock, self).__init__()\n",
    "        \n",
    "#         # 3x3 convolution\n",
    "#         self.conv1 = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1, stride=stride, bias=False)\n",
    "#         self.bn1 = nn.BatchNorm2d(in_channels)\n",
    "#         self.relu1 = nn.ReLU()\n",
    "        \n",
    "#         # 3x3 convolution\n",
    "#         self.conv2 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False)\n",
    "#         self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "#         self.relu2 = nn.ReLU()\n",
    "        \n",
    "#         # Skip connection\n",
    "#         self.skip = nn.Sequential()\n",
    "#         if stride != 1 or in_channels != out_channels:\n",
    "#             self.skip = nn.Sequential(\n",
    "#                 nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "#                 nn.BatchNorm2d(out_channels)\n",
    "#             )\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         identity = x\n",
    "        \n",
    "#         out = self.conv1(x)\n",
    "#         out = self.bn1(out)\n",
    "#         out = self.relu1(out)\n",
    "        \n",
    "#         out = self.conv2(out)\n",
    "#         out = self.bn2(out)\n",
    "        \n",
    "#         # Adding the skip connection\n",
    "#         out += self.skip(identity)\n",
    "#         out = self.relu2(out)\n",
    "        \n",
    "#         return out\n",
    "\n",
    "\n",
    "# #인코더 블럭\n",
    "# class Conv2(nn.Module):\n",
    "#     def __init__(self,in_channels, out_channels):\n",
    "#         super(Conv2,self).__init__() \n",
    "#         self.identityblock1 = IdentityBlock(in_channels,in_channels)\n",
    "#         self.identityblock2 = IdentityBlock(in_channels,out_channels)\n",
    "#         self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2,padding=1)\n",
    "#     def forward(self,x):\n",
    "#         x = self.identityblock1(x)\n",
    "#         x = self.identityblock2(x)\n",
    "#         p = self.maxpool(x)\n",
    "        \n",
    "#         return x , p\n",
    "# class Conv3(nn.Module):\n",
    "#     def __init__(self,in_channels, out_channels):\n",
    "#         super(Conv3,self).__init__()         \n",
    "#         self.identityblock1 = IdentityBlock(in_channels,in_channels)\n",
    "#         self.identityblock2 = IdentityBlock(in_channels,out_channels)\n",
    "#         self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2,padding=1)\n",
    "#     def forward(self,x):\n",
    "#         x = self.identityblock1(x)\n",
    "#         x = self.identityblock2(x)\n",
    "#         p = self.maxpool(x)\n",
    "        \n",
    "#         return x , p\n",
    "# class Conv4(nn.Module):\n",
    "#     def __init__(self,in_channels, out_channels):\n",
    "#         super(Conv4,self).__init__()         \n",
    "#         self.identityblock1 = IdentityBlock(in_channels,in_channels)\n",
    "#         self.identityblock2 = IdentityBlock(in_channels,out_channels)\n",
    "#         self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2,padding=1)\n",
    "#     def forward(self,x):\n",
    "#         x = self.identityblock1(x)\n",
    "#         x = self.identityblock2(x)\n",
    "#         p = self.maxpool(x)\n",
    "        \n",
    "#         return x , p\n",
    "# class Conv5(nn.Module):\n",
    "#     def __init__(self,in_channels, out_channels):\n",
    "#         super(Conv5,self).__init__() \n",
    "#         self.identityblock1 = IdentityBlock(in_channels,in_channels)\n",
    "#         self.identityblock2 = IdentityBlock(in_channels,out_channels)\n",
    "#         self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2,padding=1)\n",
    "        \n",
    "#     def forward(self,x):\n",
    "#         x = self.identityblock1(x)\n",
    "#         x = self.identityblock2(x)\n",
    "#         p = self.maxpool(x)\n",
    "        \n",
    "#         return x , p\n",
    "# #디코더 블럭\n",
    "# class DecoderBlock(nn.Module):\n",
    "#     def __init__(self, channels):\n",
    "#         super(DecoderBlock, self).__init__()\n",
    "#         self.upsample = nn.ConvTranspose2d(channels*2, channels, kernel_size=4, stride=2, padding=1) # output_padding 추가\n",
    "#         self.convblock1 = IdentityBlock(channels*2, channels)\n",
    "\n",
    "#     def forward(self, x, skip):\n",
    "#         x = self.upsample(x)\n",
    "#         if x.size(2) != skip.size(2) or x.size(3) != skip.size(3):\n",
    "#             x = F.interpolate(x, size=(skip.size(2), skip.size(3)))\n",
    "#         x = torch.cat([x, skip], dim=1)\n",
    "#         x = self.convblock1(x)\n",
    "#         #print(\"x\",x.shape,\"skip: \",skip.shape)\n",
    "#         return x\n",
    "\n",
    "# #Unet구조 middle의 xm값의 움직임에 주의\n",
    "# class Resnet18_Unet(nn.Module):\n",
    "#     def __init__(self,n_classes):\n",
    "#         super(Resnet18_Unet,self).__init__()\n",
    "#         self.fconv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3)\n",
    "#         self.fbn1 = nn.BatchNorm2d(64)\n",
    "#         self.frelu1 = nn.ReLU()\n",
    "#         self.fmaxpooling = nn.MaxPool2d(kernel_size=3,stride=2,padding=1)\n",
    "        \n",
    "#         self.conv2 = Conv2(64,128)\n",
    "#         self.conv3 = Conv3(128,256)\n",
    "#         self.conv4 = Conv4(256,512)\n",
    "#         self.conv5 = Conv5(512,1024)\n",
    "        \n",
    "#         self.middleconv = IdentityBlock(1024,2048)\n",
    "#         self.dropout = nn.Dropout2d(0.1) #\n",
    "           \n",
    "#         self.decoder5 = DecoderBlock(1024)\n",
    "#         self.decoder4 = DecoderBlock(512)\n",
    "#         self.decoder3 = DecoderBlock(256)\n",
    "#         self.decoder2 = DecoderBlock(128)\n",
    "#         self.decoder1 = DecoderBlock(64)\n",
    "#         self.transpose = nn.ConvTranspose2d(64, 64, kernel_size=4, stride=2, padding=1) # output_padding 추가\n",
    "        \n",
    "#         self.segmap = nn.Conv2d(64,n_classes, kernel_size=1)\n",
    "#         self.domain_classifier = domain_classifier()\n",
    "        \n",
    "#     def forward(self,x):\n",
    "#         x = self.fconv1(x)#3->64\n",
    "#         x0 = self.fbn1(x)\n",
    "#         x1 = self.frelu1(x)\n",
    "#         p = self.fmaxpooling(x1)#첫 conv: x0([8, 64, 109, 109]) p([8, 64, 54, 54])\n",
    "#         #print(\"conv1: \",x1.shape, \"maxpooling: \",p.shape)\n",
    "#         x2,p = self.conv2(p)\n",
    "#         #print(\"conv2: \",x2.shape, \"maxpooling: \",p.shape)\n",
    "#         x3,p = self.conv3(p)\n",
    "#         #print(\"conv3: \",x3.shape, \"maxpooling: \",p.shape)\n",
    "#         x4,p = self.conv4(p)\n",
    "#         #print(\"conv4: \",x4.shape, \"maxpooling: \",p.shape)\n",
    "#         x5,p = self.conv5(p)\n",
    "#         #print(\"conv5: \",x5.shape, \"maxpooling: \",p.shape)\n",
    "        \n",
    "#         xm = self.middleconv(p)#xm([8, 4096, 2, 2])\n",
    "#         #print(\"xm: \",xm.shape, \"maxpooling: \",p.shape)\n",
    "#         xm = self.dropout(xm)\n",
    "        \n",
    "#         x = self.decoder5(xm,x5)#뉴런:2048*2->2048 1\n",
    "#         x = self.decoder4(x,x4)#뉴런:1024*2->1024 \n",
    "#         x = self.decoder3(x,x3) #14\n",
    "#         x = self.decoder2(x,x2)#28\n",
    "#         x = self.decoder1(x,x1)#55\n",
    "#         x = self.transpose(x)\n",
    "        \n",
    "#         #print(x.shape)\n",
    "#         #x = F.interpolate(x, size=(224, 224))\n",
    "#         x_c = self.segmap(x)\n",
    "#         x_d = self.domain_classifier(x)\n",
    "        \n",
    "#         return x_c,x_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# # 저장된 class_weights를 불러옵니다.\n",
    "# class_weights_path = 'CLASS_WEIGHTS.pkl'\n",
    "\n",
    "# with open(class_weights_path, 'rb') as file:\n",
    "#     CLASS_WEIGHTS = pickle.load(file)\n",
    "\n",
    "# print(CLASS_WEIGHTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function과 optimizer 정의\n",
    "\n",
    "class DANN_Loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DANN_Loss, self).__init__()\n",
    "\n",
    "        #self.CE = nn.CrossEntropyLoss(weight=CLASS_WEIGHTS) # 0~9 class 분류용\n",
    "        self.CE = nn.CrossEntropyLoss()\n",
    "        #self.BCE = nn.BCELoss() # 도메인 분류용\n",
    "        \n",
    "    # result : DANN_CNN에서 반환된 값\n",
    "    # label : 숫자 0 ~ 9에 대한 라벨\n",
    "    # domain_num : 0(source) or 1(target)\n",
    "    def forward(self, result, label, domain_num, alpha = 1):\n",
    "        label_logits, domain_logits = result # DANN_CNN의 결과\n",
    "\n",
    "        batch_size = domain_logits.shape[0]\n",
    "\n",
    "        # print(\"segment_mask : \", label.shape)\n",
    "        # print(\"domain_answer : \", domain_target.shape)\n",
    "        segment_loss = self.CE(label_logits, label) # class 분류 loss\n",
    "\n",
    "        # domain_target = torch.FloatTensor([domain_num] * batch_size).unsqueeze(1).to(device)\n",
    "        # domain_loss = self.BCE(domain_logits, domain_target)\n",
    "\n",
    "        domain_target = torch.LongTensor([domain_num] * batch_size).to(device)\n",
    "        domain_loss = self.CE(domain_logits, domain_target) # domain 분류 loss\n",
    "        \n",
    "        loss = segment_loss + alpha * domain_loss\n",
    "\n",
    "        return loss, segment_loss, domain_loss\n",
    "    \n",
    "\n",
    "loss_fn = DANN_Loss().to(device)\n",
    "\n",
    "#criterion =nn.CrossEntropyLoss()\n",
    "#domain_criterion = nn.BCELoss()\n",
    "#criterion = nn.CrossEntropyLoss(weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.001\n",
    "EP = 10\n",
    "BATCH_SIZE = 16\n",
    "ACCMULATION_STEP = 1 \n",
    "N_CLASSES = 13 #IoU 점수측정하기 위한 클래스의 개수\n",
    "ALPHA = 0.0001\n",
    "Label = [0,1,2]\n",
    "N_LABELS = len(Label)\n",
    "# model 초기화\n",
    "#model = Resnet18_Unet(n_classes = N_CLASSES).to(device)\n",
    "model = Resnet34_Unet(n_classes = N_CLASSES).to(device)\n",
    "#model = Resnet50_Unet(n_classes = N_CLASSES).to(device)\n",
    "#model = Unet(n_classes = N_CLASSES).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "optimizer.zero_grad() \n",
    "\n",
    "source_dataset = CustomDataset(csv_file='./data/896_csv/train_source.csv', transform=transform)\n",
    "source_dataloader = DataLoader(source_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_target_dataset = CustomDataset(csv_file='./data/896_csv/val_source.csv', transform=transform)\n",
    "#val_target_dataset = CustomDataset(csv_file='./data/896_csv/val_source_CL.csv', transform=transform)\n",
    "val_target_dataloader = DataLoader(val_target_dataset, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import os\n",
    "# import cv2\n",
    "# import matplotlib.pyplot as plt\n",
    "# a1 = pd.read_csv(\"./data/6_fish_source.csv\")\n",
    "# p = \"./data/224/\"\n",
    "# a2 = os.path.join(p, a1.iloc[3,2])\n",
    "# a3 = cv2.imread(a2)\n",
    "# a4 = cv2.cvtColor(a3, cv2.COLOR_BGR2GRAY)\n",
    "# a4 = np.round(a4).astype(np.uint8)\n",
    "# a5 = a4*20\n",
    "\n",
    "# plt.imshow(a5, cmap='gray')\n",
    "# plt.axis('off')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "# #torch.cuda.empty_cache()\n",
    "# # import wandb\n",
    "\n",
    "\n",
    "# # wandb.init(\n",
    "# #     # set the wandb project where this run will be logged\n",
    "# #     project=\"practice_10_27_4d_res18\",\n",
    "    \n",
    "# #     # track hyperparameters and run metadata\n",
    "# #     config={\n",
    "# #     \"learning_rate\": LR,\n",
    "# #     \"architecture\": \"CNN\",\n",
    "# #     \"dataset\": \"Samsung\",\n",
    "# #     \"epochs\": EP,\n",
    "# #     }\n",
    "# # )\n",
    "\n",
    "# for epoch in range(EP):\n",
    "#     # 클래스별 IoU를 누적할 리스트 초기화\n",
    "#     train_class_ious = []\n",
    "#     fish_train_class_ious = []\n",
    "#     # 학습\n",
    "#     model.train()\n",
    "#     epoch_loss = 0\n",
    "#     seg_loss = 0\n",
    "#     dom_loss = 0\n",
    "\n",
    "#     for source_images, source_masks in tqdm(source_dataloader):\n",
    "#         label = random.randint(0,3)\n",
    "#         source_images, source_masks = apply_fisheye_distortion(source_images, source_masks, label)\n",
    "#         source_images = source_images.float().to(device)\n",
    "#         source_masks = source_masks.long().to(device)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         source_outputs = model(source_images)\n",
    "\n",
    "#         source_loss, segment_loss, domain_loss = loss_fn(source_outputs, source_masks, label, alpha = ALPHA)\n",
    "\n",
    "#         loss = source_loss\n",
    "#         epoch_loss += loss.item()\n",
    "#         seg_loss += segment_loss.item()\n",
    "#         dom_loss += domain_loss.item()\n",
    "\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         #scheduler.step()\n",
    "#                 # train 클래스별 IoU 계산\n",
    "#         source_outputs = torch.softmax(source_outputs[0], dim=1).cpu()\n",
    "#         source_outputs = torch.argmax(source_outputs, dim=1).numpy()\n",
    "\n",
    "#         for class_id in range(N_CLASSES):\n",
    "#             iou = calculate_iou_per_class(np.array(source_masks.cpu()), np.array(source_outputs), class_id)\n",
    "#             train_class_ious.append(iou)\n",
    "\n",
    "#     train_class_ious = np.array(train_class_ious).reshape(N_CLASSES, -1)\n",
    "#     train_class_ious = np.mean(train_class_ious, axis=1)\n",
    "#     print(\"--IoU Scores Train--\")\n",
    "#     for class_id, iou in enumerate(train_class_ious):\n",
    "#         print(f'Class{class_id}: {iou:.4f}', end=\" \")\n",
    "#         if (class_id+1) % 7 == 0:\n",
    "#             print()\n",
    "\n",
    "#     # mIoU 계산\n",
    "#     train_mIoU = np.mean(train_class_ious)\n",
    "\n",
    "#     # 에폭마다 결과 출력 \n",
    "#     print(f\"\\nEpoch{epoch+1}\")\n",
    "#     print(f\"Train seg Loss: {(seg_loss/len(source_dataloader))}\")\n",
    "#     print(f\"Train dom Loss: {(dom_loss/len(source_dataloader))}\")\n",
    "#     print(f\"Train Loss: {(epoch_loss/len(source_dataloader))}\")\n",
    "#     print(f\"Train mIoU: {train_mIoU}\" )\n",
    "#     print(\"___________________________________________________________________________________________\\n\")\n",
    "\n",
    "\n",
    "#     ################################################################\n",
    "#     # 클래스별 IoU를 누적할 리스트 초기화\n",
    "#     val_class_ious = []\n",
    "#     fish_val_class_ious = []\n",
    "#     val_epoch_loss = 0\n",
    "#     val_seg_loss = 0\n",
    "#     val_dom_loss = 0\n",
    "#     # 학습\n",
    "#     with torch.no_grad():\n",
    "#         model.eval()\n",
    "\n",
    "#         for target_images, target_masks in tqdm(val_target_dataloader):\n",
    "#             label = 2.5\n",
    "#             target_images, target_masks = apply_fisheye_distortion(target_images, target_masks, label)\n",
    "#             target_images = target_images.float().to(device)\n",
    "#             target_masks = target_masks.long().to(device)\n",
    "\n",
    "#             target_outputs = model(target_images)\n",
    "\n",
    "#             target_loss, val_segment_loss, val_domain_loss = loss_fn(target_outputs, target_masks, 0, alpha = ALPHA)\n",
    "\n",
    "#             loss = target_loss\n",
    "\n",
    "#             val_epoch_loss += loss.item()\n",
    "#             val_seg_loss += val_segment_loss.item()\n",
    "#             val_dom_loss += val_domain_loss.item()\n",
    "\n",
    "#             # train 클래스별 IoU 계산\n",
    "#             target_outputs = torch.softmax(target_outputs[0], dim=1).cpu()\n",
    "#             target_outputs = torch.argmax(target_outputs, dim=1).numpy()\n",
    "\n",
    "#             for class_id in range(N_CLASSES):\n",
    "#                 iou = calculate_iou_per_class(np.array(target_masks.cpu()), np.array(target_outputs), class_id)\n",
    "#                 fish_val_class_ious.append(iou)\n",
    "\n",
    "#     fish_val_class_ious = np.array(fish_val_class_ious).reshape(N_CLASSES, -1)\n",
    "#     fish_val_class_ious = np.mean(fish_val_class_ious, axis=1)\n",
    "#     print()\n",
    "#     print(\"--IoU Scores Fish val--\")\n",
    "#     for class_id, iou in enumerate(fish_val_class_ious):\n",
    "#         print(f'Class{class_id}: {iou:.4f}', end=\" \")\n",
    "#         if (class_id+1) % 7 == 0:\n",
    "#             print()\n",
    "\n",
    "#     # mIoU 계산\n",
    "#     fish_val_mIoU = np.mean(fish_val_class_ious)\n",
    "\n",
    "#     # 에폭마다 결과 출력 \n",
    "#     print(f\"\\nEpoch{epoch+1}\")\n",
    "#     print(f\"Valid seg Loss: {(val_seg_loss/len(val_target_dataloader))}\")\n",
    "#     print(f\"Valid dom Loss: {(val_dom_loss/len(val_target_dataloader))}\")\n",
    "#     print(f\"Valid Loss: {(val_epoch_loss/len(val_target_dataloader))}\")\n",
    "#     print(f\"Valid mIoU: {fish_val_mIoU}\" )\n",
    "#     print(\"___________________________________________________________________________________________\\n\")\n",
    "\n",
    "# #     # log metrics to wandb\n",
    "# #     wandb.log({\"train score\": train_mIoU})\n",
    "# #     wandb.log({\"val score\": fish_val_mIoU})\n",
    "# #     wandb.log({\"train loss\": (epoch_loss/len(source_dataloader))})\n",
    "# #     wandb.log({\"val loss\": (val_epoch_loss/len(val_target_dataloader))})\n",
    "    \n",
    "    \n",
    "# # # [optional] finish the wandb run, necessary in notebooks\n",
    "# # wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1: 100%|██████████| 138/138 [04:59<00:00,  2.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label_0: IoU Scores Train\n",
      "Class00: 0.2817 Class01: 0.4742 Class02: 0.0622 Class03: 0.3814 Class04: 0.2022 Class05: 0.0000 Class06: 0.0459 \n",
      "Class07: 0.1044 Class08: 0.4074 Class09: 0.4751 Class10: 0.0000 Class11: 0.0000 Class12: 0.4005 \n",
      "Label_1: IoU Scores Train\n",
      "Class00: 0.2419 Class01: 0.4034 Class02: 0.0501 Class03: 0.3335 Class04: 0.1695 Class05: 0.0000 Class06: 0.0299 \n",
      "Class07: 0.0825 Class08: 0.3516 Class09: 0.3972 Class10: 0.0000 Class11: 0.0000 Class12: 0.3553 \n",
      "Label_2: IoU Scores Train\n",
      "Class00: 0.2241 Class01: 0.3599 Class02: 0.0407 Class03: 0.3033 Class04: 0.1453 Class05: 0.0000 Class06: 0.0221 \n",
      "Class07: 0.0722 Class08: 0.3194 Class09: 0.3532 Class10: 0.0000 Class11: 0.0000 Class12: 0.3228 \n",
      "Train seg Loss: 0.3599675991684918 Train dom Loss: 12.466046893693779\n",
      "Train Loss: 0.36121420346308447\n",
      "Train mIoU: 0.19007400569815108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]/tmp/ipykernel_1321860/288732703.py:26: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  domain_target = torch.LongTensor([domain_num] * batch_size).to(device)\n",
      "100%|██████████| 30/30 [00:10<00:00,  2.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--IoU Scores Fish val--\n",
      "Class00: 0.9781 Class01: 0.6438 Class02: 0.0514 Class03: 0.4347 Class04: 0.0573 Class05: 0.0000 Class06: 0.0146 \n",
      "Class07: 0.1305 Class08: 0.6003 Class09: 0.7738 Class10: 0.0000 Class11: 0.0000 Class12: 0.3660 \n",
      "Epoch1\n",
      "Valid Seg Loss: 0.41529587904612225 Valid dom Loss: 9.88056266705195\n",
      "Valid Loss: 0.41628393630186716\n",
      "Valid mIoU: 0.31157408850136153\n",
      "___________________________________________________________________________________________\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 2: 100%|██████████| 138/138 [04:59<00:00,  2.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label_0: IoU Scores Train\n",
      "Class00: 0.2791 Class01: 0.4702 Class02: 0.0658 Class03: 0.3795 Class04: 0.1874 Class05: 0.0000 Class06: 0.0443 \n",
      "Class07: 0.0909 Class08: 0.4008 Class09: 0.4745 Class10: 0.0001 Class11: 0.0000 Class12: 0.3867 \n",
      "Label_1: IoU Scores Train\n",
      "Class00: 0.2413 Class01: 0.4008 Class02: 0.0538 Class03: 0.3304 Class04: 0.1552 Class05: 0.0000 Class06: 0.0295 \n",
      "Class07: 0.0708 Class08: 0.3446 Class09: 0.3978 Class10: 0.0000 Class11: 0.0000 Class12: 0.3385 \n",
      "Label_2: IoU Scores Train\n",
      "Class00: 0.2241 Class01: 0.3582 Class02: 0.0436 Class03: 0.3019 Class04: 0.1328 Class05: 0.0000 Class06: 0.0224 \n",
      "Class07: 0.0627 Class08: 0.3154 Class09: 0.3548 Class10: 0.0000 Class11: 0.0000 Class12: 0.3093 \n",
      "Train seg Loss: 0.4075430307339355 Train dom Loss: 3054.4778660873135\n",
      "Train Loss: 0.7129908102937943\n",
      "Train mIoU: 0.18634779427268192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:10<00:00,  2.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--IoU Scores Fish val--\n",
      "Class00: 0.8344 Class01: 0.2763 Class02: 0.0061 Class03: 0.1997 Class04: 0.0011 Class05: 0.0000 Class06: 0.0000 \n",
      "Class07: 0.0001 Class08: 0.2498 Class09: 0.5224 Class10: 0.0000 Class11: 0.0000 Class12: 0.0764 \n",
      "Epoch2\n",
      "Valid Seg Loss: 3.53246701558431 Valid dom Loss: 13564.20556640625\n",
      "Valid Loss: 4.888887540499369\n",
      "Valid mIoU: 0.16663861963787785\n",
      "___________________________________________________________________________________________\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 3: 100%|██████████| 138/138 [04:58<00:00,  2.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label_0: IoU Scores Train\n",
      "Class00: 0.2289 Class01: 0.1821 Class02: 0.0007 Class03: 0.1776 Class04: 0.0003 Class05: 0.0002 Class06: 0.0000 \n",
      "Class07: 0.0005 Class08: 0.2587 Class09: 0.3498 Class10: 0.0000 Class11: 0.0000 Class12: 0.0083 \n",
      "Label_1: IoU Scores Train\n",
      "Class00: 0.2191 Class01: 0.1622 Class02: 0.0003 Class03: 0.1673 Class04: 0.0000 Class05: 0.0001 Class06: 0.0000 \n",
      "Class07: 0.0003 Class08: 0.2282 Class09: 0.2950 Class10: 0.0000 Class11: 0.0000 Class12: 0.0078 \n",
      "Label_2: IoU Scores Train\n",
      "Class00: 0.2132 Class01: 0.1415 Class02: 0.0001 Class03: 0.1604 Class04: 0.0004 Class05: 0.0000 Class06: 0.0000 \n",
      "Class07: 0.0002 Class08: 0.2077 Class09: 0.2659 Class10: 0.0000 Class11: 0.0000 Class12: 0.0077 \n",
      "Train seg Loss: 1.770731371238036 Train dom Loss: 45241.39825448451\n",
      "Train Loss: 6.294871122652782\n",
      "Train mIoU: 0.08422728783041339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:10<00:00,  2.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--IoU Scores Fish val--\n",
      "Class00: 0.9087 Class01: 0.3148 Class02: 0.0000 Class03: 0.2660 Class04: 0.0000 Class05: 0.0000 Class06: 0.0000 \n",
      "Class07: 0.0000 Class08: 0.5118 Class09: 0.6754 Class10: 0.0000 Class11: 0.0000 Class12: 0.0001 \n",
      "Epoch3\n",
      "Valid Seg Loss: 0.7308792809645335 Valid dom Loss: 1248.7818725585937\n",
      "Valid Loss: 0.8557574629783631\n",
      "Valid mIoU: 0.20590402272073804\n",
      "___________________________________________________________________________________________\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 4: 100%|██████████| 138/138 [04:58<00:00,  2.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label_0: IoU Scores Train\n",
      "Class00: 0.2506 Class01: 0.3154 Class02: 0.0002 Class03: 0.2404 Class04: 0.0000 Class05: 0.0000 Class06: 0.0000 \n",
      "Class07: 0.0002 Class08: 0.3387 Class09: 0.3965 Class10: 0.0000 Class11: 0.0000 Class12: 0.0003 \n",
      "Label_1: IoU Scores Train\n",
      "Class00: 0.2311 Class01: 0.2700 Class02: 0.0001 Class03: 0.2257 Class04: 0.0000 Class05: 0.0000 Class06: 0.0000 \n",
      "Class07: 0.0001 Class08: 0.2949 Class09: 0.3344 Class10: 0.0000 Class11: 0.0000 Class12: 0.0002 \n",
      "Label_2: IoU Scores Train\n",
      "Class00: 0.2196 Class01: 0.2360 Class02: 0.0000 Class03: 0.2127 Class04: 0.0000 Class05: 0.0000 Class06: 0.0000 \n",
      "Class07: 0.0000 Class08: 0.2699 Class09: 0.2985 Class10: 0.0000 Class11: 0.0000 Class12: 0.0001 \n",
      "Train seg Loss: 0.7562346159691972 Train dom Loss: 431.9650066129818\n",
      "Train Loss: 0.7994311161087331\n",
      "Train mIoU: 0.10603526126178091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:10<00:00,  2.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--IoU Scores Fish val--\n",
      "Class00: 0.9487 Class01: 0.4176 Class02: 0.0000 Class03: 0.2964 Class04: 0.0000 Class05: 0.0000 Class06: 0.0000 \n",
      "Class07: 0.0000 Class08: 0.3712 Class09: 0.6537 Class10: 0.0000 Class11: 0.0000 Class12: 0.0000 \n",
      "Epoch4\n",
      "Valid Seg Loss: 0.6795113305250804 Valid dom Loss: 313.8914398193359\n",
      "Valid Loss: 0.7109004735946656\n",
      "Valid mIoU: 0.20672966053083305\n",
      "___________________________________________________________________________________________\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 5: 100%|██████████| 138/138 [04:58<00:00,  2.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label_0: IoU Scores Train\n",
      "Class00: 0.2648 Class01: 0.3404 Class02: 0.0000 Class03: 0.2646 Class04: 0.0000 Class05: 0.0000 Class06: 0.0000 \n",
      "Class07: 0.0015 Class08: 0.3387 Class09: 0.4064 Class10: 0.0000 Class11: 0.0000 Class12: 0.0006 \n",
      "Label_1: IoU Scores Train\n",
      "Class00: 0.2386 Class01: 0.2973 Class02: 0.0000 Class03: 0.2449 Class04: 0.0000 Class05: 0.0000 Class06: 0.0000 \n",
      "Class07: 0.0013 Class08: 0.2947 Class09: 0.3430 Class10: 0.0000 Class11: 0.0000 Class12: 0.0007 \n",
      "Label_2: IoU Scores Train\n",
      "Class00: 0.2245 Class01: 0.2617 Class02: 0.0000 Class03: 0.2291 Class04: 0.0000 Class05: 0.0000 Class06: 0.0000 \n",
      "Class07: 0.0020 Class08: 0.2695 Class09: 0.3059 Class10: 0.0000 Class11: 0.0000 Class12: 0.0006 \n",
      "Train seg Loss: 0.6631490200901953 Train dom Loss: 196.6879150283443\n",
      "Train Loss: 0.6828178110473974\n",
      "Train mIoU: 0.11104189676736766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:10<00:00,  2.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--IoU Scores Fish val--\n",
      "Class00: 0.9604 Class01: 0.4484 Class02: 0.0000 Class03: 0.3089 Class04: 0.0000 Class05: 0.0000 Class06: 0.0000 \n",
      "Class07: 0.0050 Class08: 0.4624 Class09: 0.6749 Class10: 0.0000 Class11: 0.0000 Class12: 0.0000 \n",
      "Epoch5\n",
      "Valid Seg Loss: 0.5811149011055629 Valid dom Loss: 108.51729685465494\n",
      "Valid Loss: 0.5919666310151418\n",
      "Valid mIoU: 0.2199975908147089\n",
      "___________________________________________________________________________________________\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 6: 100%|██████████| 138/138 [04:59<00:00,  2.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label_0: IoU Scores Train\n",
      "Class00: 0.2704 Class01: 0.3717 Class02: 0.0000 Class03: 0.2858 Class04: 0.0001 Class05: 0.0000 Class06: 0.0000 \n",
      "Class07: 0.0210 Class08: 0.3417 Class09: 0.4200 Class10: 0.0000 Class11: 0.0000 Class12: 0.0078 \n",
      "Label_1: IoU Scores Train\n",
      "Class00: 0.2403 Class01: 0.3265 Class02: 0.0000 Class03: 0.2608 Class04: 0.0000 Class05: 0.0000 Class06: 0.0000 \n",
      "Class07: 0.0165 Class08: 0.2978 Class09: 0.3548 Class10: 0.0000 Class11: 0.0000 Class12: 0.0110 \n",
      "Label_2: IoU Scores Train\n",
      "Class00: 0.2252 Class01: 0.2929 Class02: 0.0000 Class03: 0.2436 Class04: 0.0000 Class05: 0.0000 Class06: 0.0000 \n",
      "Class07: 0.0153 Class08: 0.2733 Class09: 0.3186 Class10: 0.0000 Class11: 0.0000 Class12: 0.0120 \n",
      "Train seg Loss: 0.599718213369305 Train dom Loss: 164.58659840240668\n",
      "Train Loss: 0.6161768726536617\n",
      "Train mIoU: 0.11813911677605908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:10<00:00,  2.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--IoU Scores Fish val--\n",
      "Class00: 0.9568 Class01: 0.4893 Class02: 0.0000 Class03: 0.3174 Class04: 0.0000 Class05: 0.0000 Class06: 0.0000 \n",
      "Class07: 0.0483 Class08: 0.4278 Class09: 0.6426 Class10: 0.0000 Class11: 0.0000 Class12: 0.0063 \n",
      "Epoch6\n",
      "Valid Seg Loss: 0.5756433228651683 Valid dom Loss: 22.72320276896159\n",
      "Valid Loss: 0.5779156446456909\n",
      "Valid mIoU: 0.22219014027685777\n",
      "___________________________________________________________________________________________\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 7: 100%|██████████| 138/138 [04:58<00:00,  2.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label_0: IoU Scores Train\n",
      "Class00: 0.2713 Class01: 0.3924 Class02: 0.0017 Class03: 0.3069 Class04: 0.0026 Class05: 0.0000 Class06: 0.0000 \n",
      "Class07: 0.0429 Class08: 0.3442 Class09: 0.4295 Class10: 0.0000 Class11: 0.0000 Class12: 0.0984 \n",
      "Label_1: IoU Scores Train\n",
      "Class00: 0.2404 Class01: 0.3481 Class02: 0.0009 Class03: 0.2767 Class04: 0.0013 Class05: 0.0000 Class06: 0.0000 \n",
      "Class07: 0.0345 Class08: 0.2987 Class09: 0.3651 Class10: 0.0000 Class11: 0.0000 Class12: 0.0998 \n",
      "Label_2: IoU Scores Train\n",
      "Class00: 0.2250 Class01: 0.3153 Class02: 0.0006 Class03: 0.2550 Class04: 0.0014 Class05: 0.0000 Class06: 0.0000 \n",
      "Class07: 0.0322 Class08: 0.2736 Class09: 0.3281 Class10: 0.0000 Class11: 0.0000 Class12: 0.0996 \n",
      "Train seg Loss: 0.5481998106995642 Train dom Loss: 172.93342789298165\n",
      "Train Loss: 0.5654931530572366\n",
      "Train mIoU: 0.13041471586479647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:10<00:00,  2.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--IoU Scores Fish val--\n",
      "Class00: 0.9633 Class01: 0.5614 Class02: 0.0036 Class03: 0.3526 Class04: 0.0008 Class05: 0.0000 Class06: 0.0002 \n",
      "Class07: 0.0827 Class08: 0.5326 Class09: 0.6918 Class10: 0.0000 Class11: 0.0000 Class12: 0.2937 \n",
      "Epoch7\n",
      "Valid Seg Loss: 0.49834171632925667 Valid dom Loss: 9.447310772413656\n",
      "Valid Loss: 0.4992864429950714\n",
      "Valid mIoU: 0.2679041449006444\n",
      "___________________________________________________________________________________________\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 8: 100%|██████████| 138/138 [04:58<00:00,  2.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label_0: IoU Scores Train\n",
      "Class00: 0.2724 Class01: 0.4049 Class02: 0.0070 Class03: 0.3133 Class04: 0.0086 Class05: 0.0000 Class06: 0.0001 \n",
      "Class07: 0.0473 Class08: 0.3441 Class09: 0.4280 Class10: 0.0000 Class11: 0.0000 Class12: 0.1863 \n",
      "Label_1: IoU Scores Train\n",
      "Class00: 0.2405 Class01: 0.3564 Class02: 0.0041 Class03: 0.2784 Class04: 0.0039 Class05: 0.0000 Class06: 0.0000 \n",
      "Class07: 0.0380 Class08: 0.2980 Class09: 0.3633 Class10: 0.0000 Class11: 0.0000 Class12: 0.1842 \n",
      "Label_2: IoU Scores Train\n",
      "Class00: 0.2249 Class01: 0.3235 Class02: 0.0021 Class03: 0.2545 Class04: 0.0033 Class05: 0.0000 Class06: 0.0000 \n",
      "Class07: 0.0359 Class08: 0.2730 Class09: 0.3262 Class10: 0.0000 Class11: 0.0000 Class12: 0.1798 \n",
      "Train seg Loss: 0.5229779360230994 Train dom Loss: 125.48136529694011\n",
      "Train Loss: 0.5355260723454941\n",
      "Train mIoU: 0.1385101509326184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:10<00:00,  2.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--IoU Scores Fish val--\n",
      "Class00: 0.9701 Class01: 0.5918 Class02: 0.0184 Class03: 0.3263 Class04: 0.0015 Class05: 0.0000 Class06: 0.0000 \n",
      "Class07: 0.0610 Class08: 0.4651 Class09: 0.6954 Class10: 0.0000 Class11: 0.0000 Class12: 0.2797 \n",
      "Epoch8\n",
      "Valid Seg Loss: 0.5081204374631246 Valid dom Loss: 28.13329652150472\n",
      "Valid Loss: 0.5109337707360585\n",
      "Valid mIoU: 0.26225473666700183\n",
      "___________________________________________________________________________________________\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 9: 100%|██████████| 138/138 [04:57<00:00,  2.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label_0: IoU Scores Train\n",
      "Class00: 0.2746 Class01: 0.4268 Class02: 0.0144 Class03: 0.3310 Class04: 0.0248 Class05: 0.0001 Class06: 0.0021 \n",
      "Class07: 0.0648 Class08: 0.3565 Class09: 0.4395 Class10: 0.0000 Class11: 0.0000 Class12: 0.2329 \n",
      "Label_1: IoU Scores Train\n",
      "Class00: 0.2409 Class01: 0.3723 Class02: 0.0097 Class03: 0.2921 Class04: 0.0158 Class05: 0.0000 Class06: 0.0018 \n",
      "Class07: 0.0503 Class08: 0.3091 Class09: 0.3711 Class10: 0.0000 Class11: 0.0000 Class12: 0.2257 \n",
      "Label_2: IoU Scores Train\n",
      "Class00: 0.2248 Class01: 0.3385 Class02: 0.0067 Class03: 0.2668 Class04: 0.0134 Class05: 0.0000 Class06: 0.0014 \n",
      "Class07: 0.0459 Class08: 0.2835 Class09: 0.3324 Class10: 0.0000 Class11: 0.0000 Class12: 0.2176 \n",
      "Train seg Loss: 0.48499615195292783 Train dom Loss: 115.60909656063671\n",
      "Train Loss: 0.49655706115103\n",
      "Train mIoU: 0.14839188345403215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:10<00:00,  2.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--IoU Scores Fish val--\n",
      "Class00: 0.9719 Class01: 0.6340 Class02: 0.0197 Class03: 0.3986 Class04: 0.0033 Class05: 0.0000 Class06: 0.0008 \n",
      "Class07: 0.0799 Class08: 0.4907 Class09: 0.7511 Class10: 0.0000 Class11: 0.0000 Class12: 0.3822 \n",
      "Epoch9\n",
      "Valid Seg Loss: 0.44091831545035043 Valid dom Loss: 0.0454291919390196\n",
      "Valid Loss: 0.4409228593111038\n",
      "Valid mIoU: 0.287087801220372\n",
      "___________________________________________________________________________________________\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 10: 100%|██████████| 138/138 [04:55<00:00,  2.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label_0: IoU Scores Train\n",
      "Class00: 0.2749 Class01: 0.4313 Class02: 0.0223 Class03: 0.3371 Class04: 0.0871 Class05: 0.0000 Class06: 0.0073 \n",
      "Class07: 0.0612 Class08: 0.3613 Class09: 0.4370 Class10: 0.0000 Class11: 0.0000 Class12: 0.2817 \n",
      "Label_1: IoU Scores Train\n",
      "Class00: 0.2408 Class01: 0.3741 Class02: 0.0177 Class03: 0.2965 Class04: 0.0736 Class05: 0.0000 Class06: 0.0056 \n",
      "Class07: 0.0477 Class08: 0.3105 Class09: 0.3682 Class10: 0.0000 Class11: 0.0000 Class12: 0.2669 \n",
      "Label_2: IoU Scores Train\n",
      "Class00: 0.2245 Class01: 0.3384 Class02: 0.0141 Class03: 0.2722 Class04: 0.0649 Class05: 0.0000 Class06: 0.0038 \n",
      "Class07: 0.0445 Class08: 0.2843 Class09: 0.3306 Class10: 0.0000 Class11: 0.0000 Class12: 0.2549 \n",
      "Train seg Loss: 0.4662574621790273 Train dom Loss: 129.7695314331376\n",
      "Train Loss: 0.4792344149760002\n",
      "Train mIoU: 0.15731582476869782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:09<00:00,  3.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--IoU Scores Fish val--\n",
      "Class00: 0.9703 Class01: 0.6068 Class02: 0.0476 Class03: 0.4109 Class04: 0.0350 Class05: 0.0000 Class06: 0.0061 \n",
      "Class07: 0.0906 Class08: 0.5210 Class09: 0.7420 Class10: 0.0000 Class11: 0.0000 Class12: 0.4228 \n",
      "Epoch10\n",
      "Valid Seg Loss: 0.45157559514045714 Valid dom Loss: 130.43284810384114\n",
      "Valid Loss: 0.4646188805500666\n",
      "Valid mIoU: 0.2963916521719974\n",
      "___________________________________________________________________________________________\n",
      "\n",
      "Hyperparamerters\n",
      "LR = 0.001 | EP = 10, BATCH_SIZE = 16, N_CLASSES = 13, init_alpha = 0.0001, N_LABELS = 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "#torch.cuda.empty_cache()\n",
    "# import wandb\n",
    "\n",
    "\n",
    "# wandb.init(\n",
    "#     # set the wandb project where this run will be logged\n",
    "#     project=\"11_08_4d_res34_0.0001\",\n",
    "    \n",
    "#     # track hyperparameters and run metadata\n",
    "#     config={\n",
    "#     \"learning_rate\": LR,\n",
    "#     \"architecture\": \"CNN\",\n",
    "#     \"dataset\": \"Samsung\",\n",
    "#     \"epochs\": EP,\n",
    "#     }\n",
    "# )\n",
    "\n",
    "for epoch in range(EP):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    seg_losses = 0\n",
    "    domain_losses = 0\n",
    "    if epoch < 10:  # 전체 EP 40%\n",
    "        alpha = 0.0001\n",
    "    elif epoch <25: # 전체 EP 30%\n",
    "        alpha = 0.0005\n",
    "    else:          # 전체 EP 30%\n",
    "        alpha = 0.0001\n",
    "    train_class_ious = [[],[],[]]\n",
    "    for source_images, source_masks in tqdm(source_dataloader,desc=f\"Epoch: {epoch+1}\"):\n",
    "        random.shuffle(Label)\n",
    "        for l in range(N_LABELS):\n",
    "            label = Label[l]\n",
    "            source_image, source_mask = apply_fisheye_distortion(source_images, source_masks, label)\n",
    "            source_image = source_image.float().to(device)\n",
    "            source_mask = source_mask.long().to(device)\n",
    "            source_outputs = model(source_image)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            target_loss, seg_loss, domain_loss = loss_fn(source_outputs, source_mask, label, alpha = alpha)\n",
    "            epoch_loss += target_loss.item()\n",
    "            seg_losses +=  seg_loss.item()\n",
    "            domain_losses += domain_loss.item()\n",
    "            target_loss.backward()\n",
    "            optimizer.step()\n",
    "            # miou측정\n",
    "            source_outputs = model(source_image)\n",
    "            source_outputs = torch.softmax(source_outputs[0], dim=1).cpu()\n",
    "            source_outputs = torch.argmax(source_outputs, dim=1).numpy()\n",
    "            for class_id in range(N_CLASSES):\n",
    "                iou = calculate_iou_per_class(np.array(source_masks.cpu()), np.array(source_outputs), class_id)\n",
    "                train_class_ious[label].append(iou)\n",
    "            #print(train_class_ious[0])\n",
    "    \n",
    "    for i in range(N_LABELS):\n",
    "        buff = np.array(train_class_ious[i]).reshape(-1, N_CLASSES)\n",
    "        buff = np.mean(buff, axis=0)\n",
    "        print(f\"\\nLabel_{i}: IoU Scores Train\") \n",
    "        for class_id, iou in enumerate(buff):\n",
    "            print(f'Class{class_id:02d}: {iou:.4f}', end=\" \")\n",
    "            if (class_id+1) % 7 == 0:\n",
    "                print()   \n",
    "    print()    \n",
    "    print(f\"Train seg Loss: {(seg_losses/(N_LABELS*len(source_dataloader)))}\", f\"Train dom Loss: {(domain_losses/(N_LABELS*len(source_dataloader)))}\")\n",
    "    print(f\"Train Loss: {(epoch_loss/(N_LABELS*len(source_dataloader)))}\")\n",
    "    print(f\"Train mIoU: {np.mean(train_class_ious)}\" )\n",
    "    ################################################################\n",
    "    # 클래스별 IoU를 누적할 리스트 초기화\n",
    "    val_class_ious = []\n",
    "    fish_val_class_ious = []\n",
    "    val_epoch_loss = 0\n",
    "    val_seg_loss = 0\n",
    "    val_domain_loss = 0\n",
    "    # valid\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "\n",
    "        for target_images, target_masks in tqdm(val_target_dataloader):\n",
    "            label = 1.5\n",
    "            target_images, target_masks = apply_fisheye_distortion(target_images, target_masks, label)\n",
    "            target_images = target_images.float().to(device)\n",
    "            target_masks = target_masks.long().to(device)\n",
    "\n",
    "            target_outputs = model(target_images)\n",
    "\n",
    "            target_loss, target_seg_loss, target_domain_loss = loss_fn(target_outputs, target_masks, label, alpha = alpha)\n",
    "\n",
    "            val_seg_loss +=  target_seg_loss.item()\n",
    "            val_domain_loss += target_domain_loss.item()\n",
    "            \n",
    "            loss = target_loss\n",
    "\n",
    "            val_epoch_loss += loss.item()\n",
    "\n",
    "            # train 클래스별 IoU 계산\n",
    "            target_outputs = torch.softmax(target_outputs[0], dim=1).cpu()\n",
    "            target_outputs = torch.argmax(target_outputs, dim=1).numpy()\n",
    "\n",
    "            for class_id in range(N_CLASSES):\n",
    "                iou = calculate_iou_per_class(np.array(target_masks.cpu()), np.array(target_outputs), class_id)\n",
    "                fish_val_class_ious.append(iou)\n",
    "\n",
    "    fish_val_class_ious = np.array(fish_val_class_ious).reshape(-1, N_CLASSES)\n",
    "    fish_val_class_ious = np.mean(fish_val_class_ious, axis=0)\n",
    "    print()\n",
    "    print(\"--IoU Scores Fish val--\")\n",
    "    for class_id, iou in enumerate(fish_val_class_ious):\n",
    "        print(f'Class{class_id:02d}: {iou:.4f}', end=\" \")\n",
    "        if (class_id+1) % 7 == 0:\n",
    "            print()\n",
    "\n",
    "    # mIoU 계산\n",
    "    fish_val_mIoU = np.mean(fish_val_class_ious)\n",
    "\n",
    "    # 에폭마다 결과 출력 \n",
    "    print(f\"\\nEpoch{epoch+1}\")\n",
    "    print(f\"Valid Seg Loss: {(val_seg_loss/len(val_target_dataloader))}\",f\"Valid dom Loss: {(val_domain_loss/len(val_target_dataloader))}\")\n",
    "    print(f\"Valid Loss: {(val_epoch_loss/len(val_target_dataloader))}\")\n",
    "    print(f\"Valid mIoU: {fish_val_mIoU}\" )\n",
    "    print(\"___________________________________________________________________________________________\\n\")\n",
    "\n",
    "\n",
    "#     # log metrics to wandb\n",
    "#     wandb.log({\"train score\": np.mean(train_class_ious)})\n",
    "#     wandb.log({\"val score\": fish_val_mIoU})\n",
    "#     wandb.log({\"train loss\": (epoch_loss/(N_LABELS*len(source_dataloader)))})\n",
    "#     wandb.log({\"val loss\": (val_epoch_loss/len(val_target_dataloader))})\n",
    "    \n",
    "    \n",
    "# # [optional] finish the wandb run, necessary in notebooks\n",
    "# wandb.finish()\n",
    "\n",
    "print(\"Hyperparamerters\")\n",
    "print(f\"LR = {LR} | EP = {EP}, BATCH_SIZE = {BATCH_SIZE}, N_CLASSES = {N_CLASSES}, init_alpha = {ALPHA}, N_LABELS = {N_LABELS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './data/resnet34_1109_nos_0001.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "byungwan_resn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
