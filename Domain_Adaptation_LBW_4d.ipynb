{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/MMI24limbyungwan/.conda/envs/byungwan_resn/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "#from torch.optim.lr_scheduler import _LRScheduler\n",
    "\n",
    "from tqdm import tqdm\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# GPU 사용이 가능할 경우, GPU를 사용할 수 있게 함.'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = torch.device(device)\n",
    "print(device)\n",
    "\n",
    "print(os.environ.get('CUDA_VISIBLE_DEVICES'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RLE 인코딩 함수\n",
    "def rle_encode(mask):\n",
    "    pixels = mask.flatten()\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)\n",
    "\n",
    "# 클래스별 IoU를 계산하기 위한 함수\n",
    "def calculate_iou_per_class(y_true, y_pred, class_id):\n",
    "    intersection = np.sum((y_true == class_id) & (y_pred == class_id))\n",
    "    union = np.sum((y_true == class_id) | (y_pred == class_id))\n",
    "    iou = intersection / union if union > 0 else 0\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "\n",
    "def apply_fisheye_distortion(images, masks, label):\n",
    "    # 이미지 크기 가져오기\n",
    "    batch, channel, height, width = images.shape\n",
    "\n",
    "    # 카메라 매트릭스 생성\n",
    "    focal_length = width / 4\n",
    "    center_x = width / 2\n",
    "    center_y = height / 2\n",
    "    camera_matrix = np.array([[focal_length, 0, center_x],\n",
    "                              [0, focal_length, center_y],\n",
    "                              [0, 0, 1]], dtype=np.float32)\n",
    "\n",
    "    # 왜곡 계수 생성\n",
    "    # dist_num = 0\n",
    "    # if label == 1:\n",
    "    #     dist_num = random.randint(1,3)\n",
    "    # elif label == 2.5:\n",
    "    #     dist_num = 2.5\n",
    "    dist_num = label+1\n",
    "    dist_coeffs = np.array([0, 0.03 * dist_num, 0, 0], dtype=np.float32)\n",
    "\n",
    "    # 왜곡 보정\n",
    "    undistorted_images = []\n",
    "    undistorted_masks = []\n",
    "\n",
    "    for i in range(batch):\n",
    "        image = images[i].permute(1, 2, 0).cpu().numpy()  # 텐서를 NumPy 배열로 변환\n",
    "        mask = masks[i].cpu().numpy()\n",
    "        undistorted_image = cv2.undistort(image, camera_matrix, dist_coeffs)\n",
    "        undistorted_mask = cv2.undistort(mask, camera_matrix, dist_coeffs)\n",
    "        undistorted_mask = np.round(undistorted_mask).astype(np.uint8)\n",
    "        undistorted_mask[undistorted_mask > 12] = 12\n",
    "\n",
    "        # 다시 텐서로 변환\n",
    "        undistorted_image = torch.from_numpy(undistorted_image).permute(2, 0, 1).float().to(device)\n",
    "        undistorted_mask = torch.from_numpy(undistorted_mask).long().to(device)\n",
    "\n",
    "        undistorted_images.append(undistorted_image)\n",
    "        undistorted_masks.append(undistorted_mask)\n",
    "\n",
    "    undistorted_images = torch.stack(undistorted_images, dim=0)\n",
    "    undistorted_masks = torch.stack(undistorted_masks, dim=0)\n",
    "\n",
    "    return undistorted_images, undistorted_masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# # 이미지 데이터 (batch, channel, height, width)\n",
    "# image_data = torch.randn(4, 3, 256, 256)\n",
    "\n",
    "# # 이미지 차원 변경 (batch, height, width, channel)\n",
    "# image_data_permuted = image_data.permute(0, 2, 3, 1)\n",
    "\n",
    "# # 이미지 크기 확인\n",
    "# print(image_data_permuted.shape)\n",
    "\n",
    "# # 4개의 이미지로 나누기\n",
    "# images = image_data_permuted.split(1, dim=0)\n",
    "# # 또는 images = torch.split(image_data_permuted, 1, dim=0)\n",
    "# undistorted_images = []\n",
    "# # 4개 이미지의 크기 확인\n",
    "# for i, image in enumerate(images):\n",
    "#     print(f\"Image {i + 1} shape: {image.shape}\")\n",
    "#     undistorted_images.append(images[i].squeeze())\n",
    "\n",
    "# undistorted_images2 = torch.stack(undistorted_images, dim=0)\n",
    "# undistorted_images3 = undistorted_images2.permute(0,3,1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None, infer=False):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "        self.infer = infer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #directory_path = \"/mnt/nas27/Dataset/Samsung_DM\"\n",
    "        directory_path = './data/224'\n",
    "        img_path = self.data.iloc[idx, 1]\n",
    "        img_path = os.path.join(directory_path, img_path)\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        #image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        if self.infer:\n",
    "            if self.transform:\n",
    "                image = self.transform(image=image)['image']\n",
    "            return image\n",
    "        \n",
    "        mask_path = self.data.iloc[idx, 2]\n",
    "        mask_path = os.path.join(directory_path, mask_path)\n",
    "        mask = cv2.imread(mask_path)\n",
    "        #mask = cv2.cvtColor(mask, cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)\n",
    "        mask = np.round(mask).astype(np.uint8)\n",
    "        mask[mask > 12] = 12 #배경을 픽셀값 12로 간주\n",
    "        mask += 1\n",
    "        mask[mask == 13] = 0\n",
    "\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "# class CustomDataset_target(Dataset):\n",
    "#     def __init__(self, csv_file, transform=None, infer=False):\n",
    "#         self.data = pd.read_csv(csv_file)\n",
    "#         self.transform = transform\n",
    "#         self.infer = infer\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         directory_path = \"/mnt/nas27/Dataset/Samsung_DM\"\n",
    "#         img_path = self.data.iloc[idx, 1]\n",
    "#         img_path = os.path.join(directory_path, img_path[2:])\n",
    "#         image = cv2.imread(img_path)\n",
    "#         image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "#         if self.infer:\n",
    "#             if self.transform:\n",
    "#                 image = self.transform(image=image)['image']\n",
    "#             return image\n",
    "\n",
    "\n",
    "#         if self.transform:\n",
    "#             augmented = self.transform(image=image)\n",
    "#             image = augmented['image']\n",
    "            \n",
    "\n",
    "#         return image\n",
    "     \n",
    "\n",
    "transform = A.Compose(\n",
    "    [   \n",
    "        #A.Resize(224, 224),\n",
    "        #A.Resize(128, 128),\n",
    "        A.Normalize(),\n",
    "        \n",
    "        # 변형\n",
    "        # A.VerticalFlip(p=0.5),\n",
    "        # A.RandomRotate90(p=0.5),\n",
    "        # A.HueSaturationValue(p=0.2),\n",
    "        \n",
    "        ToTensorV2()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Unet의 기본이 되는 conv블럭\n",
    "# class ConvBlock(nn.Module):\n",
    "#     def __init__(self, in_channels, out_channels):\n",
    "#         super(ConvBlock, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "#         self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "#         self.relu1 = nn.ReLU()\n",
    "        \n",
    "#         self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)  # 여기서 in_channels는 out_channels와 동일해야 합니다.\n",
    "#         self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "#         self.relu2 = nn.ReLU()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.conv1(x)\n",
    "#         x = self.bn1(x)\n",
    "#         x = self.relu1(x)\n",
    "        \n",
    "#         x = self.conv2(x)\n",
    "#         x = self.bn2(x)\n",
    "#         x = self.relu2(x)\n",
    "#         return x\n",
    "\n",
    "# #인코더 블럭\n",
    "# class EncoderBlock(nn.Module):\n",
    "#     def __init__(self, in_channels, out_channels):\n",
    "#         super(EncoderBlock,self).__init__()\n",
    "#         self.convblock1 = ConvBlock(in_channels, out_channels)  # 첫 번째 ConvBlock의 in_channels는 입력 이미지의 채널 수와 일치해야 합니다.\n",
    "#         #self.convblock2 = ConvBlock(out_channels, out_channels)  # 두 번째 ConvBlock의 in_channels는 out_channels와 일치해야 합니다.\n",
    "#         self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "#     def forward(self,x):\n",
    "#         x = self.convblock1(x)\n",
    "#         #x = self.convblock2(x)\n",
    "#         p = self.maxpool(x)\n",
    "#         return x , p\n",
    "# #디코더 블럭\n",
    "# #디코더는 업샘플링 이후 스킵연결과 붙어서 convblock을 통과해야함\n",
    "# #skip보다 작은 x x먼저 업샘플링 32 -> 64 , skip과 결합 6464 \n",
    "# class DecoderBlock(nn.Module):\n",
    "#     def __init__(self, channels):\n",
    "#         super(DecoderBlock,self).__init__()\n",
    "#         self.upsample = nn.ConvTranspose2d(channels*2, channels, kernel_size=4, stride=2, padding=1)#x 업샘플링\n",
    "#         self.convblock1 = ConvBlock(channels*2, channels)#차원감소\n",
    "#         #self.convblock2 = ConvBlock(channels, channels)\n",
    "#     def forward(self,x,skip):\n",
    "#         x = self.upsample(x)\n",
    "#         x = torch.cat([x, skip], dim=1)\n",
    "#         x = self.convblock1(x)\n",
    "#         #x = self.convblock2(x)\n",
    "#         return x\n",
    "\n",
    "# ###########################################\n",
    "# class GradReverse(torch.autograd.Function):\n",
    "#     @staticmethod\n",
    "#     def forward(self, x):\n",
    "#         return x.view_as(x)\n",
    "#     @staticmethod\n",
    "#     def backward(self, grad_output): # 역전파 시에 gradient에 음수를 취함\n",
    "#         return (grad_output * -1)\n",
    "\n",
    "# class domain_classifier(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(domain_classifier, self).__init__()\n",
    "#         self.fc1 = nn.Linear(224*224*64, 10)\n",
    "#         self.fc2 = nn.Linear(10, 4) # source = 0, target = 1 회귀 가정\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = x.view(-1, 224*224*64)\n",
    "#         x = GradReverse.apply(x) # gradient reverse\n",
    "#         x = F.leaky_relu(self.fc1(x))\n",
    "#         x = self.fc2(x)\n",
    "        \n",
    "#         return x\n",
    "\n",
    "# ###########################################\n",
    "\n",
    "\n",
    "# #Unet구조 middle의 xm값의 움직임에 주의\n",
    "# class Unet(nn.Module):\n",
    "#     def __init__(self,n_classes):\n",
    "#         super(Unet,self).__init__()\n",
    "#         self.encoder1 = EncoderBlock(3,64)\n",
    "#         self.encoder2 = EncoderBlock(64,128)\n",
    "#         self.encoder3 = EncoderBlock(128,256)\n",
    "#         self.encoder4 = EncoderBlock(256,512)\n",
    "        \n",
    "#         self.middleconv = ConvBlock(512,1024)\n",
    "        \n",
    "        \n",
    "#         self.decoder4 = DecoderBlock(512)\n",
    "#         self.decoder3 = DecoderBlock(256)\n",
    "#         self.decoder2 = DecoderBlock(128)\n",
    "#         self.decoder1 = DecoderBlock(64)\n",
    "#         self.segmap = nn.Conv2d(64,n_classes, kernel_size=1)\n",
    "        \n",
    "#         self.domain_classifier = domain_classifier()\n",
    "                                        \n",
    "\n",
    "#     def forward(self,x):\n",
    "#         x1,p = self.encoder1(x)#3->64   #P:256,256 x1 :512,512\n",
    "#         x2,p = self.encoder2(p)#64->128 #P:128,128 x2:256,256\n",
    "#         x3,p = self.encoder3(p)#128->256#p:64,64 x3:128,128\n",
    "#         x4,p = self.encoder4(p)#256->512#p:32,32 x4:64,64\n",
    "        \n",
    "#         xm = self.middleconv(p)#512->1024#32,32\n",
    "        \n",
    "#         x = self.decoder4(xm,x4)#뉴런:1024->512->512 #출력tensor:64,64\n",
    "#         x = self.decoder3(x,x3)#뉴런:512->256->256 #출력tensor:128,128\n",
    "#         x = self.decoder2(x,x2)#뉴런:256->128->128 #출력tensor:256,256\n",
    "#         x = self.decoder1(x,x1)#뉴런:128->64->64 #출력tensor:512,512\n",
    "\n",
    "#         x_c = self.segmap(x)\n",
    "#         x_d = self.domain_classifier(x)\n",
    "#         # print(\"x_c\", x_c.shape)\n",
    "#         # print(\"x_d\", x_d.shape)\n",
    "#         return x_c, x_d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class GradReverse(torch.autograd.Function):\n",
    "#     @staticmethod\n",
    "#     def forward(self, x):\n",
    "#         return x.view_as(x)\n",
    "#     @staticmethod\n",
    "#     def backward(self, grad_output): # 역전파 시에 gradient에 음수를 취함\n",
    "#         return grad_output * (-1)\n",
    "\n",
    "# class domain_classifier(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(domain_classifier, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(128, 1, kernel_size=1)\n",
    "#         self.fc1 = nn.Linear(224*224*1, 10)\n",
    "#         self.fc2 = nn.Linear(10, 4) # source = 0, target = 1 회귀 가정\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.conv1(x)\n",
    "#         x = x.view(-1, 224*224*1)\n",
    "#         #print(x.shape)\n",
    "#         x = GradReverse.apply(x) # gradient reverse\n",
    "#         x = F.leaky_relu(self.fc1(x))\n",
    "#         x = self.fc2(x)\n",
    "#         #print(x.shape)\n",
    "#         #return torch.sigmoid(x)\n",
    "#         return x\n",
    "\n",
    "# #Unet의 기본이 되는 conv블럭\n",
    "# class ConvBlock(nn.Module):\n",
    "#     def __init__(self, in_channels, out_channels,kernel_size = 3):\n",
    "#         super(ConvBlock, self).__init__()\n",
    "#         self.kernel_size = kernel_size\n",
    "#         self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=1)\n",
    "#         self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "#         self.relu1 = nn.ReLU()\n",
    "        \n",
    "#         self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size, padding=1)  # 여기서 in_channels는 out_channels와 동일해야 합니다.\n",
    "#         self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "#         self.relu2 = nn.ReLU()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.conv1(x)\n",
    "#         x = self.bn1(x)\n",
    "#         x = self.relu1(x)\n",
    "        \n",
    "#         x = self.conv2(x)\n",
    "#         x = self.bn2(x)\n",
    "#         x = self.relu2(x)\n",
    "#         return x\n",
    "# class IdentityBlock(nn.Module):\n",
    "#     def __init__(self, in_channels, mid_channels, out_channels, stride=1):\n",
    "#         super(IdentityBlock, self).__init__()\n",
    "        \n",
    "#         # 1x1 convolution\n",
    "#         self.conv1 = nn.Conv2d(in_channels, mid_channels, kernel_size=1, stride=stride, bias=False)\n",
    "#         self.bn1 = nn.BatchNorm2d(mid_channels)\n",
    "#         self.relu1 = nn.ReLU()\n",
    "\n",
    "#         # 3x3 convolution\n",
    "#         self.conv2 = nn.Conv2d(mid_channels, mid_channels, kernel_size=3, padding=1, bias=False)\n",
    "#         self.bn2 = nn.BatchNorm2d(mid_channels)\n",
    "#         self.relu2 = nn.ReLU()\n",
    "\n",
    "#         # 1x1 convolution\n",
    "#         self.conv3 = nn.Conv2d(mid_channels, out_channels, kernel_size=1, bias=False)\n",
    "#         self.bn3 = nn.BatchNorm2d(out_channels)\n",
    "#         self.relu3 = nn.ReLU()\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         out = self.conv1(x)\n",
    "#         out = self.bn1(out)\n",
    "#         out = self.relu1(out)\n",
    "        \n",
    "#         out = self.conv2(out)\n",
    "#         out = self.bn2(out)\n",
    "#         out = self.relu2(out)\n",
    "         \n",
    "#         out = self.conv3(out)\n",
    "#         out = self.bn3(out)\n",
    "#         out = self.relu3(out)\n",
    "        \n",
    "#         return out\n",
    "# class HeadBlock(IdentityBlock):\n",
    "#     def __init__(self, in_channels, mid_channels, out_channels, stride=1):\n",
    "#         super(HeadBlock, self).__init__(in_channels, mid_channels, out_channels, stride)\n",
    "        \n",
    "#         self.shortcut = nn.Sequential(\n",
    "#             nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "#             nn.BatchNorm2d(out_channels)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         identity = x\n",
    "#         out = super().forward(x)\n",
    "        \n",
    "#         if identity.size() != out.size():\n",
    "#             identity = F.interpolate(identity, size=out.size()[2:])\n",
    "#         identity = self.shortcut(identity)\n",
    "        \n",
    "#         out += identity\n",
    "#         out = self.relu3(out)\n",
    "        \n",
    "#         return out\n",
    "# #인코더 블럭\n",
    "# class Conv2(nn.Module):\n",
    "#     def __init__(self,in_channels, mid_channels, out_channels):\n",
    "#         super(Conv2,self).__init__() \n",
    "#         self.headblock = HeadBlock(in_channels,mid_channels,out_channels)\n",
    "#         self.identityblock1 = IdentityBlock(out_channels,mid_channels,out_channels)\n",
    "#         self.identityblock2 = IdentityBlock(out_channels,mid_channels,out_channels)\n",
    "#         self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "#     def forward(self,x):\n",
    "#         x = self.headblock(x)\n",
    "#         x = self.identityblock1(x)\n",
    "#         x = self.identityblock2(x)\n",
    "#         p = self.maxpool(x)\n",
    "#         return x , p\n",
    "# class Conv3(nn.Module):\n",
    "#     def __init__(self,in_channels, mid_channels, out_channels):\n",
    "#         super(Conv3,self).__init__() \n",
    "#         self.headblock = HeadBlock(in_channels,mid_channels,out_channels)\n",
    "#         self.identityblock1 = IdentityBlock(out_channels,mid_channels,out_channels)\n",
    "#         self.identityblock2 = IdentityBlock(out_channels,mid_channels,out_channels)\n",
    "#         self.identityblock3 = IdentityBlock(out_channels,mid_channels,out_channels)\n",
    "#         self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "#     def forward(self,x):\n",
    "#         x = self.headblock(x)\n",
    "#         x = self.identityblock1(x)\n",
    "#         x = self.identityblock2(x)\n",
    "#         x = self.identityblock3(x)\n",
    "#         p = self.maxpool(x)\n",
    "#         return x , p\n",
    "# class Conv4(nn.Module):\n",
    "#     def __init__(self,in_channels, mid_channels, out_channels):\n",
    "#         super(Conv4,self).__init__() \n",
    "#         self.headblock = HeadBlock(in_channels,mid_channels,out_channels)\n",
    "#         self.identityblock1 = IdentityBlock(out_channels,mid_channels,out_channels)\n",
    "#         self.identityblock2 = IdentityBlock(out_channels,mid_channels,out_channels)\n",
    "#         self.identityblock3 = IdentityBlock(out_channels,mid_channels,out_channels)\n",
    "#         self.identityblock4 = IdentityBlock(out_channels,mid_channels,out_channels)\n",
    "#         self.identityblock5 = IdentityBlock(out_channels,mid_channels,out_channels)\n",
    "#         self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "#     def forward(self,x):\n",
    "#         x = self.headblock(x)\n",
    "#         x = self.identityblock1(x)\n",
    "#         x = self.identityblock2(x)\n",
    "#         x = self.identityblock3(x)\n",
    "#         x = self.identityblock4(x)\n",
    "#         x = self.identityblock5(x)\n",
    "#         p = self.maxpool(x)\n",
    "#         return x , p\n",
    "# class Conv5(nn.Module):\n",
    "#     def __init__(self,in_channels, mid_channels, out_channels):\n",
    "#         super(Conv5,self).__init__() \n",
    "#         self.headblock = HeadBlock(in_channels,mid_channels,out_channels)\n",
    "#         self.identityblock1 = IdentityBlock(out_channels,mid_channels,out_channels)\n",
    "#         self.identityblock2 = IdentityBlock(out_channels,mid_channels,out_channels)\n",
    "#         self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "#     def forward(self,x):\n",
    "#         x = self.headblock(x)\n",
    "#         x = self.identityblock1(x)\n",
    "#         x = self.identityblock2(x)\n",
    "#         p = self.maxpool(x)\n",
    "#         return x , p\n",
    "# #디코더 블럭\n",
    "# class DecoderBlock(nn.Module):\n",
    "#     def __init__(self, channels):\n",
    "#         super(DecoderBlock, self).__init__()\n",
    "#         self.upsample = nn.ConvTranspose2d(channels*2, channels, kernel_size=4, stride=2, padding=1, output_padding=1) # output_padding 추가\n",
    "#         self.convblock1 = ConvBlock(channels*2, channels)\n",
    "\n",
    "#     def forward(self, x, skip):\n",
    "#         x = self.upsample(x)\n",
    "#         if x.size(2) != skip.size(2) or x.size(3) != skip.size(3):\n",
    "#             x = F.interpolate(x, size=(skip.size(2), skip.size(3)))\n",
    "#         x = torch.cat([x, skip], dim=1)\n",
    "#         x = self.convblock1(x)\n",
    "#         return x\n",
    "\n",
    "# #Unet구조 middle의 xm값의 움직임에 주의\n",
    "# class Resnet50_Unet(nn.Module):\n",
    "#     def __init__(self,n_classes):\n",
    "#         super(Resnet50_Unet,self).__init__()\n",
    "#         self.fconv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2)\n",
    "#         self.fbn1 = nn.BatchNorm2d(64)\n",
    "#         self.frelu1 = nn.ReLU()\n",
    "#         self.fconv2 = nn.Conv2d(64, 128, kernel_size=1, stride=1)\n",
    "#         self.fbn2 = nn.BatchNorm2d(128)\n",
    "#         self.frelu2 = nn.ReLU()\n",
    "#         self.fmaxpooling = nn.MaxPool2d(kernel_size=3,stride=2)\n",
    "        \n",
    "#         self.conv2 = Conv2(128,64,256)\n",
    "#         self.conv3 = Conv3(256,128,512)\n",
    "#         self.conv4 = Conv4(512,256,1024)\n",
    "#         self.conv5 = Conv5(1024,512,2048)\n",
    "        \n",
    "#         self.middleconv = ConvBlock(2048,4096)\n",
    "#         self.dropout = nn.Dropout2d(0.4) #\n",
    "           \n",
    "#         self.decoder5 = DecoderBlock(2048)\n",
    "#         self.decoder4 = DecoderBlock(1024)\n",
    "#         self.decoder3 = DecoderBlock(512)\n",
    "#         self.decoder2 = DecoderBlock(256)\n",
    "#         self.decoder1 = DecoderBlock(128)\n",
    "        \n",
    "#         self.segmap = nn.Conv2d(128,n_classes, kernel_size=1)\n",
    "#         self.domain_classifier = domain_classifier()\n",
    "        \n",
    "#     def forward(self,x):\n",
    "#         x = self.fconv1(x)#3->64\n",
    "#         x = self.fbn1(x)\n",
    "#         x = self.frelu1(x)\n",
    "#         x = self.fconv2(x)\n",
    "#         x = self.fbn2(x)\n",
    "#         x1 = self.frelu2(x)\n",
    "#         p = self.fmaxpooling(x)#첫 conv: x0([8, 64, 109, 109]) p([8, 64, 54, 54])\n",
    "#         x2,p = self.conv2(p)#conv2:  x1:([8, 256, 54, 54]) p([8, 256, 26, 26])\n",
    "#         x3,p = self.conv3(p)#conv3:  x2([8, 512, 26, 26]) p([8, 512, 12, 12])\n",
    "#         x4,p = self.conv4(p)#conv4:  x3([8, 1024, 12, 12]) p([8, 1024, 5, 5])\n",
    "#         x5,p = self.conv5(p)#conv5:  x4([8, 2048, 5, 5]) p([8, 2048, 2, 2])\n",
    "        \n",
    "#         xm = self.middleconv(p)#xm([8, 4096, 2, 2])\n",
    "#         xm = self.dropout(xm)\n",
    "        \n",
    "#         x = self.decoder5(xm,x5)#뉴런:2048*2->2048 1\n",
    "#         x = self.decoder4(x,x4)#뉴런:1024*2->1024 \n",
    "#         x = self.decoder3(x,x3) #14\n",
    "#         x = self.decoder2(x,x2)#28\n",
    "#         x = self.decoder1(x,x1)#55\n",
    "        \n",
    "#         x = F.interpolate(x, size=(224, 224))\n",
    "#         x_c = self.segmap(x)\n",
    "#         x_d = self.domain_classifier(x)\n",
    "#         #print(x.shape)\n",
    "#         return x_c,x_d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resnet34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradReverse(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(self, x):\n",
    "        return x.view_as(x)\n",
    "    @staticmethod\n",
    "    def backward(self, grad_output): # 역전파 시에 gradient에 음수를 취함\n",
    "        return grad_output * (-1)\n",
    "\n",
    "class domain_classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(domain_classifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(224*224*64, 50)\n",
    "        self.fc2 = nn.Linear(50, 3) # source = 0, target = 1 회귀 가정\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 224*224*64)\n",
    "        x = GradReverse.apply(x) # gradient reverse\n",
    "        x = F.leaky_relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        #return torch.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "class domain_linear(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(domain_linear, self).__init__()\n",
    "        self.fc1 = nn.Linear(224*224*64, 10)\n",
    "        self.fc2 = nn.Linear(10, 1) # source = 0, target = 1 회귀 가정\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 224*224*64)\n",
    "        x = GradReverse.apply(x) # gradient reverse\n",
    "        x = F.leaky_relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return torch.sigmoid(x)\n",
    "        #return x\n",
    "class IdentityBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(IdentityBlock, self).__init__()\n",
    "        \n",
    "        # 3x3 convolution\n",
    "        self.conv1 = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1, stride=stride, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(in_channels)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        # 3x3 convolution\n",
    "        self.conv2 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        # Skip connection\n",
    "        self.skip = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.skip = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        \n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu1(out)\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        \n",
    "        # Adding the skip connection\n",
    "        out += self.skip(identity)\n",
    "        out = self.relu2(out)\n",
    "        \n",
    "        return out\n",
    "#인코더 블럭\n",
    "class Conv2(nn.Module):\n",
    "    def __init__(self,in_channels, out_channels):\n",
    "        super(Conv2,self).__init__() \n",
    "        self.identityblock1 = IdentityBlock(in_channels,in_channels)\n",
    "        self.identityblock2 = IdentityBlock(in_channels,in_channels)\n",
    "        self.identityblock3 = IdentityBlock(in_channels,out_channels)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2,padding=1)\n",
    "    def forward(self,x):\n",
    "        x = self.identityblock1(x)\n",
    "        x = self.identityblock2(x)\n",
    "        x = self.identityblock3(x)\n",
    "        p = self.maxpool(x)\n",
    "        \n",
    "        return x , p\n",
    "class Conv3(nn.Module):\n",
    "    def __init__(self,in_channels, out_channels):\n",
    "        super(Conv3,self).__init__()         \n",
    "        self.identityblock1 = IdentityBlock(in_channels,in_channels)\n",
    "        self.identityblock2 = IdentityBlock(in_channels,in_channels)\n",
    "        self.identityblock3 = IdentityBlock(in_channels,in_channels)\n",
    "        self.identityblock4 = IdentityBlock(in_channels,out_channels)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2,padding=1)\n",
    "    def forward(self,x):\n",
    "        x = self.identityblock1(x)\n",
    "        x = self.identityblock2(x)\n",
    "        x = self.identityblock3(x)\n",
    "        x = self.identityblock4(x)\n",
    "        p = self.maxpool(x)\n",
    "        \n",
    "        return x , p\n",
    "class Conv4(nn.Module):\n",
    "    def __init__(self,in_channels, out_channels):\n",
    "        super(Conv4,self).__init__()         \n",
    "        self.identityblock1 = IdentityBlock(in_channels,in_channels)\n",
    "        self.identityblock2 = IdentityBlock(in_channels,in_channels)\n",
    "        self.identityblock3 = IdentityBlock(in_channels,in_channels)\n",
    "        self.identityblock4 = IdentityBlock(in_channels,in_channels)\n",
    "        self.identityblock5 = IdentityBlock(in_channels,in_channels)\n",
    "        self.identityblock6 = IdentityBlock(in_channels,out_channels)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2,padding=1)\n",
    "    def forward(self,x):\n",
    "        x = self.identityblock1(x)\n",
    "        x = self.identityblock2(x)\n",
    "        x = self.identityblock3(x)\n",
    "        x = self.identityblock4(x)\n",
    "        x = self.identityblock5(x)\n",
    "        x = self.identityblock6(x)\n",
    "        p = self.maxpool(x)\n",
    "        \n",
    "        return x , p\n",
    "class Conv5(nn.Module):\n",
    "    def __init__(self,in_channels, out_channels):\n",
    "        super(Conv5,self).__init__() \n",
    "        self.identityblock1 = IdentityBlock(in_channels,in_channels)\n",
    "        self.identityblock2 = IdentityBlock(in_channels,in_channels)\n",
    "        self.identityblock3 = IdentityBlock(in_channels,out_channels)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2,padding=1)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.identityblock1(x)\n",
    "        x = self.identityblock2(x)\n",
    "        x = self.identityblock3(x)\n",
    "        p = self.maxpool(x)\n",
    "        \n",
    "        return x , p\n",
    "#디코더 블럭\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.upsample = nn.ConvTranspose2d(channels*2, channels, kernel_size=4, stride=2, padding=1) # output_padding 추가\n",
    "        self.convblock1 = IdentityBlock(channels*2, channels)\n",
    "\n",
    "    def forward(self, x, skip):\n",
    "        x = self.upsample(x)\n",
    "        if x.size(2) != skip.size(2) or x.size(3) != skip.size(3):\n",
    "            x = F.interpolate(x, size=(skip.size(2), skip.size(3)))\n",
    "        x = torch.cat([x, skip], dim=1)\n",
    "        x = self.convblock1(x)\n",
    "        #print(\"x\",x.shape,\"skip: \",skip.shape)\n",
    "        return x\n",
    "\n",
    "#Unet구조 middle의 xm값의 움직임에 주의\n",
    "class Resnet34_Unet(nn.Module):\n",
    "    def __init__(self,n_classes):\n",
    "        super(Resnet34_Unet,self).__init__()\n",
    "        self.fconv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3)\n",
    "        self.fbn1 = nn.BatchNorm2d(64)\n",
    "        self.frelu1 = nn.ReLU()\n",
    "        self.fmaxpooling = nn.MaxPool2d(kernel_size=3,stride=2,padding=1)\n",
    "        \n",
    "        self.conv2 = Conv2(64,128)\n",
    "        self.conv3 = Conv3(128,256)\n",
    "        self.conv4 = Conv4(256,512)\n",
    "        self.conv5 = Conv5(512,1024)\n",
    "        \n",
    "        self.middleconv = IdentityBlock(1024,2048)\n",
    "        self.dropout = nn.Dropout2d(0.1) #\n",
    "           \n",
    "        self.decoder5 = DecoderBlock(1024)\n",
    "        self.decoder4 = DecoderBlock(512)\n",
    "        self.decoder3 = DecoderBlock(256)\n",
    "        self.decoder2 = DecoderBlock(128)\n",
    "        self.decoder1 = DecoderBlock(64)\n",
    "        self.transpose = nn.ConvTranspose2d(64, 64, kernel_size=4, stride=2, padding=1) # output_padding 추가\n",
    "        \n",
    "        self.segmap = nn.Conv2d(64,n_classes, kernel_size=1)\n",
    "        self.domain_classifier = domain_classifier()\n",
    "        self.domain_linear = domain_linear()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.fconv1(x)#3->64\n",
    "        x0 = self.fbn1(x)\n",
    "        x1 = self.frelu1(x)\n",
    "        p = self.fmaxpooling(x1)#첫 conv: x0([8, 64, 109, 109]) p([8, 64, 54, 54])\n",
    "        #print(\"conv1: \",x1.shape, \"maxpooling: \",p.shape)\n",
    "        x2,p = self.conv2(p)\n",
    "        #print(\"conv2: \",x2.shape, \"maxpooling: \",p.shape)\n",
    "        x3,p = self.conv3(p)\n",
    "        #print(\"conv3: \",x3.shape, \"maxpooling: \",p.shape)\n",
    "        x4,p = self.conv4(p)\n",
    "        #print(\"conv4: \",x4.shape, \"maxpooling: \",p.shape)\n",
    "        x5,p = self.conv5(p)\n",
    "        #print(\"conv5: \",x5.shape, \"maxpooling: \",p.shape)\n",
    "        \n",
    "        xm = self.middleconv(p)#xm([8, 4096, 2, 2])\n",
    "        #print(\"xm: \",xm.shape, \"maxpooling: \",p.shape)\n",
    "        xm = self.dropout(xm)\n",
    "        \n",
    "        x = self.decoder5(xm,x5)#뉴런:2048*2->2048 1\n",
    "        x = self.decoder4(x,x4)#뉴런:1024*2->1024 \n",
    "        x = self.decoder3(x,x3) #14\n",
    "        x = self.decoder2(x,x2)#28\n",
    "        x = self.decoder1(x,x1)#55\n",
    "        x = self.transpose(x)\n",
    "        \n",
    "        #print(x.shape)\n",
    "        #x = F.interpolate(x, size=(224, 224))\n",
    "        x_c = self.segmap(x)\n",
    "        #x_d = self.domain_linear(x)\n",
    "        x_d = self.domain_classifier(x)\n",
    "        \n",
    "        \n",
    "        return x_c,x_d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resnet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class GradReverse(torch.autograd.Function):\n",
    "#     @staticmethod\n",
    "#     def forward(self, x):\n",
    "#         return x.view_as(x)\n",
    "#     @staticmethod\n",
    "#     def backward(self, grad_output): # 역전파 시에 gradient에 음수를 취함\n",
    "#         return grad_output * (-1)\n",
    "\n",
    "# class domain_classifier(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(domain_classifier, self).__init__()\n",
    "#         self.fc1 = nn.Linear(224*224*64, 10)\n",
    "#         self.fc2 = nn.Linear(10, 4) # source = 0, target = 1 회귀 가정\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = x.view(-1, 224*224*64)\n",
    "#         x = GradReverse.apply(x) # gradient reverse\n",
    "#         x = F.leaky_relu(self.fc1(x))\n",
    "#         x = self.fc2(x)\n",
    "        \n",
    "#         #return torch.sigmoid(x)\n",
    "#         return x\n",
    "\n",
    "# class IdentityBlock(nn.Module):\n",
    "#     def __init__(self, in_channels, out_channels, stride=1):\n",
    "#         super(IdentityBlock, self).__init__()\n",
    "        \n",
    "#         # 3x3 convolution\n",
    "#         self.conv1 = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1, stride=stride, bias=False)\n",
    "#         self.bn1 = nn.BatchNorm2d(in_channels)\n",
    "#         self.relu1 = nn.ReLU()\n",
    "        \n",
    "#         # 3x3 convolution\n",
    "#         self.conv2 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False)\n",
    "#         self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "#         self.relu2 = nn.ReLU()\n",
    "        \n",
    "#         # Skip connection\n",
    "#         self.skip = nn.Sequential()\n",
    "#         if stride != 1 or in_channels != out_channels:\n",
    "#             self.skip = nn.Sequential(\n",
    "#                 nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "#                 nn.BatchNorm2d(out_channels)\n",
    "#             )\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         identity = x\n",
    "        \n",
    "#         out = self.conv1(x)\n",
    "#         out = self.bn1(out)\n",
    "#         out = self.relu1(out)\n",
    "        \n",
    "#         out = self.conv2(out)\n",
    "#         out = self.bn2(out)\n",
    "        \n",
    "#         # Adding the skip connection\n",
    "#         out += self.skip(identity)\n",
    "#         out = self.relu2(out)\n",
    "        \n",
    "#         return out\n",
    "\n",
    "\n",
    "# #인코더 블럭\n",
    "# class Conv2(nn.Module):\n",
    "#     def __init__(self,in_channels, out_channels):\n",
    "#         super(Conv2,self).__init__() \n",
    "#         self.identityblock1 = IdentityBlock(in_channels,in_channels)\n",
    "#         self.identityblock2 = IdentityBlock(in_channels,out_channels)\n",
    "#         self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2,padding=1)\n",
    "#     def forward(self,x):\n",
    "#         x = self.identityblock1(x)\n",
    "#         x = self.identityblock2(x)\n",
    "#         p = self.maxpool(x)\n",
    "        \n",
    "#         return x , p\n",
    "# class Conv3(nn.Module):\n",
    "#     def __init__(self,in_channels, out_channels):\n",
    "#         super(Conv3,self).__init__()         \n",
    "#         self.identityblock1 = IdentityBlock(in_channels,in_channels)\n",
    "#         self.identityblock2 = IdentityBlock(in_channels,out_channels)\n",
    "#         self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2,padding=1)\n",
    "#     def forward(self,x):\n",
    "#         x = self.identityblock1(x)\n",
    "#         x = self.identityblock2(x)\n",
    "#         p = self.maxpool(x)\n",
    "        \n",
    "#         return x , p\n",
    "# class Conv4(nn.Module):\n",
    "#     def __init__(self,in_channels, out_channels):\n",
    "#         super(Conv4,self).__init__()         \n",
    "#         self.identityblock1 = IdentityBlock(in_channels,in_channels)\n",
    "#         self.identityblock2 = IdentityBlock(in_channels,out_channels)\n",
    "#         self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2,padding=1)\n",
    "#     def forward(self,x):\n",
    "#         x = self.identityblock1(x)\n",
    "#         x = self.identityblock2(x)\n",
    "#         p = self.maxpool(x)\n",
    "        \n",
    "#         return x , p\n",
    "# class Conv5(nn.Module):\n",
    "#     def __init__(self,in_channels, out_channels):\n",
    "#         super(Conv5,self).__init__() \n",
    "#         self.identityblock1 = IdentityBlock(in_channels,in_channels)\n",
    "#         self.identityblock2 = IdentityBlock(in_channels,out_channels)\n",
    "#         self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2,padding=1)\n",
    "        \n",
    "#     def forward(self,x):\n",
    "#         x = self.identityblock1(x)\n",
    "#         x = self.identityblock2(x)\n",
    "#         p = self.maxpool(x)\n",
    "        \n",
    "#         return x , p\n",
    "# #디코더 블럭\n",
    "# class DecoderBlock(nn.Module):\n",
    "#     def __init__(self, channels):\n",
    "#         super(DecoderBlock, self).__init__()\n",
    "#         self.upsample = nn.ConvTranspose2d(channels*2, channels, kernel_size=4, stride=2, padding=1) # output_padding 추가\n",
    "#         self.convblock1 = IdentityBlock(channels*2, channels)\n",
    "\n",
    "#     def forward(self, x, skip):\n",
    "#         x = self.upsample(x)\n",
    "#         if x.size(2) != skip.size(2) or x.size(3) != skip.size(3):\n",
    "#             x = F.interpolate(x, size=(skip.size(2), skip.size(3)))\n",
    "#         x = torch.cat([x, skip], dim=1)\n",
    "#         x = self.convblock1(x)\n",
    "#         #print(\"x\",x.shape,\"skip: \",skip.shape)\n",
    "#         return x\n",
    "\n",
    "# #Unet구조 middle의 xm값의 움직임에 주의\n",
    "# class Resnet18_Unet(nn.Module):\n",
    "#     def __init__(self,n_classes):\n",
    "#         super(Resnet18_Unet,self).__init__()\n",
    "#         self.fconv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3)\n",
    "#         self.fbn1 = nn.BatchNorm2d(64)\n",
    "#         self.frelu1 = nn.ReLU()\n",
    "#         self.fmaxpooling = nn.MaxPool2d(kernel_size=3,stride=2,padding=1)\n",
    "        \n",
    "#         self.conv2 = Conv2(64,128)\n",
    "#         self.conv3 = Conv3(128,256)\n",
    "#         self.conv4 = Conv4(256,512)\n",
    "#         self.conv5 = Conv5(512,1024)\n",
    "        \n",
    "#         self.middleconv = IdentityBlock(1024,2048)\n",
    "#         self.dropout = nn.Dropout2d(0.1) #\n",
    "           \n",
    "#         self.decoder5 = DecoderBlock(1024)\n",
    "#         self.decoder4 = DecoderBlock(512)\n",
    "#         self.decoder3 = DecoderBlock(256)\n",
    "#         self.decoder2 = DecoderBlock(128)\n",
    "#         self.decoder1 = DecoderBlock(64)\n",
    "#         self.transpose = nn.ConvTranspose2d(64, 64, kernel_size=4, stride=2, padding=1) # output_padding 추가\n",
    "        \n",
    "#         self.segmap = nn.Conv2d(64,n_classes, kernel_size=1)\n",
    "#         self.domain_classifier = domain_classifier()\n",
    "        \n",
    "#     def forward(self,x):\n",
    "#         x = self.fconv1(x)#3->64\n",
    "#         x0 = self.fbn1(x)\n",
    "#         x1 = self.frelu1(x)\n",
    "#         p = self.fmaxpooling(x1)#첫 conv: x0([8, 64, 109, 109]) p([8, 64, 54, 54])\n",
    "#         #print(\"conv1: \",x1.shape, \"maxpooling: \",p.shape)\n",
    "#         x2,p = self.conv2(p)\n",
    "#         #print(\"conv2: \",x2.shape, \"maxpooling: \",p.shape)\n",
    "#         x3,p = self.conv3(p)\n",
    "#         #print(\"conv3: \",x3.shape, \"maxpooling: \",p.shape)\n",
    "#         x4,p = self.conv4(p)\n",
    "#         #print(\"conv4: \",x4.shape, \"maxpooling: \",p.shape)\n",
    "#         x5,p = self.conv5(p)\n",
    "#         #print(\"conv5: \",x5.shape, \"maxpooling: \",p.shape)\n",
    "        \n",
    "#         xm = self.middleconv(p)#xm([8, 4096, 2, 2])\n",
    "#         #print(\"xm: \",xm.shape, \"maxpooling: \",p.shape)\n",
    "#         xm = self.dropout(xm)\n",
    "        \n",
    "#         x = self.decoder5(xm,x5)#뉴런:2048*2->2048 1\n",
    "#         x = self.decoder4(x,x4)#뉴런:1024*2->1024 \n",
    "#         x = self.decoder3(x,x3) #14\n",
    "#         x = self.decoder2(x,x2)#28\n",
    "#         x = self.decoder1(x,x1)#55\n",
    "#         x = self.transpose(x)\n",
    "        \n",
    "#         #print(x.shape)\n",
    "#         #x = F.interpolate(x, size=(224, 224))\n",
    "#         x_c = self.segmap(x)\n",
    "#         x_d = self.domain_classifier(x)\n",
    "        \n",
    "#         return x_c,x_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# # 저장된 class_weights를 불러옵니다.\n",
    "# class_weights_path = 'CLASS_WEIGHTS.pkl'\n",
    "\n",
    "# with open(class_weights_path, 'rb') as file:\n",
    "#     CLASS_WEIGHTS = pickle.load(file)\n",
    "\n",
    "# print(CLASS_WEIGHTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function과 optimizer 정의\n",
    "\n",
    "class DANN_Loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DANN_Loss, self).__init__()\n",
    "\n",
    "        #self.CE = nn.CrossEntropyLoss(weight=CLASS_WEIGHTS) # 0~9 class 분류용\n",
    "        self.CE = nn.CrossEntropyLoss()\n",
    "        #self.BCE = nn.BCELoss() # 도메인 분류용\n",
    "        \n",
    "    # result : DANN_CNN에서 반환된 값\n",
    "    # label : 숫자 0 ~ 9에 대한 라벨\n",
    "    # domain_num : 0(source) or 1(target)\n",
    "    def forward(self, result, label, domain_num, alpha = 1):\n",
    "        label_logits, domain_logits = result # DANN_CNN의 결과\n",
    "\n",
    "        batch_size = domain_logits.shape[0]\n",
    "\n",
    "        # print(\"segment_mask : \", label.shape)\n",
    "        # print(\"domain_answer : \", domain_target.shape)\n",
    "        segment_loss = self.CE(label_logits, label) # class 분류 loss\n",
    "\n",
    "        # domain_target = torch.FloatTensor([domain_num] * batch_size).unsqueeze(1).to(device)\n",
    "        # domain_loss = self.BCE(domain_logits, domain_target)\n",
    "\n",
    "        domain_target = torch.LongTensor([domain_num] * batch_size).to(device)\n",
    "        domain_loss = self.CE(domain_logits, domain_target) # domain 분류 loss\n",
    "        \n",
    "        loss = segment_loss + alpha * domain_loss\n",
    "\n",
    "        return loss, segment_loss, domain_loss\n",
    "    \n",
    "\n",
    "loss_fn = DANN_Loss().to(device)\n",
    "\n",
    "#criterion =nn.CrossEntropyLoss()\n",
    "#domain_criterion = nn.BCELoss()\n",
    "#criterion = nn.CrossEntropyLoss(weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.001\n",
    "EP = 10\n",
    "BATCH_SIZE = 16\n",
    "ACCMULATION_STEP = 1 \n",
    "N_CLASSES = 13 #IoU 점수측정하기 위한 클래스의 개수\n",
    "ALPHA = 0.0001\n",
    "Label = [0,1,2]\n",
    "N_LABELS = len(Label)\n",
    "# model 초기화\n",
    "#model = Resnet18_Unet(n_classes = N_CLASSES).to(device)\n",
    "model = Resnet34_Unet(n_classes = N_CLASSES).to(device)\n",
    "#model = Resnet50_Unet(n_classes = N_CLASSES).to(device)\n",
    "#model = Unet(n_classes = N_CLASSES).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "optimizer.zero_grad() \n",
    "\n",
    "source_dataset = CustomDataset(csv_file='./data/896_csv/train_source.csv', transform=transform)\n",
    "source_dataloader = DataLoader(source_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_target_dataset = CustomDataset(csv_file='./data/896_csv/val_source.csv', transform=transform)\n",
    "#val_target_dataset = CustomDataset(csv_file='./data/896_csv/val_source_CL.csv', transform=transform)\n",
    "val_target_dataloader = DataLoader(val_target_dataset, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import os\n",
    "# import cv2\n",
    "# import matplotlib.pyplot as plt\n",
    "# a1 = pd.read_csv(\"./data/6_fish_source.csv\")\n",
    "# p = \"./data/224/\"\n",
    "# a2 = os.path.join(p, a1.iloc[3,2])\n",
    "# a3 = cv2.imread(a2)\n",
    "# a4 = cv2.cvtColor(a3, cv2.COLOR_BGR2GRAY)\n",
    "# a4 = np.round(a4).astype(np.uint8)\n",
    "# a5 = a4*20\n",
    "\n",
    "# plt.imshow(a5, cmap='gray')\n",
    "# plt.axis('off')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "# #torch.cuda.empty_cache()\n",
    "# # import wandb\n",
    "\n",
    "\n",
    "# # wandb.init(\n",
    "# #     # set the wandb project where this run will be logged\n",
    "# #     project=\"practice_10_27_4d_res18\",\n",
    "    \n",
    "# #     # track hyperparameters and run metadata\n",
    "# #     config={\n",
    "# #     \"learning_rate\": LR,\n",
    "# #     \"architecture\": \"CNN\",\n",
    "# #     \"dataset\": \"Samsung\",\n",
    "# #     \"epochs\": EP,\n",
    "# #     }\n",
    "# # )\n",
    "\n",
    "# for epoch in range(EP):\n",
    "#     # 클래스별 IoU를 누적할 리스트 초기화\n",
    "#     train_class_ious = []\n",
    "#     fish_train_class_ious = []\n",
    "#     # 학습\n",
    "#     model.train()\n",
    "#     epoch_loss = 0\n",
    "#     seg_loss = 0\n",
    "#     dom_loss = 0\n",
    "\n",
    "#     for source_images, source_masks in tqdm(source_dataloader):\n",
    "#         label = random.randint(0,3)\n",
    "#         source_images, source_masks = apply_fisheye_distortion(source_images, source_masks, label)\n",
    "#         source_images = source_images.float().to(device)\n",
    "#         source_masks = source_masks.long().to(device)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         source_outputs = model(source_images)\n",
    "\n",
    "#         source_loss, segment_loss, domain_loss = loss_fn(source_outputs, source_masks, label, alpha = ALPHA)\n",
    "\n",
    "#         loss = source_loss\n",
    "#         epoch_loss += loss.item()\n",
    "#         seg_loss += segment_loss.item()\n",
    "#         dom_loss += domain_loss.item()\n",
    "\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         #scheduler.step()\n",
    "#                 # train 클래스별 IoU 계산\n",
    "#         source_outputs = torch.softmax(source_outputs[0], dim=1).cpu()\n",
    "#         source_outputs = torch.argmax(source_outputs, dim=1).numpy()\n",
    "\n",
    "#         for class_id in range(N_CLASSES):\n",
    "#             iou = calculate_iou_per_class(np.array(source_masks.cpu()), np.array(source_outputs), class_id)\n",
    "#             train_class_ious.append(iou)\n",
    "\n",
    "#     train_class_ious = np.array(train_class_ious).reshape(N_CLASSES, -1)\n",
    "#     train_class_ious = np.mean(train_class_ious, axis=1)\n",
    "#     print(\"--IoU Scores Train--\")\n",
    "#     for class_id, iou in enumerate(train_class_ious):\n",
    "#         print(f'Class{class_id}: {iou:.4f}', end=\" \")\n",
    "#         if (class_id+1) % 7 == 0:\n",
    "#             print()\n",
    "\n",
    "#     # mIoU 계산\n",
    "#     train_mIoU = np.mean(train_class_ious)\n",
    "\n",
    "#     # 에폭마다 결과 출력 \n",
    "#     print(f\"\\nEpoch{epoch+1}\")\n",
    "#     print(f\"Train seg Loss: {(seg_loss/len(source_dataloader))}\")\n",
    "#     print(f\"Train dom Loss: {(dom_loss/len(source_dataloader))}\")\n",
    "#     print(f\"Train Loss: {(epoch_loss/len(source_dataloader))}\")\n",
    "#     print(f\"Train mIoU: {train_mIoU}\" )\n",
    "#     print(\"___________________________________________________________________________________________\\n\")\n",
    "\n",
    "\n",
    "#     ################################################################\n",
    "#     # 클래스별 IoU를 누적할 리스트 초기화\n",
    "#     val_class_ious = []\n",
    "#     fish_val_class_ious = []\n",
    "#     val_epoch_loss = 0\n",
    "#     val_seg_loss = 0\n",
    "#     val_dom_loss = 0\n",
    "#     # 학습\n",
    "#     with torch.no_grad():\n",
    "#         model.eval()\n",
    "\n",
    "#         for target_images, target_masks in tqdm(val_target_dataloader):\n",
    "#             label = 2.5\n",
    "#             target_images, target_masks = apply_fisheye_distortion(target_images, target_masks, label)\n",
    "#             target_images = target_images.float().to(device)\n",
    "#             target_masks = target_masks.long().to(device)\n",
    "\n",
    "#             target_outputs = model(target_images)\n",
    "\n",
    "#             target_loss, val_segment_loss, val_domain_loss = loss_fn(target_outputs, target_masks, 0, alpha = ALPHA)\n",
    "\n",
    "#             loss = target_loss\n",
    "\n",
    "#             val_epoch_loss += loss.item()\n",
    "#             val_seg_loss += val_segment_loss.item()\n",
    "#             val_dom_loss += val_domain_loss.item()\n",
    "\n",
    "#             # train 클래스별 IoU 계산\n",
    "#             target_outputs = torch.softmax(target_outputs[0], dim=1).cpu()\n",
    "#             target_outputs = torch.argmax(target_outputs, dim=1).numpy()\n",
    "\n",
    "#             for class_id in range(N_CLASSES):\n",
    "#                 iou = calculate_iou_per_class(np.array(target_masks.cpu()), np.array(target_outputs), class_id)\n",
    "#                 fish_val_class_ious.append(iou)\n",
    "\n",
    "#     fish_val_class_ious = np.array(fish_val_class_ious).reshape(N_CLASSES, -1)\n",
    "#     fish_val_class_ious = np.mean(fish_val_class_ious, axis=1)\n",
    "#     print()\n",
    "#     print(\"--IoU Scores Fish val--\")\n",
    "#     for class_id, iou in enumerate(fish_val_class_ious):\n",
    "#         print(f'Class{class_id}: {iou:.4f}', end=\" \")\n",
    "#         if (class_id+1) % 7 == 0:\n",
    "#             print()\n",
    "\n",
    "#     # mIoU 계산\n",
    "#     fish_val_mIoU = np.mean(fish_val_class_ious)\n",
    "\n",
    "#     # 에폭마다 결과 출력 \n",
    "#     print(f\"\\nEpoch{epoch+1}\")\n",
    "#     print(f\"Valid seg Loss: {(val_seg_loss/len(val_target_dataloader))}\")\n",
    "#     print(f\"Valid dom Loss: {(val_dom_loss/len(val_target_dataloader))}\")\n",
    "#     print(f\"Valid Loss: {(val_epoch_loss/len(val_target_dataloader))}\")\n",
    "#     print(f\"Valid mIoU: {fish_val_mIoU}\" )\n",
    "#     print(\"___________________________________________________________________________________________\\n\")\n",
    "\n",
    "# #     # log metrics to wandb\n",
    "# #     wandb.log({\"train score\": train_mIoU})\n",
    "# #     wandb.log({\"val score\": fish_val_mIoU})\n",
    "# #     wandb.log({\"train loss\": (epoch_loss/len(source_dataloader))})\n",
    "# #     wandb.log({\"val loss\": (val_epoch_loss/len(val_target_dataloader))})\n",
    "    \n",
    "    \n",
    "# # # [optional] finish the wandb run, necessary in notebooks\n",
    "# # wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1:   0%|          | 0/138 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1: 100%|██████████| 138/138 [05:51<00:00,  2.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label_0: IoU Scores Train\n",
      "Class00: 0.2452 Class01: 0.3327 Class02: 0.0001 Class03: 0.2068 Class04: 0.0000 Class05: 0.0000 Class06: 0.0000 \n",
      "Class07: 0.0000 Class08: 0.2341 Class09: 0.4107 Class10: 0.0000 Class11: 0.0000 Class12: 0.0996 \n",
      "Label_1: IoU Scores Train\n",
      "Class00: 0.2260 Class01: 0.3123 Class02: 0.0001 Class03: 0.1898 Class04: 0.0000 Class05: 0.0000 Class06: 0.0000 \n",
      "Class07: 0.0000 Class08: 0.2130 Class09: 0.3553 Class10: 0.0000 Class11: 0.0002 Class12: 0.0943 \n",
      "Label_2: IoU Scores Train\n",
      "Class00: 0.2149 Class01: 0.2952 Class02: 0.0000 Class03: 0.1757 Class04: 0.0000 Class05: 0.0000 Class06: 0.0000 \n",
      "Class07: 0.0000 Class08: 0.1924 Class09: 0.3209 Class10: 0.0000 Class11: 0.0000 Class12: 0.0861 \n",
      "Train seg Loss: 0.8615133053294702 Train dom Loss: 401.9903159603983\n",
      "Train Loss: 1.2635036370748483\n",
      "Train mIoU: 0.10783672057900597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]/tmp/ipykernel_1181295/288732703.py:26: DeprecationWarning: an integer is required (got type float).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  domain_target = torch.LongTensor([domain_num] * batch_size).to(device)\n",
      "100%|██████████| 30/30 [00:22<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--IoU Scores Fish val--\n",
      "Class00: 0.9804 Class01: 0.7037 Class02: 0.0000 Class03: 0.4103 Class04: 0.0323 Class05: 0.0000 Class06: 0.0000 \n",
      "Class07: 0.0000 Class08: 0.5321 Class09: 0.8749 Class10: 0.0000 Class11: 0.0000 Class12: 0.1110 \n",
      "Epoch1\n",
      "Valid Seg Loss: 0.4117868095636368 Valid dom Loss: 56.55978488922119\n",
      "Valid Loss: 0.4683465987443924\n",
      "Valid mIoU: 0.28034918133388015\n",
      "___________________________________________________________________________________________\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 2: 100%|██████████| 138/138 [05:39<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label_0: IoU Scores Train\n",
      "Class00: 0.2777 Class01: 0.4692 Class02: 0.0001 Class03: 0.3582 Class04: 0.0007 Class05: 0.0000 Class06: 0.0000 \n",
      "Class07: 0.0000 Class08: 0.3867 Class09: 0.5125 Class10: 0.0000 Class11: 0.0000 Class12: 0.2533 \n",
      "Label_1: IoU Scores Train\n",
      "Class00: 0.2434 Class01: 0.4158 Class02: 0.0000 Class03: 0.3146 Class04: 0.0007 Class05: 0.0000 Class06: 0.0000 \n",
      "Class07: 0.0000 Class08: 0.3383 Class09: 0.4290 Class10: 0.0000 Class11: 0.0000 Class12: 0.2335 \n",
      "Label_2: IoU Scores Train\n",
      "Class00: 0.2267 Class01: 0.3832 Class02: 0.0000 Class03: 0.2813 Class04: 0.0005 Class05: 0.0000 Class06: 0.0000 \n",
      "Class07: 0.0000 Class08: 0.3101 Class09: 0.3825 Class10: 0.0000 Class11: 0.0000 Class12: 0.2238 \n",
      "Train seg Loss: 0.4311309996722401 Train dom Loss: 54.74163889104517\n",
      "Train Loss: 0.48587264120578766\n",
      "Train mIoU: 0.15492028079968212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:23<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--IoU Scores Fish val--\n",
      "Class00: 0.9717 Class01: 0.6706 Class02: 0.0000 Class03: 0.4598 Class04: 0.0010 Class05: 0.0000 Class06: 0.0000 \n",
      "Class07: 0.0005 Class08: 0.5893 Class09: 0.8908 Class10: 0.0000 Class11: 0.0000 Class12: 0.4063 \n",
      "Epoch2\n",
      "Valid Seg Loss: 0.42054875393708546 Valid dom Loss: 394.8579416910807\n",
      "Valid Loss: 0.8154067118962606\n",
      "Valid mIoU: 0.3069234192351256\n",
      "___________________________________________________________________________________________\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 3: 100%|██████████| 138/138 [05:52<00:00,  2.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label_0: IoU Scores Train\n",
      "Class00: 0.2740 Class01: 0.4571 Class02: 0.0000 Class03: 0.3344 Class04: 0.0000 Class05: 0.0000 Class06: 0.0002 \n",
      "Class07: 0.0084 Class08: 0.3680 Class09: 0.4950 Class10: 0.0000 Class11: 0.0000 Class12: 0.2364 \n",
      "Label_1: IoU Scores Train\n",
      "Class00: 0.2422 Class01: 0.4080 Class02: 0.0000 Class03: 0.2912 Class04: 0.0000 Class05: 0.0000 Class06: 0.0002 \n",
      "Class07: 0.0075 Class08: 0.3193 Class09: 0.4173 Class10: 0.0000 Class11: 0.0000 Class12: 0.2214 \n",
      "Label_2: IoU Scores Train\n",
      "Class00: 0.2260 Class01: 0.3757 Class02: 0.0000 Class03: 0.2686 Class04: 0.0000 Class05: 0.0000 Class06: 0.0002 \n",
      "Class07: 0.0055 Class08: 0.2892 Class09: 0.3711 Class10: 0.0000 Class11: 0.0000 Class12: 0.2096 \n",
      "Train seg Loss: 0.48374193258907483 Train dom Loss: 164.05068100902582\n",
      "Train Loss: 0.6477926209353019\n",
      "Train mIoU: 0.14939440421318412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:32<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--IoU Scores Fish val--\n",
      "Class00: 0.9854 Class01: 0.7333 Class02: 0.0000 Class03: 0.4704 Class04: 0.0000 Class05: 0.0000 Class06: 0.0000 \n",
      "Class07: 0.0262 Class08: 0.5754 Class09: 0.8941 Class10: 0.0000 Class11: 0.0000 Class12: 0.3532 \n",
      "Epoch3\n",
      "Valid Seg Loss: 0.3733426183462143 Valid dom Loss: 113.52785669962564\n",
      "Valid Loss: 0.48687047759691876\n",
      "Valid mIoU: 0.3106107147473244\n",
      "___________________________________________________________________________________________\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 4: 100%|██████████| 138/138 [05:41<00:00,  2.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label_0: IoU Scores Train\n",
      "Class00: 0.2825 Class01: 0.4953 Class02: 0.0017 Class03: 0.3790 Class04: 0.0018 Class05: 0.0000 Class06: 0.0027 \n",
      "Class07: 0.0469 Class08: 0.4087 Class09: 0.5195 Class10: 0.0000 Class11: 0.0000 Class12: 0.3330 \n",
      "Label_1: IoU Scores Train\n",
      "Class00: 0.2438 Class01: 0.4251 Class02: 0.0012 Class03: 0.3294 Class04: 0.0012 Class05: 0.0000 Class06: 0.0020 \n",
      "Class07: 0.0374 Class08: 0.3497 Class09: 0.4325 Class10: 0.0000 Class11: 0.0000 Class12: 0.3069 \n",
      "Label_2: IoU Scores Train\n",
      "Class00: 0.2259 Class01: 0.3804 Class02: 0.0009 Class03: 0.2976 Class04: 0.0011 Class05: 0.0000 Class06: 0.0017 \n",
      "Class07: 0.0362 Class08: 0.3184 Class09: 0.3834 Class10: 0.0000 Class11: 0.0000 Class12: 0.2891 \n",
      "Train seg Loss: 0.3704761174565928 Train dom Loss: 1.7199091710884575\n",
      "Train Loss: 0.3721960266143228\n",
      "Train mIoU: 0.16756845554203945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:31<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--IoU Scores Fish val--\n",
      "Class00: 0.9879 Class01: 0.7475 Class02: 0.0000 Class03: 0.4558 Class04: 0.0000 Class05: 0.0000 Class06: 0.0000 \n",
      "Class07: 0.0420 Class08: 0.5844 Class09: 0.8846 Class10: 0.0000 Class11: 0.0000 Class12: 0.4574 \n",
      "Epoch4\n",
      "Valid Seg Loss: 0.36057326893011726 Valid dom Loss: 198.98553759256998\n",
      "Valid Loss: 0.5595588107903798\n",
      "Valid mIoU: 0.31996720759414515\n",
      "___________________________________________________________________________________________\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 5: 100%|██████████| 138/138 [05:51<00:00,  2.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label_0: IoU Scores Train\n",
      "Class00: 0.2310 Class01: 0.2223 Class02: 0.0033 Class03: 0.1558 Class04: 0.0027 Class05: 0.0004 Class06: 0.0009 \n",
      "Class07: 0.0174 Class08: 0.2300 Class09: 0.3293 Class10: 0.0000 Class11: 0.0000 Class12: 0.1176 \n",
      "Label_1: IoU Scores Train\n",
      "Class00: 0.2152 Class01: 0.1923 Class02: 0.0027 Class03: 0.1408 Class04: 0.0024 Class05: 0.0003 Class06: 0.0005 \n",
      "Class07: 0.0143 Class08: 0.2061 Class09: 0.2755 Class10: 0.0000 Class11: 0.0001 Class12: 0.1065 \n",
      "Label_2: IoU Scores Train\n",
      "Class00: 0.2066 Class01: 0.1748 Class02: 0.0020 Class03: 0.1335 Class04: 0.0025 Class05: 0.0003 Class06: 0.0004 \n",
      "Class07: 0.0124 Class08: 0.1886 Class09: 0.2517 Class10: 0.0002 Class11: 0.0001 Class12: 0.1001 \n",
      "Train seg Loss: 1.0704454444168847 Train dom Loss: 4448.645983166831\n",
      "Train Loss: 5.5190916440769096\n",
      "Train mIoU: 0.09078533289694424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:10<00:00,  2.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--IoU Scores Fish val--\n",
      "Class00: 0.8096 Class01: 0.0051 Class02: 0.0000 Class03: 0.0437 Class04: 0.0000 Class05: 0.0000 Class06: 0.0000 \n",
      "Class07: 0.0000 Class08: 0.3332 Class09: 0.4789 Class10: 0.0000 Class11: 0.0000 Class12: 0.0011 \n",
      "Epoch5\n",
      "Valid Seg Loss: 0.852304345369339 Valid dom Loss: 25649.73515625\n",
      "Valid Loss: 26.502040735880534\n",
      "Valid mIoU: 0.12858713058474047\n",
      "___________________________________________________________________________________________\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 6: 100%|██████████| 138/138 [05:04<00:00,  2.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label_0: IoU Scores Train\n",
      "Class00: 0.1966 Class01: 0.1363 Class02: 0.0026 Class03: 0.0505 Class04: 0.0035 Class05: 0.0019 Class06: 0.0007 \n",
      "Class07: 0.0014 Class08: 0.0709 Class09: 0.2728 Class10: 0.0004 Class11: 0.0002 Class12: 0.0183 \n",
      "Label_1: IoU Scores Train\n",
      "Class00: 0.1917 Class01: 0.1270 Class02: 0.0015 Class03: 0.0491 Class04: 0.0018 Class05: 0.0020 Class06: 0.0006 \n",
      "Class07: 0.0015 Class08: 0.0740 Class09: 0.2448 Class10: 0.0003 Class11: 0.0003 Class12: 0.0176 \n",
      "Label_2: IoU Scores Train\n",
      "Class00: 0.1839 Class01: 0.1195 Class02: 0.0018 Class03: 0.0490 Class04: 0.0032 Class05: 0.0019 Class06: 0.0008 \n",
      "Class07: 0.0017 Class08: 0.0711 Class09: 0.2144 Class10: 0.0003 Class11: 0.0003 Class12: 0.0165 \n",
      "Train seg Loss: 3.0572765439316845 Train dom Loss: 140895.38434648744\n",
      "Train Loss: 143.95266845145665\n",
      "Train mIoU: 0.05468528587318086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:10<00:00,  2.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--IoU Scores Fish val--\n",
      "Class00: 0.6729 Class01: 0.0000 Class02: 0.0000 Class03: 0.0243 Class04: 0.0000 Class05: 0.0000 Class06: 0.0000 \n",
      "Class07: 0.0000 Class08: 0.0086 Class09: 0.4985 Class10: 0.0000 Class11: 0.0000 Class12: 0.0000 \n",
      "Epoch6\n",
      "Valid Seg Loss: 1.1486278255780538 Valid dom Loss: 121862.18932291666\n",
      "Valid Loss: 123.01082229614258\n",
      "Valid mIoU: 0.09264647231068827\n",
      "___________________________________________________________________________________________\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 7: 100%|██████████| 138/138 [05:04<00:00,  2.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label_0: IoU Scores Train\n",
      "Class00: 0.1995 Class01: 0.0374 Class02: 0.0012 Class03: 0.0719 Class04: 0.0017 Class05: 0.0009 Class06: 0.0004 \n",
      "Class07: 0.0012 Class08: 0.0468 Class09: 0.2809 Class10: 0.0001 Class11: 0.0000 Class12: 0.0289 \n",
      "Label_1: IoU Scores Train\n",
      "Class00: 0.1922 Class01: 0.0343 Class02: 0.0023 Class03: 0.0668 Class04: 0.0016 Class05: 0.0008 Class06: 0.0004 \n",
      "Class07: 0.0015 Class08: 0.0500 Class09: 0.2340 Class10: 0.0001 Class11: 0.0001 Class12: 0.0253 \n",
      "Label_2: IoU Scores Train\n",
      "Class00: 0.1879 Class01: 0.0353 Class02: 0.0024 Class03: 0.0665 Class04: 0.0019 Class05: 0.0009 Class06: 0.0003 \n",
      "Class07: 0.0009 Class08: 0.0485 Class09: 0.2169 Class10: 0.0000 Class11: 0.0001 Class12: 0.0283 \n",
      "Train seg Loss: 3.342294332485844 Train dom Loss: 89198.17110722678\n",
      "Train Loss: 92.54046980741519\n",
      "Train mIoU: 0.04795004784361812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:10<00:00,  2.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--IoU Scores Fish val--\n",
      "Class00: 0.8098 Class01: 0.1827 Class02: 0.0000 Class03: 0.1844 Class04: 0.0000 Class05: 0.0000 Class06: 0.0000 \n",
      "Class07: 0.0000 Class08: 0.0400 Class09: 0.6861 Class10: 0.0000 Class11: 0.0000 Class12: 0.0886 \n",
      "Epoch7\n",
      "Valid Seg Loss: 3.795060125986735 Valid dom Loss: 57.64165496826172\n",
      "Valid Loss: 3.852701767285665\n",
      "Valid mIoU: 0.15320191841355124\n",
      "___________________________________________________________________________________________\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 8: 100%|██████████| 138/138 [05:03<00:00,  2.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label_0: IoU Scores Train\n",
      "Class00: 0.2295 Class01: 0.0989 Class02: 0.0067 Class03: 0.1504 Class04: 0.0052 Class05: 0.0004 Class06: 0.0003 \n",
      "Class07: 0.0002 Class08: 0.0874 Class09: 0.4079 Class10: 0.0005 Class11: 0.0001 Class12: 0.0899 \n",
      "Label_1: IoU Scores Train\n",
      "Class00: 0.2180 Class01: 0.0920 Class02: 0.0059 Class03: 0.1468 Class04: 0.0061 Class05: 0.0004 Class06: 0.0003 \n",
      "Class07: 0.0003 Class08: 0.0961 Class09: 0.3413 Class10: 0.0003 Class11: 0.0004 Class12: 0.0811 \n",
      "Label_2: IoU Scores Train\n",
      "Class00: 0.2130 Class01: 0.0885 Class02: 0.0069 Class03: 0.1410 Class04: 0.0064 Class05: 0.0003 Class06: 0.0003 \n",
      "Class07: 0.0002 Class08: 0.0987 Class09: 0.3039 Class10: 0.0003 Class11: 0.0001 Class12: 0.0809 \n",
      "Train seg Loss: 2.9729835707784273 Train dom Loss: 317.58674102049906\n",
      "Train Loss: 3.290570326473402\n",
      "Train mIoU: 0.07710300027285885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:10<00:00,  2.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--IoU Scores Fish val--\n",
      "Class00: 0.8400 Class01: 0.0941 Class02: 0.0000 Class03: 0.1623 Class04: 0.0000 Class05: 0.0000 Class06: 0.0000 \n",
      "Class07: 0.0000 Class08: 0.1482 Class09: 0.7593 Class10: 0.0000 Class11: 0.0000 Class12: 0.1667 \n",
      "Epoch8\n",
      "Valid Seg Loss: 5.498427859942118 Valid dom Loss: 455.9660308837891\n",
      "Valid Loss: 5.954393943150838\n",
      "Valid mIoU: 0.16696130183818655\n",
      "___________________________________________________________________________________________\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 9: 100%|██████████| 138/138 [05:04<00:00,  2.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label_0: IoU Scores Train\n",
      "Class00: 0.2375 Class01: 0.1282 Class02: 0.0058 Class03: 0.1736 Class04: 0.0064 Class05: 0.0007 Class06: 0.0002 \n",
      "Class07: 0.0005 Class08: 0.1660 Class09: 0.4341 Class10: 0.0005 Class11: 0.0002 Class12: 0.0900 \n",
      "Label_1: IoU Scores Train\n",
      "Class00: 0.2259 Class01: 0.1197 Class02: 0.0071 Class03: 0.1679 Class04: 0.0067 Class05: 0.0008 Class06: 0.0002 \n",
      "Class07: 0.0005 Class08: 0.1685 Class09: 0.3689 Class10: 0.0007 Class11: 0.0004 Class12: 0.0925 \n",
      "Label_2: IoU Scores Train\n",
      "Class00: 0.2187 Class01: 0.1270 Class02: 0.0089 Class03: 0.1623 Class04: 0.0060 Class05: 0.0005 Class06: 0.0001 \n",
      "Class07: 0.0004 Class08: 0.1635 Class09: 0.3319 Class10: 0.0008 Class11: 0.0006 Class12: 0.0690 \n",
      "Train seg Loss: 1.9685812223936625 Train dom Loss: 295.21164278362403\n",
      "Train Loss: 2.2637928793395776\n",
      "Train mIoU: 0.08955910469704803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:10<00:00,  2.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--IoU Scores Fish val--\n",
      "Class00: 0.8887 Class01: 0.0468 Class02: 0.0000 Class03: 0.2109 Class04: 0.0010 Class05: 0.0000 Class06: 0.0000 \n",
      "Class07: 0.0000 Class08: 0.2650 Class09: 0.8071 Class10: 0.0000 Class11: 0.0000 Class12: 0.1858 \n",
      "Epoch9\n",
      "Valid Seg Loss: 2.794280735651652 Valid dom Loss: 25.440267345060906\n",
      "Valid Loss: 2.819721007347107\n",
      "Valid mIoU: 0.18502037511364655\n",
      "___________________________________________________________________________________________\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 10: 100%|██████████| 138/138 [05:02<00:00,  2.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label_0: IoU Scores Train\n",
      "Class00: 0.2479 Class01: 0.1634 Class02: 0.0094 Class03: 0.1939 Class04: 0.0063 Class05: 0.0006 Class06: 0.0005 \n",
      "Class07: 0.0003 Class08: 0.2539 Class09: 0.4381 Class10: 0.0011 Class11: 0.0004 Class12: 0.0841 \n",
      "Label_1: IoU Scores Train\n",
      "Class00: 0.2322 Class01: 0.1488 Class02: 0.0098 Class03: 0.1833 Class04: 0.0080 Class05: 0.0004 Class06: 0.0004 \n",
      "Class07: 0.0004 Class08: 0.2274 Class09: 0.3723 Class10: 0.0009 Class11: 0.0003 Class12: 0.0836 \n",
      "Label_2: IoU Scores Train\n",
      "Class00: 0.2216 Class01: 0.1495 Class02: 0.0082 Class03: 0.1741 Class04: 0.0071 Class05: 0.0003 Class06: 0.0004 \n",
      "Class07: 0.0004 Class08: 0.2066 Class09: 0.3351 Class10: 0.0007 Class11: 0.0003 Class12: 0.0810 \n",
      "Train seg Loss: 1.550597511915769 Train dom Loss: 354.3098322063487\n",
      "Train Loss: 1.904907358729321\n",
      "Train mIoU: 0.09879534517217471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:10<00:00,  2.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--IoU Scores Fish val--\n",
      "Class00: 0.9048 Class01: 0.2792 Class02: 0.0000 Class03: 0.2140 Class04: 0.0000 Class05: 0.0000 Class06: 0.0000 \n",
      "Class07: 0.0000 Class08: 0.4116 Class09: 0.8137 Class10: 0.0000 Class11: 0.0000 Class12: 0.1411 \n",
      "Epoch10\n",
      "Valid Seg Loss: 1.21255730787913 Valid dom Loss: 666.8744197770953\n",
      "Valid Loss: 1.8794317603111268\n",
      "Valid mIoU: 0.2126469231657622\n",
      "___________________________________________________________________________________________\n",
      "\n",
      "Hyperparamerters\n",
      "LR = 0.001 | EP = 10, BATCH_SIZE = 16, N_CLASSES = 13, init_alpha = 0.0001, N_LABELS = 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "#torch.cuda.empty_cache()\n",
    "# import wandb\n",
    "\n",
    "\n",
    "# wandb.init(\n",
    "#     # set the wandb project where this run will be logged\n",
    "#     project=\"11_08_4d_res34_0.0001\",\n",
    "    \n",
    "#     # track hyperparameters and run metadata\n",
    "#     config={\n",
    "#     \"learning_rate\": LR,\n",
    "#     \"architecture\": \"CNN\",\n",
    "#     \"dataset\": \"Samsung\",\n",
    "#     \"epochs\": EP,\n",
    "#     }\n",
    "# )\n",
    "\n",
    "for epoch in range(EP):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    seg_losses = 0\n",
    "    domain_losses = 0\n",
    "    if epoch < 10:  # 전체 EP 40%\n",
    "        alpha = 0.001\n",
    "    elif epoch <25: # 전체 EP 30%\n",
    "        alpha = 0.0005\n",
    "    else:          # 전체 EP 30%\n",
    "        alpha = 0.0001\n",
    "    train_class_ious = [[],[],[]]\n",
    "    for source_images, source_masks in tqdm(source_dataloader,desc=f\"Epoch: {epoch+1}\"):\n",
    "        random.shuffle(Label)\n",
    "        for l in range(N_LABELS):\n",
    "            label = Label[l]\n",
    "            source_image, source_mask = apply_fisheye_distortion(source_images, source_masks, label)\n",
    "            source_image = source_image.float().to(device)\n",
    "            source_mask = source_mask.long().to(device)\n",
    "            source_outputs = model(source_image)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            target_loss, seg_loss, domain_loss = loss_fn(source_outputs, source_mask, label, alpha = alpha)\n",
    "            epoch_loss += target_loss.item()\n",
    "            seg_losses +=  seg_loss.item()\n",
    "            domain_losses += domain_loss.item()\n",
    "            target_loss.backward()\n",
    "            optimizer.step()\n",
    "            # miou측정\n",
    "            source_outputs = model(source_image)\n",
    "            source_outputs = torch.softmax(source_outputs[0], dim=1).cpu()\n",
    "            source_outputs = torch.argmax(source_outputs, dim=1).numpy()\n",
    "            for class_id in range(N_CLASSES):\n",
    "                iou = calculate_iou_per_class(np.array(source_masks.cpu()), np.array(source_outputs), class_id)\n",
    "                train_class_ious[label].append(iou)\n",
    "            #print(train_class_ious[0])\n",
    "    \n",
    "    for i in range(N_LABELS):\n",
    "        buff = np.array(train_class_ious[i]).reshape(-1, N_CLASSES)\n",
    "        buff = np.mean(buff, axis=0)\n",
    "        print(f\"\\nLabel_{i}: IoU Scores Train\") \n",
    "        for class_id, iou in enumerate(buff):\n",
    "            print(f'Class{class_id:02d}: {iou:.4f}', end=\" \")\n",
    "            if (class_id+1) % 7 == 0:\n",
    "                print()   \n",
    "    print()    \n",
    "    print(f\"Train seg Loss: {(seg_losses/(N_LABELS*len(source_dataloader)))}\", f\"Train dom Loss: {(domain_losses/(N_LABELS*len(source_dataloader)))}\")\n",
    "    print(f\"Train Loss: {(epoch_loss/(N_LABELS*len(source_dataloader)))}\")\n",
    "    print(f\"Train mIoU: {np.mean(train_class_ious)}\" )\n",
    "    ################################################################\n",
    "    # 클래스별 IoU를 누적할 리스트 초기화\n",
    "    val_class_ious = []\n",
    "    fish_val_class_ious = []\n",
    "    val_epoch_loss = 0\n",
    "    val_seg_loss = 0\n",
    "    val_domain_loss = 0\n",
    "    # valid\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "\n",
    "        for target_images, target_masks in tqdm(val_target_dataloader):\n",
    "            label = 1.5\n",
    "            target_images, target_masks = apply_fisheye_distortion(target_images, target_masks, label)\n",
    "            target_images = target_images.float().to(device)\n",
    "            target_masks = target_masks.long().to(device)\n",
    "\n",
    "            target_outputs = model(target_images)\n",
    "\n",
    "            target_loss, target_seg_loss, target_domain_loss = loss_fn(target_outputs, target_masks, label, alpha = alpha)\n",
    "\n",
    "            val_seg_loss +=  target_seg_loss.item()\n",
    "            val_domain_loss += target_domain_loss.item()\n",
    "            \n",
    "            loss = target_loss\n",
    "\n",
    "            val_epoch_loss += loss.item()\n",
    "\n",
    "            # train 클래스별 IoU 계산\n",
    "            target_outputs = torch.softmax(target_outputs[0], dim=1).cpu()\n",
    "            target_outputs = torch.argmax(target_outputs, dim=1).numpy()\n",
    "\n",
    "            for class_id in range(N_CLASSES):\n",
    "                iou = calculate_iou_per_class(np.array(target_masks.cpu()), np.array(target_outputs), class_id)\n",
    "                fish_val_class_ious.append(iou)\n",
    "\n",
    "    fish_val_class_ious = np.array(fish_val_class_ious).reshape(-1, N_CLASSES)\n",
    "    fish_val_class_ious = np.mean(fish_val_class_ious, axis=0)\n",
    "    print()\n",
    "    print(\"--IoU Scores Fish val--\")\n",
    "    for class_id, iou in enumerate(fish_val_class_ious):\n",
    "        print(f'Class{class_id:02d}: {iou:.4f}', end=\" \")\n",
    "        if (class_id+1) % 7 == 0:\n",
    "            print()\n",
    "\n",
    "    # mIoU 계산\n",
    "    fish_val_mIoU = np.mean(fish_val_class_ious)\n",
    "\n",
    "    # 에폭마다 결과 출력 \n",
    "    print(f\"\\nEpoch{epoch+1}\")\n",
    "    print(f\"Valid Seg Loss: {(val_seg_loss/len(val_target_dataloader))}\",f\"Valid dom Loss: {(val_domain_loss/len(val_target_dataloader))}\")\n",
    "    print(f\"Valid Loss: {(val_epoch_loss/len(val_target_dataloader))}\")\n",
    "    print(f\"Valid mIoU: {fish_val_mIoU}\" )\n",
    "    print(\"___________________________________________________________________________________________\\n\")\n",
    "\n",
    "\n",
    "#     # log metrics to wandb\n",
    "#     wandb.log({\"train score\": np.mean(train_class_ious)})\n",
    "#     wandb.log({\"val score\": fish_val_mIoU})\n",
    "#     wandb.log({\"train loss\": (epoch_loss/(N_LABELS*len(source_dataloader)))})\n",
    "#     wandb.log({\"val loss\": (val_epoch_loss/len(val_target_dataloader))})\n",
    "    \n",
    "    \n",
    "# # [optional] finish the wandb run, necessary in notebooks\n",
    "# wandb.finish()\n",
    "\n",
    "print(\"Hyperparamerters\")\n",
    "print(f\"LR = {LR} | EP = {EP}, BATCH_SIZE = {BATCH_SIZE}, N_CLASSES = {N_CLASSES}, init_alpha = {ALPHA}, N_LABELS = {N_LABELS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './data/resnet34_1109_nos_0001.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "byungwan_resn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
