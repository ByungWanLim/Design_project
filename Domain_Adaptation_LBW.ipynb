{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "#from torch.optim.lr_scheduler import _LRScheduler\n",
    "\n",
    "from tqdm import tqdm\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# GPU 사용이 가능할 경우, GPU를 사용할 수 있게 함.'\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = torch.device(device)\n",
    "print(device)\n",
    "\n",
    "#print(os.environ.get('CUDA_VISIBLE_DEVICES'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RLE 인코딩 함수\n",
    "def rle_encode(mask):\n",
    "    pixels = mask.flatten()\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)\n",
    "\n",
    "# 클래스별 IoU를 계산하기 위한 함수\n",
    "def calculate_iou_per_class(y_true, y_pred, class_id):\n",
    "    intersection = np.sum((y_true == class_id) & (y_pred == class_id))\n",
    "    union = np.sum((y_true == class_id) | (y_pred == class_id))\n",
    "    iou = intersection / union if union > 0 else 0\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None, infer=False):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "        self.infer = infer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #directory_path = \"/mnt/nas27/Dataset/Samsung_DM\"\n",
    "        directory_path = './data/224'\n",
    "        img_path = self.data.iloc[idx, 1]\n",
    "        img_path = os.path.join(directory_path, img_path)\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        #image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        if self.infer:\n",
    "            if self.transform:\n",
    "                image = self.transform(image=image)['image']\n",
    "            return image\n",
    "        \n",
    "        mask_path = self.data.iloc[idx, 2]\n",
    "        mask_path = os.path.join(directory_path, mask_path)\n",
    "        mask = cv2.imread(mask_path)\n",
    "        #mask = cv2.cvtColor(mask, cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)\n",
    "        mask = np.round(mask).astype(np.uint8)\n",
    "        mask[mask > 12] = 12 #배경을 픽셀값 12로 간주\n",
    "\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "# class CustomDataset_target(Dataset):\n",
    "#     def __init__(self, csv_file, transform=None, infer=False):\n",
    "#         self.data = pd.read_csv(csv_file)\n",
    "#         self.transform = transform\n",
    "#         self.infer = infer\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         directory_path = \"/mnt/nas27/Dataset/Samsung_DM\"\n",
    "#         img_path = self.data.iloc[idx, 1]\n",
    "#         img_path = os.path.join(directory_path, img_path[2:])\n",
    "#         image = cv2.imread(img_path)\n",
    "#         image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "#         if self.infer:\n",
    "#             if self.transform:\n",
    "#                 image = self.transform(image=image)['image']\n",
    "#             return image\n",
    "\n",
    "\n",
    "#         if self.transform:\n",
    "#             augmented = self.transform(image=image)\n",
    "#             image = augmented['image']\n",
    "            \n",
    "\n",
    "#         return image\n",
    "     \n",
    "\n",
    "transform = A.Compose(\n",
    "    [   \n",
    "        #A.Resize(224, 224),\n",
    "        #A.Resize(128, 128),\n",
    "        A.Normalize(),\n",
    "        \n",
    "        # 변형\n",
    "        # A.VerticalFlip(p=0.5),\n",
    "        # A.RandomRotate90(p=0.5),\n",
    "        # A.HueSaturationValue(p=0.2),\n",
    "        \n",
    "        ToTensorV2()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unet의 기본이 되는 conv블럭\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)  # 여기서 in_channels는 out_channels와 동일해야 합니다.\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "        return x\n",
    "\n",
    "#인코더 블럭\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(EncoderBlock,self).__init__()\n",
    "        self.convblock1 = ConvBlock(in_channels, out_channels)  # 첫 번째 ConvBlock의 in_channels는 입력 이미지의 채널 수와 일치해야 합니다.\n",
    "        #self.convblock2 = ConvBlock(out_channels, out_channels)  # 두 번째 ConvBlock의 in_channels는 out_channels와 일치해야 합니다.\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.convblock1(x)\n",
    "        #x = self.convblock2(x)\n",
    "        p = self.maxpool(x)\n",
    "        return x , p\n",
    "#디코더 블럭\n",
    "#디코더는 업샘플링 이후 스킵연결과 붙어서 convblock을 통과해야함\n",
    "#skip보다 작은 x x먼저 업샘플링 32 -> 64 , skip과 결합 6464 \n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(DecoderBlock,self).__init__()\n",
    "        self.upsample = nn.ConvTranspose2d(channels*2, channels, kernel_size=4, stride=2, padding=1)#x 업샘플링\n",
    "        self.convblock1 = ConvBlock(channels*2, channels)#차원감소\n",
    "        #self.convblock2 = ConvBlock(channels, channels)\n",
    "    def forward(self,x,skip):\n",
    "        x = self.upsample(x)\n",
    "        x = torch.cat([x, skip], dim=1)\n",
    "        x = self.convblock1(x)\n",
    "        #x = self.convblock2(x)\n",
    "        return x\n",
    "\n",
    "###########################################\n",
    "class GradReverse(torch.autograd.Function):\n",
    "    def forward(self, x):\n",
    "        return x.view_as(x)\n",
    "\n",
    "    def backward(self, grad_output): # 역전파 시에 gradient에 음수를 취함\n",
    "        return (grad_output * -1)\n",
    "\n",
    "class domain_classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(domain_classifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(224*224*64, 10)\n",
    "        self.fc2 = nn.Linear(10, 1) # source = 0, target = 1 회귀 가정\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 224*224*64)\n",
    "        x = GradReverse.apply(x) # gradient reverse\n",
    "        x = F.leaky_relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "###########################################\n",
    "\n",
    "\n",
    "#Unet구조 middle의 xm값의 움직임에 주의\n",
    "class Unet(nn.Module):\n",
    "    def __init__(self,n_classes):\n",
    "        super(Unet,self).__init__()\n",
    "        self.encoder1 = EncoderBlock(3,64)\n",
    "        self.encoder2 = EncoderBlock(64,128)\n",
    "        self.encoder3 = EncoderBlock(128,256)\n",
    "        self.encoder4 = EncoderBlock(256,512)\n",
    "        \n",
    "        self.middleconv = ConvBlock(512,1024)\n",
    "        \n",
    "        \n",
    "        self.decoder4 = DecoderBlock(512)\n",
    "        self.decoder3 = DecoderBlock(256)\n",
    "        self.decoder2 = DecoderBlock(128)\n",
    "        self.decoder1 = DecoderBlock(64)\n",
    "        self.segmap = nn.Conv2d(64,n_classes, kernel_size=1)\n",
    "        \n",
    "        self.domain_classifier = domain_classifier()\n",
    "                                        \n",
    "\n",
    "    def forward(self,x):\n",
    "        x1,p = self.encoder1(x)#3->64   #P:256,256 x1 :512,512\n",
    "        x2,p = self.encoder2(p)#64->128 #P:128,128 x2:256,256\n",
    "        x3,p = self.encoder3(p)#128->256#p:64,64 x3:128,128\n",
    "        x4,p = self.encoder4(p)#256->512#p:32,32 x4:64,64\n",
    "        \n",
    "        xm = self.middleconv(p)#512->1024#32,32\n",
    "        \n",
    "        x = self.decoder4(xm,x4)#뉴런:1024->512->512 #출력tensor:64,64\n",
    "        x = self.decoder3(x,x3)#뉴런:512->256->256 #출력tensor:128,128\n",
    "        x = self.decoder2(x,x2)#뉴런:256->128->128 #출력tensor:256,256\n",
    "        x = self.decoder1(x,x1)#뉴런:128->64->64 #출력tensor:512,512\n",
    "\n",
    "        x_c = self.segmap(x)\n",
    "        x_d = self.domain_classifier(x)\n",
    "\n",
    "        return x_c, x_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([   6.2232,   81.0473,    5.1017,   66.2180,  170.2419,  506.2671,\n",
      "         159.9452,    6.9654,    4.3830,  907.1918, 2223.4863,   10.2576,\n",
      "           7.6339], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# 저장된 class_weights를 불러옵니다.\n",
    "class_weights_path = 'CLASS_WEIGHTS.pkl'\n",
    "\n",
    "with open(class_weights_path, 'rb') as file:\n",
    "    CLASS_WEIGHTS = pickle.load(file)\n",
    "\n",
    "print(CLASS_WEIGHTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function과 optimizer 정의\n",
    "\n",
    "class DANN_Loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DANN_Loss, self).__init__()\n",
    "\n",
    "        self.CE = nn.CrossEntropyLoss(weight=CLASS_WEIGHTS) # 0~9 class 분류용\n",
    "        self.BCE = nn.BCELoss() # 도메인 분류용\n",
    "        \n",
    "    # result : DANN_CNN에서 반환된 값\n",
    "    # label : 숫자 0 ~ 9에 대한 라벨\n",
    "    # domain_num : 0(source) or 1(target)\n",
    "    def forward(self, result, label, domain_num, alpha = 1):\n",
    "        label_logits, domain_logits = result # DANN_CNN의 결과\n",
    "\n",
    "        batch_size = domain_logits.shape[0]\n",
    "\n",
    "        domain_target = torch.FloatTensor([domain_num] * batch_size).unsqueeze(1).to(device)\n",
    "\n",
    "        domain_loss = self.BCE(domain_logits, domain_target) # domain 분류 loss\n",
    "        target_loss = self.CE(label_logits, label) # class 분류 loss\n",
    "\n",
    "        loss = target_loss + alpha * domain_loss\n",
    "\n",
    "        return loss\n",
    "    \n",
    "\n",
    "loss_fn = DANN_Loss().to(device)\n",
    "\n",
    "#criterion =nn.CrossEntropyLoss()\n",
    "#domain_criterion = nn.BCELoss()\n",
    "#criterion = nn.CrossEntropyLoss(weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.001\n",
    "EP = 10\n",
    "BATCH_SIZE = 8\n",
    "ACCMULATION_STEP = 1 \n",
    "N_CLASSES = 13 #IoU 점수측정하기 위한 클래스의 개수\n",
    "ALPHA = 0.5\n",
    "# model 초기화\n",
    "#model = Unet_resnet18(n_classes = N_CLASSES).to(device)\n",
    "#model = ResNet50(num_classes=N_CLASSES).to(device)\n",
    "model = Unet(n_classes = N_CLASSES).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "optimizer.zero_grad() \n",
    "\n",
    "source_dataset = CustomDataset(csv_file='./data/DA_csv/100train_source.csv', transform=transform)\n",
    "source_dataloader = DataLoader(source_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "target_dataset = CustomDataset(csv_file='./data/DA_csv/100fish_train_source.csv', transform=transform)\n",
    "target_dataloader = DataLoader(target_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "val_source_dataset = CustomDataset(csv_file='./data/DA_csv/20val_source.csv', transform=transform)\n",
    "val_source_dataloader = DataLoader(val_source_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "val_target_dataset = CustomDataset(csv_file='./data/DA_csv/20fish_val_source.csv', transform=transform)\n",
    "val_target_dataloader = DataLoader(val_target_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "# source_dataset = CustomDataset(csv_file=os.path.join(\"/mnt/nas27/Dataset/Samsung_DM\",'./train_source.csv'), transform=transform)#\n",
    "# source_dataloader = DataLoader(source_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=8)\n",
    "# s_valid_dataset = CustomDataset(csv_file=os.path.join(\"/mnt/nas27/Dataset/Samsung_DM\",'./val_source.csv'), transform=transform)\n",
    "# s_valid_dataloader = DataLoader(s_valid_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=8)\n",
    "# #target_dataset = CustomDataset_target(csv_file='./f_train_source.csv', transform=transform)\n",
    "# target_dataset = CustomDataset(csv_file=os.path.join(\"/mnt/nas27/Dataset/Samsung_DM\",'./fish_val_source.csv'), transform=transform)\n",
    "# target_dataloader = DataLoader(target_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=8)\n",
    "\n",
    "# t_test_dataset = CustomDataset(csv_file=os.path.join(\"/mnt/nas27/Dataset/Samsung_DM\",'./test.csv'), transform=transform, infer=True)\n",
    "# t_test_dataloader = DataLoader(t_test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import os\n",
    "# import cv2\n",
    "# import matplotlib.pyplot as plt\n",
    "# a1 = pd.read_csv(\"./data/6_fish_source.csv\")\n",
    "# p = \"./data/224/\"\n",
    "# a2 = os.path.join(p, a1.iloc[3,2])\n",
    "# a3 = cv2.imread(a2)\n",
    "# a4 = cv2.cvtColor(a3, cv2.COLOR_BGR2GRAY)\n",
    "# a4 = np.round(a4).astype(np.uint8)\n",
    "# a5 = a4*20\n",
    "\n",
    "# plt.imshow(a5, cmap='gray')\n",
    "# plt.axis('off')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13it [01:42,  7.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--IoU Scores Train--\n",
      "Class0: 0.0114 Class1: 0.0338 Class2: 0.0779 Class3: 0.0498 Class4: 0.0396 Class5: 0.0412 Class6: 0.0227 \n",
      "Class7: 0.0349 Class8: 0.0385 Class9: 0.0366 Class10: 0.0296 Class11: 0.0327 Class12: 0.0267 \n",
      "--IoU Scores Fish Train--\n",
      "Class0: 0.0077 Class1: 0.0087 Class2: 0.0472 Class3: 0.0350 Class4: 0.0365 Class5: 0.0347 Class6: 0.0353 \n",
      "Class7: 0.0355 Class8: 0.0353 Class9: 0.0307 Class10: 0.0305 Class11: 0.0315 Class12: 0.0453 \n",
      "Epoch1\n",
      "Train Loss: 4.222572326660156\n",
      "Train mIoU: 0.036571221800543015, Fish_Train_mIoU: 0.03182807385356154\n",
      "___________________________________________________________________________________________\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13it [01:45,  8.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--IoU Scores Train--\n",
      "Class0: 0.0327 Class1: 0.0230 Class2: 0.0292 Class3: 0.0286 Class4: 0.0223 Class5: 0.0249 Class6: 0.0227 \n",
      "Class7: 0.0288 Class8: 0.0234 Class9: 0.0260 Class10: 0.0218 Class11: 0.0263 Class12: 0.0191 \n",
      "--IoU Scores Fish Train--\n",
      "Class0: 0.0604 Class1: 0.0701 Class2: 0.0714 Class3: 0.0732 Class4: 0.0713 Class5: 0.0712 Class6: 0.0737 \n",
      "Class7: 0.0737 Class8: 0.0724 Class9: 0.0703 Class10: 0.0722 Class11: 0.0714 Class12: 0.0697 \n",
      "Epoch2\n",
      "Train Loss: 4.195852756500244\n",
      "Train mIoU: 0.025291251363917414, Fish_Train_mIoU: 0.07085061673573549\n",
      "___________________________________________________________________________________________\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13it [01:45,  8.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--IoU Scores Train--\n",
      "Class0: 0.0280 Class1: 0.0240 Class2: 0.0193 Class3: 0.0212 Class4: 0.0246 Class5: 0.0228 Class6: 0.0242 \n",
      "Class7: 0.0176 Class8: 0.0257 Class9: 0.0242 Class10: 0.0238 Class11: 0.0285 Class12: 0.0317 \n",
      "--IoU Scores Fish Train--\n",
      "Class0: 0.0701 Class1: 0.0705 Class2: 0.0716 Class3: 0.0708 Class4: 0.0701 Class5: 0.0709 Class6: 0.0702 \n",
      "Class7: 0.0713 Class8: 0.0722 Class9: 0.0701 Class10: 0.0712 Class11: 0.0705 Class12: 0.0698 \n",
      "Epoch3\n",
      "Train Loss: 4.184803485870361\n",
      "Train mIoU: 0.024285136540512967, Fish_Train_mIoU: 0.07072257137702163\n",
      "___________________________________________________________________________________________\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13it [01:46,  8.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--IoU Scores Train--\n",
      "Class0: 0.0281 Class1: 0.0349 Class2: 0.0353 Class3: 0.0429 Class4: 0.0445 Class5: 0.0548 Class6: 0.0536 \n",
      "Class7: 0.0561 Class8: 0.0499 Class9: 0.0643 Class10: 0.0442 Class11: 0.0495 Class12: 0.0605 \n",
      "--IoU Scores Fish Train--\n",
      "Class0: 0.0874 Class1: 0.0963 Class2: 0.1015 Class3: 0.1025 Class4: 0.1110 Class5: 0.1055 Class6: 0.1146 \n",
      "Class7: 0.1036 Class8: 0.1012 Class9: 0.1023 Class10: 0.1006 Class11: 0.1088 Class12: 0.1045 \n",
      "Epoch4\n",
      "Train Loss: 4.1749749183654785\n",
      "Train mIoU: 0.04758397329591132, Fish_Train_mIoU: 0.10304943738337216\n",
      "___________________________________________________________________________________________\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13it [01:48,  8.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--IoU Scores Train--\n",
      "Class0: 0.0630 Class1: 0.0556 Class2: 0.0565 Class3: 0.0559 Class4: 0.0667 Class5: 0.0698 Class6: 0.0634 \n",
      "Class7: 0.0688 Class8: 0.0532 Class9: 0.0535 Class10: 0.0654 Class11: 0.0613 Class12: 0.0659 \n",
      "--IoU Scores Fish Train--\n",
      "Class0: 0.1061 Class1: 0.1022 Class2: 0.1012 Class3: 0.1005 Class4: 0.1032 Class5: 0.0981 Class6: 0.1031 \n",
      "Class7: 0.1084 Class8: 0.1013 Class9: 0.1089 Class10: 0.1086 Class11: 0.1094 Class12: 0.1111 \n",
      "Epoch5\n",
      "Train Loss: 4.140742301940918\n",
      "Train mIoU: 0.06146074608198034, Fish_Train_mIoU: 0.10478850163938715\n",
      "___________________________________________________________________________________________\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13it [01:47,  8.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--IoU Scores Train--\n",
      "Class0: 0.0616 Class1: 0.0671 Class2: 0.0633 Class3: 0.0689 Class4: 0.0577 Class5: 0.0663 Class6: 0.0610 \n",
      "Class7: 0.0624 Class8: 0.0660 Class9: 0.0585 Class10: 0.0518 Class11: 0.0605 Class12: 0.0583 \n",
      "--IoU Scores Fish Train--\n",
      "Class0: 0.1100 Class1: 0.1090 Class2: 0.1016 Class3: 0.1058 Class4: 0.1090 Class5: 0.1030 Class6: 0.1090 \n",
      "Class7: 0.0997 Class8: 0.1107 Class9: 0.1073 Class10: 0.1064 Class11: 0.1095 Class12: 0.1085 \n",
      "Epoch6\n",
      "Train Loss: 4.141851902008057\n",
      "Train mIoU: 0.061790492148505134, Fish_Train_mIoU: 0.1068910381778403\n",
      "___________________________________________________________________________________________\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13it [01:44,  8.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--IoU Scores Train--\n",
      "Class0: 0.0570 Class1: 0.0594 Class2: 0.0593 Class3: 0.0567 Class4: 0.0655 Class5: 0.0525 Class6: 0.0717 \n",
      "Class7: 0.0640 Class8: 0.0616 Class9: 0.0758 Class10: 0.0633 Class11: 0.0770 Class12: 0.0773 \n",
      "--IoU Scores Fish Train--\n",
      "Class0: 0.1078 Class1: 0.1057 Class2: 0.1153 Class3: 0.1139 Class4: 0.1078 Class5: 0.1196 Class6: 0.1095 \n",
      "Class7: 0.1055 Class8: 0.1083 Class9: 0.1057 Class10: 0.1065 Class11: 0.1079 Class12: 0.1075 \n",
      "Epoch7\n",
      "Train Loss: 4.140923976898193\n",
      "Train mIoU: 0.06471417120224868, Fish_Train_mIoU: 0.10929805720239233\n",
      "___________________________________________________________________________________________\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13it [01:47,  8.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--IoU Scores Train--\n",
      "Class0: 0.0538 Class1: 0.0643 Class2: 0.0660 Class3: 0.0803 Class4: 0.0706 Class5: 0.0863 Class6: 0.0865 \n",
      "Class7: 0.0783 Class8: 0.0691 Class9: 0.0842 Class10: 0.0716 Class11: 0.0819 Class12: 0.0469 \n",
      "--IoU Scores Fish Train--\n",
      "Class0: 0.1071 Class1: 0.1038 Class2: 0.1065 Class3: 0.1005 Class4: 0.1075 Class5: 0.1063 Class6: 0.1192 \n",
      "Class7: 0.1097 Class8: 0.1179 Class9: 0.1107 Class10: 0.1142 Class11: 0.1138 Class12: 0.1023 \n",
      "Epoch8\n",
      "Train Loss: 4.143531322479248\n",
      "Train mIoU: 0.07227833899404218, Fish_Train_mIoU: 0.10919030449187006\n",
      "___________________________________________________________________________________________\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:11, 11.47s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Workspace\\Design_project\\Domain_Adaptation_LBW.ipynb 셀 13\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Workspace/Design_project/Domain_Adaptation_LBW.ipynb#X13sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m source_outputs \u001b[39m=\u001b[39m model(source_images)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Workspace/Design_project/Domain_Adaptation_LBW.ipynb#X13sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m target_outputs \u001b[39m=\u001b[39m model(target_images)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Workspace/Design_project/Domain_Adaptation_LBW.ipynb#X13sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m source_loss \u001b[39m=\u001b[39m loss_fn(source_outputs, source_masks, \u001b[39m0\u001b[39m, alpha \u001b[39m=\u001b[39m ALPHA)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Workspace/Design_project/Domain_Adaptation_LBW.ipynb#X13sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m target_loss \u001b[39m=\u001b[39m loss_fn(target_outputs, target_masks, \u001b[39m1\u001b[39m, alpha \u001b[39m=\u001b[39m ALPHA)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Workspace/Design_project/Domain_Adaptation_LBW.ipynb#X13sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m loss \u001b[39m=\u001b[39m source_loss \u001b[39m+\u001b[39m target_loss\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\UM\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Workspace\\Design_project\\Domain_Adaptation_LBW.ipynb 셀 13\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Workspace/Design_project/Domain_Adaptation_LBW.ipynb#X13sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m label_logits, domain_logits \u001b[39m=\u001b[39m result \u001b[39m# DANN_CNN의 결과\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Workspace/Design_project/Domain_Adaptation_LBW.ipynb#X13sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m batch_size \u001b[39m=\u001b[39m domain_logits\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Workspace/Design_project/Domain_Adaptation_LBW.ipynb#X13sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m domain_target \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mFloatTensor([domain_num] \u001b[39m*\u001b[39m batch_size)\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Workspace/Design_project/Domain_Adaptation_LBW.ipynb#X13sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m domain_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mBCE(domain_logits, domain_target) \u001b[39m# domain 분류 loss\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Workspace/Design_project/Domain_Adaptation_LBW.ipynb#X13sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m target_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mCE(label_logits, label) \u001b[39m# class 분류 loss\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import random\n",
    "#torch.cuda.empty_cache()\n",
    "#import wandb\n",
    "\n",
    "\n",
    "# wandb.init(\n",
    "#     # set the wandb project where this run will be logged\n",
    "#     project=\"practice_10_06\",\n",
    "    \n",
    "#     # track hyperparameters and run metadata\n",
    "#     config={\n",
    "#     \"learning_rate\": LR,\n",
    "#     \"architecture\": \"CNN\",\n",
    "#     \"dataset\": \"Samsung\",\n",
    "#     \"epochs\": EP,\n",
    "#     }\n",
    "# )\n",
    "\n",
    "for epoch in range(EP):\n",
    "    # 클래스별 IoU를 누적할 리스트 초기화\n",
    "    train_class_ious = []\n",
    "    fish_train_class_ious = []\n",
    "    # 학습\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for source_data, target_data in tqdm(zip(source_dataloader, target_dataloader)):\n",
    "\n",
    "        source_images = source_data[0].float().to(device)\n",
    "        source_masks = source_data[1].long().to(device)\n",
    "        target_images = target_data[0].float().to(device)\n",
    "        target_masks = target_data[1].long().to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        source_outputs = model(source_images)\n",
    "        target_outputs = model(target_images)\n",
    "\n",
    "        source_loss = loss_fn(source_outputs, source_masks, 0, alpha = ALPHA)\n",
    "        target_loss = loss_fn(target_outputs, target_masks, 1, alpha = ALPHA)\n",
    "\n",
    "        loss = source_loss + target_loss\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #scheduler.step()\n",
    "                # train 클래스별 IoU 계산\n",
    "        source_outputs = torch.softmax(source_outputs[0], dim=1).cpu()\n",
    "        source_outputs = torch.argmax(source_outputs, dim=1).numpy()\n",
    "\n",
    "        for class_id in range(N_CLASSES):\n",
    "            iou = calculate_iou_per_class(np.array(source_masks.cpu()), np.array(source_outputs), class_id)\n",
    "            train_class_ious.append(iou)\n",
    "\n",
    "        target_outputs = torch.softmax(target_outputs[0], dim=1).cpu()\n",
    "        target_outputs = torch.argmax(target_outputs, dim=1).numpy()\n",
    "\n",
    "        for class_id in range(N_CLASSES):\n",
    "            iou = calculate_iou_per_class(np.array(target_masks.cpu()), np.array(target_outputs), class_id)\n",
    "            fish_train_class_ious.append(iou)\n",
    "\n",
    "    train_class_ious = np.array(train_class_ious).reshape(N_CLASSES, -1)\n",
    "    train_class_ious = np.mean(train_class_ious, axis=1)\n",
    "    print(\"--IoU Scores Train--\")\n",
    "    for class_id, iou in enumerate(train_class_ious):\n",
    "        print(f'Class{class_id}: {iou:.4f}', end=\" \")\n",
    "        if (class_id+1) % 7 == 0:\n",
    "            print()\n",
    "\n",
    "    fish_train_class_ious = np.array(fish_train_class_ious).reshape(N_CLASSES, -1)\n",
    "    fish_train_class_ious = np.mean(fish_train_class_ious, axis=1)\n",
    "    print()\n",
    "    print(\"--IoU Scores Fish Train--\")\n",
    "    for class_id, iou in enumerate(fish_train_class_ious):\n",
    "        print(f'Class{class_id}: {iou:.4f}', end=\" \")\n",
    "        if (class_id+1) % 7 == 0:\n",
    "            print()\n",
    "\n",
    "    # mIoU 계산\n",
    "    train_mIoU = np.mean(train_class_ious)\n",
    "    fish_train_mIoU = np.mean(fish_train_class_ious)\n",
    "\n",
    "    # 에폭마다 결과 출력 \n",
    "    print(f\"\\nEpoch{epoch+1}\")\n",
    "    print(f\"Train Loss: {(loss/len(source_dataloader))}\")\n",
    "    print(f\"Train mIoU: {train_mIoU}, Fish_Train_mIoU: {fish_train_mIoU}\" )\n",
    "    print(\"___________________________________________________________________________________________\\n\")\n",
    "\n",
    "\n",
    "    #################################################################\n",
    "    # # 클래스별 IoU를 누적할 리스트 초기화\n",
    "    # val_class_ious = []\n",
    "    # fish_val_class_ious = []\n",
    "    # epoch_loss = 0\n",
    "    # # 학습\n",
    "    # with torch.no_grad():\n",
    "    #     model.eval()\n",
    "\n",
    "    #     for source_data, target_data in tqdm(zip(val_source_dataloader, val_target_dataloader)):\n",
    "\n",
    "    #         source_images = source_data[0].float().to(device)\n",
    "    #         source_masks = source_data[1].long().to(device)\n",
    "    #         target_images = target_data[0].float().to(device)\n",
    "    #         target_masks = target_data[1].long().to(device)\n",
    "\n",
    "    #         source_outputs = model(source_images)\n",
    "    #         target_outputs = model(target_images)\n",
    "\n",
    "    #         source_loss = loss_fn(source_outputs, source_masks, 0, alpha = ALPHA)\n",
    "    #         target_loss = loss_fn(target_outputs, target_masks, 1, alpha = ALPHA)\n",
    "\n",
    "    #         loss = source_loss + target_loss\n",
    "\n",
    "    #         epoch_loss += loss.item()\n",
    "\n",
    "    #         # train 클래스별 IoU 계산\n",
    "    #         source_outputs = torch.softmax(source_outputs[0], dim=1).cpu()\n",
    "    #         source_outputs = torch.argmax(source_outputs, dim=1).numpy()\n",
    "\n",
    "    #         for class_id in range(N_CLASSES):\n",
    "    #             iou = calculate_iou_per_class(np.array(source_masks.cpu()), np.array(source_outputs), class_id)\n",
    "    #             val_class_ious.append(iou)\n",
    "\n",
    "    #         target_outputs = torch.softmax(target_outputs[0], dim=1).cpu()\n",
    "    #         target_outputs = torch.argmax(target_outputs, dim=1).numpy()\n",
    "\n",
    "    #         for class_id in range(N_CLASSES):\n",
    "    #             iou = calculate_iou_per_class(np.array(target_masks.cpu()), np.array(target_outputs), class_id)\n",
    "    #             fish_val_class_ious.append(iou)\n",
    "\n",
    "    # val_class_ious = np.array(val_class_ious).reshape(N_CLASSES, -1)\n",
    "    # val_class_ious = np.mean(val_class_ious, axis=1)\n",
    "    # print(\"--IoU Scores val--\")\n",
    "    # for class_id, iou in enumerate(val_class_ious):\n",
    "    #     print(f'Class{class_id}: {iou:.4f}', end=\" \")\n",
    "    #     if (class_id+1) % 7 == 0:\n",
    "    #         print()\n",
    "\n",
    "    # fish_val_class_ious = np.array(fish_val_class_ious).reshape(N_CLASSES, -1)\n",
    "    # fish_val_class_ious = np.mean(fish_val_class_ious, axis=1)\n",
    "    # print()\n",
    "    # print(\"--IoU Scores Fish val--\")\n",
    "    # for class_id, iou in enumerate(fish_val_class_ious):\n",
    "    #     print(f'Class{class_id}: {iou:.4f}', end=\" \")\n",
    "    #     if (class_id+1) % 7 == 0:\n",
    "    #         print()\n",
    "\n",
    "    # # mIoU 계산\n",
    "    # val_mIoU = np.mean(val_class_ious)\n",
    "    # fish_val_mIoU = np.mean(fish_val_class_ious)\n",
    "\n",
    "    # # 에폭마다 결과 출력 \n",
    "    # print(f\"\\nEpoch{epoch+1}\")\n",
    "    # print(f\"Train Loss: {(loss/len(source_dataloader))}\")\n",
    "    # print(f\"Train mIoU: {val_mIoU}, Fish_Train_mIoU: {fish_val_mIoU}\" )\n",
    "    # print(\"___________________________________________________________________________________________\\n\")\n",
    "\n",
    "#     # log metrics to wandb\n",
    "#     wandb.log({\"train score\": train_mIoU, \"train loss\": epoch_loss})\n",
    "#     wandb.log({\"val score\": val_mIoU, \"val loss\": val_loss})\n",
    "    \n",
    "    \n",
    "# # [optional] finish the wandb run, necessary in notebooks\n",
    "# wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "byungwan_resn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
