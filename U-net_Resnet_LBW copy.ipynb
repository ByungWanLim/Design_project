{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d73d24e3-5c9e-4ade-9e6e-ca6f46a2d914",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad9b681e-370a-4cfa-a452-dd2d7f0cd77f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/MMI24leehyunwook/.conda/envs/net_bw/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "\n",
    "from tqdm import tqdm\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "\n",
    "# GPU 사용이 가능할 경우, GPU를 사용할 수 있게 함.'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = torch.device(device)\n",
    "print(device)\n",
    "\n",
    "print(os.environ.get('CUDA_VISIBLE_DEVICES'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "20ff3de5-0d0e-497b-ac75-d5179a3f65d3",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "838e1d83-8670-407b-82f6-bf9652f58639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RLE 인코딩 함수\n",
    "def rle_encode(mask):\n",
    "    pixels = mask.flatten()\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)\n",
    "\n",
    "# 클래스별 IoU를 계산하기 위한 함수\n",
    "def calculate_iou_per_class(y_true, y_pred, class_id):\n",
    "    intersection = np.sum((y_true == class_id) & (y_pred == class_id))\n",
    "    union = np.sum((y_true == class_id) | (y_pred == class_id))\n",
    "    iou = intersection / union if union > 0 else 0\n",
    "    return iou"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "be76a29e-e9c2-411a-a569-04166f074184",
   "metadata": {},
   "source": [
    "## Dataset, Data Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5acf65a",
   "metadata": {},
   "source": [
    "출력이미지 크기 키우기->ex) resnet 2048->1024->512->256 conv 256->512->1024->2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8496767-2f64-4285-bec4-c6f53a1fd9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None, infer=False):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "        self.infer = infer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        directory_path = \"/mnt/nas27/Dataset/Samsung_DM\"\n",
    "        img_path = self.data.iloc[idx, 1]\n",
    "        img_path = os.path.join(directory_path, img_path[2:])\n",
    "        image = cv2.imread(img_path)\n",
    "        if image is None:\n",
    "            raise ValueError(f\"Failed to load image at {img_path}\")\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        if self.infer:\n",
    "            if self.transform:\n",
    "                image = self.transform(image=image)['image']\n",
    "            return image\n",
    "        \n",
    "        mask_path = self.data.iloc[idx, 2]\n",
    "        mask_path = os.path.join(directory_path, mask_path[2:])\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if mask is None:\n",
    "            raise ValueError(f\"Failed to load mask at {mask_path}\")\n",
    "        mask[mask == 255] = 12\n",
    "\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "    \n",
    "    # 이미지 전처리 클래스\n",
    "class ImageTransform():\n",
    "  \"\"\"\n",
    "  훈련, 검증 동작 다르게 설정\n",
    "  이미지 크기 resize, 색상 표준화\n",
    "  훈련시 RandomResizedCrop, RandomHorizontalFilp으로 데이터 확장\n",
    "  \"\"\"\n",
    "  def __init__(self, resize, mean, std):\n",
    "    self.data_transform = {\n",
    "        'train' : transforms.Compose([\n",
    "            #transforms.RandomResizedCrop(\n",
    "            #    resize, scale = (0.5, 1.0)), # 데이터 확장\n",
    "            transforms.RandomHorizontalFlip(), # 데이터 확장\n",
    "            transforms.ToTensor(), # Tensor로 변환\n",
    "            transforms.Normalize(mean = mean, std = std) #표준화\n",
    "        ]),\n",
    "        'test': transforms.Compose([\n",
    "            #transforms.Resize(resize), # Resize\n",
    "            #transforms.CenterCrop(resize), # 중앙을 resize*resize로 crop\n",
    "            transforms.ToTensor(), # Tensor로 변환\n",
    "            transforms.Normalize(mean = mean, std = std) # 표준화\n",
    "        ])\n",
    "\n",
    "    }\n",
    "  def __call__(self, img, phase = 'train'):\n",
    "    \"\"\"\n",
    "    phase : 'train' or 'test'\n",
    "    전처리 모드 지정\n",
    "    \"\"\"\n",
    "    return self.data_transform[phase](img)\n",
    "\n",
    "\n",
    "transform = A.Compose(\n",
    "    [   \n",
    "        #A.Resize(224, 224),\n",
    "        A.Resize(256, 256),\n",
    "        A.Normalize(),\n",
    "        \n",
    "        # 변형\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        # A.RandomRotate90(p=0.5),\n",
    "        A.HueSaturationValue(p=0.2),\n",
    "        \n",
    "        ToTensorV2()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a644a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function CustomDataset.__len__ at 0x7f180d6ccaf0>\n"
     ]
    }
   ],
   "source": [
    "print(CustomDataset.__len__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a91d6b1",
   "metadata": {},
   "source": [
    "Warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9e62c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WarmUpLR(_LRScheduler):\n",
    "    def __init__(self, optimizer, total_iters, last_epoch=-1):\n",
    "        self.total_iters = total_iters\n",
    "        super(WarmUpLR, self).__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        return [base_lr * self.last_epoch / self.total_iters for base_lr in self.base_lrs]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f42501fc-b573-4893-a7c4-5e280dfdaf09",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "867c4b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "\n",
    "        # 1x1 convolution\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, padding=1, stride=stride, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "\n",
    "        # 3x3 convolution\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "\n",
    "        # 1x1 convolution\n",
    "        self.conv3 = nn.Conv2d(out_channels, out_channels * self.expansion, kernel_size=1, padding=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(out_channels * self.expansion)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels * self.expansion:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels * self.expansion, kernel_size=1, padding=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels * self.expansion)\n",
    "            )\n",
    "\n",
    "        self.relu3 = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu1(out)\n",
    "        print(\"Bottleneck:\",out.shape)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu2(out)\n",
    "         \n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "        print(\"Bottleneck:\",out.shape)\n",
    "        identity = self.shortcut(identity)\n",
    "        \n",
    "        out += identity\n",
    "        out = self.relu3(out)\n",
    "        print(\"Bottleneck:\",out.shape)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3574888b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unet의 기본이 되는 conv블럭\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels,kernel_size = 3):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size, padding=1)  # 여기서 in_channels는 out_channels와 동일해야 합니다.\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "        return x\n",
    "\n",
    "#인코더 블럭\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(EncoderBlock,self).__init__() \n",
    "        self.convblock1 = Bottleneck(in_channels, out_channels)  # 첫 번째 ConvBlock의 in_channels는 입력 이미지의 채널 수와 일치해야 합니다.\n",
    "        self.convblock2 = Bottleneck(out_channels, out_channels)  # 두 번째 ConvBlock의 in_channels는 out_channels와 일치해야 합니다.\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.convblock1(x)\n",
    "        x = self.convblock2(x)\n",
    "        p = self.maxpool(x)\n",
    "        return x , p\n",
    "    \n",
    "#디코더 블럭\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(DecoderBlock,self).__init__()\n",
    "        self.upsample = nn.ConvTranspose2d(channels*2, channels, kernel_size=4, stride=2, padding=1)#x 업샘플링\n",
    "        self.convblock1 = ConvBlock(channels*2, channels)#차원감소\n",
    "        self.convblock2 = ConvBlock(channels, channels)\n",
    "    def forward(self,x,skip):\n",
    "        x = self.upsample(x)\n",
    "        x = torch.cat([x, skip], dim=1)\n",
    "        x = self.convblock1(x)\n",
    "        x = self.convblock2(x)\n",
    "        return x\n",
    "        \n",
    "#Unet구조 middle의 xm값의 움직임에 주의\n",
    "class Unet(nn.Module):\n",
    "    def __init__(self,n_classes):\n",
    "        super(Unet,self).__init__()\n",
    "        #\n",
    "        self.fconv = nn.Conv2d(3, 64, kernel_size=7, padding=1)\n",
    "        self.fbn = nn.BatchNorm2d(64)\n",
    "        self.frelu = nn.ReLU()\n",
    "        \n",
    "        self.encoder1 = EncoderBlock(64,128)\n",
    "        self.encoder2 = EncoderBlock(128,256)\n",
    "        self.encoder3 = EncoderBlock(256,512)\n",
    "        self.encoder4 = EncoderBlock(512,1024)\n",
    "        \n",
    "        self.middleconv = Bottleneck(1024,1024)\n",
    "        \n",
    "        self.decoder4 = DecoderBlock(512)\n",
    "        self.decoder3 = DecoderBlock(256)\n",
    "        self.decoder2 = DecoderBlock(128)\n",
    "        self.decoder1 = DecoderBlock(64)\n",
    "        self.segmap = nn.Conv2d(64,n_classes, kernel_size=1)\n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.fconv(x)#3->64\n",
    "        x = self.fbn(x)\n",
    "        x = self.frelu(x)\n",
    "        print(x.shape)\n",
    "        x1,p = self.encoder1(x)#64->128   #P:256,256 x1 :512,512\n",
    "        print(x1.shape, p.shape)\n",
    "        x2,p = self.encoder2(p)#128->256 #P:128,128 x2:256,256\n",
    "        print(x2.shape, p.shape)\n",
    "        x3,p = self.encoder3(p)#256->512#p:64,64 x3:128,128\n",
    "        print(x3.shape, p.shape)\n",
    "        x4,p = self.encoder4(p)#512->1024#p:32,32 x4:64,64\n",
    "        print(x4.shape, p.shape)\n",
    "        xm = self.middleconv(p)#1024->1024#32,32\n",
    "        print(xm.shape, p.shape)\n",
    "        x = self.decoder4(xm,x4)#뉴런:1024->512->512 #출력tensor:64,64\n",
    "        x = self.decoder3(x,x3)#뉴런:512->256->256 #출력tensor:128,128\n",
    "        x = self.decoder2(x,x2)#뉴런:256->128->128 #출력tensor:256,256\n",
    "        x = self.decoder1(x,x1)#뉴런:128->64->64 #출력tensor:512,512\n",
    "        x = self.segmap(x)\n",
    "    \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdb7c3b",
   "metadata": {},
   "source": [
    "## ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "baf22f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class UNetWithResnet50Encoder(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        # ResNet50 백본을 불러옵니다.\n",
    "        self.resnet = models.resnet50(pretrained=True)\n",
    "        \n",
    "        # 인코더 부분\n",
    "        self.encoder0 = nn.Sequential(\n",
    "            self.resnet.conv1,\n",
    "            self.resnet.bn1,\n",
    "            self.resnet.relu,\n",
    "            self.resnet.maxpool\n",
    "        )\n",
    "        self.encoder1 = self.resnet.layer1\n",
    "        self.encoder2 = self.resnet.layer2\n",
    "        self.encoder3 = self.resnet.layer3\n",
    "        self.encoder4 = self.resnet.layer4\n",
    "        \n",
    "        # 중간 컨볼루션\n",
    "        self.middle_conv = nn.Sequential(\n",
    "            nn.Conv2d(2048, 1024, kernel_size=3, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # 디코더 부분\n",
    "        self.decoder3 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(1024+1024, 512, kernel_size=2, stride=2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(512+512, 256, kernel_size=2, stride=2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # 디코더 부분\n",
    "        self.decoder1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256+256, 128, kernel_size=2, stride=2),  # 출력 채널 수를 128로 변경\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder0 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128+64, n_classes, kernel_size=2, stride=2),  # 입력 채널 수를 192로 변경\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 인코더 부분\n",
    "        x0 = self.encoder0(x)\n",
    "        x1 = self.encoder1(x0)\n",
    "        x2 = self.encoder2(x1)\n",
    "        x3 = self.encoder3(x2)\n",
    "        x4 = self.encoder4(x3)\n",
    "        \n",
    "        # 중간 컨볼루션 후 업샘플링\n",
    "        middle = self.middle_conv(x4)\n",
    "        middle_upsampled = nn.functional.interpolate(middle, scale_factor=2, mode='bilinear', align_corners=True)\n",
    "\n",
    "        # 디코더 부분 (skip connection 추가)\n",
    "        d3 = self.decoder3(torch.cat([middle_upsampled, x3], dim=1))\n",
    "        d2 = self.decoder2(torch.cat([d3, x2], dim=1))\n",
    "        d1 = self.decoder1(torch.cat([d2, x1], dim=1))\n",
    "        out = self.decoder0(torch.cat([d1, x0], dim=1))\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd34483e",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_LR = 0.01\n",
    "END_LR = 0.01\n",
    "EP = 30\n",
    "BATCH_SIZE = 12\n",
    "ACCMULATION_STEP = 12 \n",
    "N_CLASSES = 13 #IoU 점수측정하기 위한 클래스의 개수\n",
    "WUP_ITERS = 10  # 웜업을 위한 반복 횟수\n",
    "# model 초기화\n",
    "#model = Unet_resnet18(n_classes = N_CLASSES).to(device)\n",
    "#model = ResNet50(num_classes=N_CLASSES).to(device)\n",
    "model = Unet(n_classes = N_CLASSES).to(device)\n",
    "\n",
    "\n",
    "# loss function과 optimizer 정의\n",
    "criterion =nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=BASE_LR)\n",
    "optimizer.zero_grad() \n",
    "# Warmup을 위한 스케줄러 설정\n",
    "scheduler_warmup = WarmUpLR(optimizer, WUP_ITERS)\n",
    "\n",
    "dataset = CustomDataset(csv_file=os.path.join(\"/mnt/nas27/Dataset/Samsung_DM\",'./train_source.csv'), transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=8)\n",
    "valid_dataset = CustomDataset(csv_file=os.path.join(\"/mnt/nas27/Dataset/Samsung_DM\",'./val_source.csv'), transform=transform)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=8)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a0895765-fba0-4fd9-b955-a6c0e43012e9",
   "metadata": {},
   "source": [
    "## Model Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dec3e516",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/183 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 64, 252, 252])\n",
      "Bottleneck: torch.Size([12, 128, 254, 254])\n",
      "Bottleneck: torch.Size([12, 128, 256, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/183 [00:07<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (256) must match the size of tensor b (254) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/MMI24leehyunwook/Design_project/U-net_Resnet_LBW copy.ipynb 셀 18\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B163.239.13.23/home/MMI24leehyunwook/Design_project/U-net_Resnet_LBW%20copy.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m images \u001b[39m=\u001b[39m images\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B163.239.13.23/home/MMI24leehyunwook/Design_project/U-net_Resnet_LBW%20copy.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m masks \u001b[39m=\u001b[39m masks\u001b[39m.\u001b[39mlong()\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B163.239.13.23/home/MMI24leehyunwook/Design_project/U-net_Resnet_LBW%20copy.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(images)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B163.239.13.23/home/MMI24leehyunwook/Design_project/U-net_Resnet_LBW%20copy.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, masks)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B163.239.13.23/home/MMI24leehyunwook/Design_project/U-net_Resnet_LBW%20copy.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.conda/envs/net_bw/lib/python3.8/site-packages/torch/nn/modules/module.py:889\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    888\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 889\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    890\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(\n\u001b[1;32m    891\u001b[0m         _global_forward_hooks\u001b[39m.\u001b[39mvalues(),\n\u001b[1;32m    892\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[1;32m    893\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n",
      "\u001b[1;32m/home/MMI24leehyunwook/Design_project/U-net_Resnet_LBW copy.ipynb 셀 18\u001b[0m line \u001b[0;36m8\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B163.239.13.23/home/MMI24leehyunwook/Design_project/U-net_Resnet_LBW%20copy.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=77'>78</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfrelu(x)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B163.239.13.23/home/MMI24leehyunwook/Design_project/U-net_Resnet_LBW%20copy.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=78'>79</a>\u001b[0m \u001b[39mprint\u001b[39m(x\u001b[39m.\u001b[39mshape)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B163.239.13.23/home/MMI24leehyunwook/Design_project/U-net_Resnet_LBW%20copy.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=79'>80</a>\u001b[0m x1,p \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder1(x)\u001b[39m#64->128   #P:256,256 x1 :512,512\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B163.239.13.23/home/MMI24leehyunwook/Design_project/U-net_Resnet_LBW%20copy.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=80'>81</a>\u001b[0m \u001b[39mprint\u001b[39m(x1\u001b[39m.\u001b[39mshape, p\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B163.239.13.23/home/MMI24leehyunwook/Design_project/U-net_Resnet_LBW%20copy.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=81'>82</a>\u001b[0m x2,p \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder2(p)\u001b[39m#128->256 #P:128,128 x2:256,256\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/net_bw/lib/python3.8/site-packages/torch/nn/modules/module.py:889\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    888\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 889\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    890\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(\n\u001b[1;32m    891\u001b[0m         _global_forward_hooks\u001b[39m.\u001b[39mvalues(),\n\u001b[1;32m    892\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[1;32m    893\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n",
      "\u001b[1;32m/home/MMI24leehyunwook/Design_project/U-net_Resnet_LBW copy.ipynb 셀 18\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B163.239.13.23/home/MMI24leehyunwook/Design_project/U-net_Resnet_LBW%20copy.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m,x):\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B163.239.13.23/home/MMI24leehyunwook/Design_project/U-net_Resnet_LBW%20copy.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvblock1(x)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B163.239.13.23/home/MMI24leehyunwook/Design_project/U-net_Resnet_LBW%20copy.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconvblock2(x)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B163.239.13.23/home/MMI24leehyunwook/Design_project/U-net_Resnet_LBW%20copy.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m     p \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmaxpool(x)\n",
      "File \u001b[0;32m~/.conda/envs/net_bw/lib/python3.8/site-packages/torch/nn/modules/module.py:889\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    888\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 889\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    890\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(\n\u001b[1;32m    891\u001b[0m         _global_forward_hooks\u001b[39m.\u001b[39mvalues(),\n\u001b[1;32m    892\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[1;32m    893\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n",
      "\u001b[1;32m/home/MMI24leehyunwook/Design_project/U-net_Resnet_LBW copy.ipynb 셀 18\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B163.239.13.23/home/MMI24leehyunwook/Design_project/U-net_Resnet_LBW%20copy.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mBottleneck:\u001b[39m\u001b[39m\"\u001b[39m,out\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B163.239.13.23/home/MMI24leehyunwook/Design_project/U-net_Resnet_LBW%20copy.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=43'>44</a>\u001b[0m identity \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshortcut(identity)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B163.239.13.23/home/MMI24leehyunwook/Design_project/U-net_Resnet_LBW%20copy.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=45'>46</a>\u001b[0m out \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m identity\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B163.239.13.23/home/MMI24leehyunwook/Design_project/U-net_Resnet_LBW%20copy.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=46'>47</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu3(out)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B163.239.13.23/home/MMI24leehyunwook/Design_project/U-net_Resnet_LBW%20copy.ipynb#X23sdnNjb2RlLXJlbW90ZQ%3D%3D?line=47'>48</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mBottleneck:\u001b[39m\u001b[39m\"\u001b[39m,out\u001b[39m.\u001b[39mshape)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (256) must match the size of tensor b (254) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "import random\n",
    "torch.cuda.empty_cache()\n",
    "for epoch in range(EP):\n",
    "    # 클래스별 IoU를 누적할 리스트 초기화\n",
    "    train_class_ious = []\n",
    "    # 학습\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for images, masks in tqdm(dataloader):\n",
    "        images = images.float().to(device)\n",
    "        masks = masks.long().to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks)\n",
    "        loss.backward()\n",
    "        if (epoch+1) % ACCMULATION_STEP == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            # Warmup 스케줄러 업데이트\n",
    "            # if epoch < WUP_ITERS:\n",
    "            #     scheduler_warmup.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # train 클래스별 IoU 계산\n",
    "        outputs = torch.softmax(outputs, dim=1).cpu()\n",
    "        outputs = torch.argmax(outputs, dim=1).numpy()\n",
    "\n",
    "        for class_id in range(N_CLASSES):\n",
    "            iou = calculate_iou_per_class(np.array(masks.cpu()), np.array(outputs), class_id)\n",
    "            train_class_ious.append(iou)\n",
    "\n",
    "    train_class_ious = np.array(train_class_ious).reshape(N_CLASSES, -1)\n",
    "    train_class_ious = np.mean(train_class_ious, axis=1)\n",
    "    print(\"--IoU Scores Train--\")\n",
    "    for class_id, iou in enumerate(train_class_ious):\n",
    "        print(f'Class{class_id}: {iou:.4f}', end=\" \")\n",
    "        if (class_id+1) % 6 == 0:\n",
    "            print()\n",
    "\n",
    "    # mIoU 계산\n",
    "    train_mIoU = np.mean(train_class_ious)\n",
    "\n",
    "    # validation\n",
    "    val_loss = 0\n",
    "    val_class_ious = []  # 클래스별 IoU를 누적할 리스트 초기화\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for images, masks in tqdm(valid_dataloader):\n",
    "            images = images.float().to(device)\n",
    "            masks = masks.long().to(device)\n",
    "            outputs = model(images)\n",
    "\n",
    "            # validation loss 계산\n",
    "            val_loss += criterion(outputs, masks).item()\n",
    "\n",
    "            # validation 클래스별 IoU 계산\n",
    "            outputs = torch.softmax(outputs, dim=1).cpu()\n",
    "            outputs = torch.argmax(outputs, dim=1).numpy()\n",
    "\n",
    "            for class_id in range(N_CLASSES):\n",
    "                iou = calculate_iou_per_class(np.array(masks.cpu()), np.array(outputs), class_id)\n",
    "                val_class_ious.append(iou)\n",
    "\n",
    "    val_class_ious = np.array(val_class_ious).reshape(N_CLASSES, -1)\n",
    "    val_class_ious = np.mean(val_class_ious, axis=1)\n",
    "    print(\"--IoU Scores Valid--\")\n",
    "    for class_id, iou in enumerate(val_class_ious):\n",
    "        print(f'Class{class_id}: {iou:.4f}', end=\" \")\n",
    "        if (class_id+1) % 6 == 0:\n",
    "            print(\"\")\n",
    "\n",
    "    # mIoU 계산\n",
    "    val_mIoU = np.mean(val_class_ious)\n",
    "\n",
    "    # 에폭마다 결과 출력 \n",
    "    print(f\"\\nEpoch{epoch+1}\")\n",
    "    print(f\"Train Loss: {(epoch_loss/len(dataloader))}, Train mIoU Score: {train_mIoU:.4f}\")\n",
    "    print(f\"Validation Loss: {val_loss/len(valid_dataloader)}, Validation mIoU Score: {val_mIoU:.4f}\")\n",
    "    print(\"___________________________________________________________________________________________\\n\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c32eb51c-a3fe-4e11-a616-3a717ba16f7e",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12371c8b-0c78-47df-89ec-2d8b55c8ea94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dataset = CustomDataset(csv_file='./test.csv', transform=transform, infer=True)\n",
    "# test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355b431c-ac8e-4c40-9046-4d53e4bab14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#     model.eval()\n",
    "#     result = []\n",
    "#     for images in tqdm(test_dataloader):\n",
    "#         images = images.float().to(device)\n",
    "#         outputs = model(images)\n",
    "#         outputs = torch.softmax(outputs, dim=1).cpu()\n",
    "#         outputs = torch.argmax(outputs, dim=1).numpy()\n",
    "#         # batch에 존재하는 각 이미지에 대해서 반복\n",
    "#         for pred in outputs:\n",
    "#             pred = pred.astype(np.uint8)\n",
    "#             pred = Image.fromarray(pred) # 이미지로 변환\n",
    "#             pred = pred.resize((960, 540), Image.NEAREST) # 960 x 540 사이즈로 변환\n",
    "#             pred = np.array(pred) # 다시 수치로 변환\n",
    "#             # class 0 ~ 11에 해당하는 경우에 마스크 형성 / 12(배경)는 제외하고 진행\n",
    "#             for class_id in range(12):\n",
    "#                 class_mask = (pred == class_id).astype(np.uint8)\n",
    "#                 if np.sum(class_mask) > 0: # 마스크가 존재하는 경우 encode\n",
    "#                     mask_rle = rle_encode(class_mask)\n",
    "#                     result.append(mask_rle)\n",
    "#                 else: # 마스크가 존재하지 않는 경우 -1\n",
    "#                     result.append(-1)\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "36c2cbbb-04f1-4f9c-b4df-4b744dfce046",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ac2a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# submit = pd.read_csv('./sample_submission.csv')\n",
    "# submit['mask_rle'] = result\n",
    "# submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da10cb6f-0826-4755-a376-97b695ae8f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# submit.to_csv('./baseline_submit.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seungyoon2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
