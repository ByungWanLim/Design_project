{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d73d24e3-5c9e-4ade-9e6e-ca6f46a2d914",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad9b681e-370a-4cfa-a452-dd2d7f0cd77f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "\n",
    "from tqdm import tqdm\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "\n",
    "# GPU 사용이 가능할 경우, GPU를 사용할 수 있게 함.'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = torch.device(device)\n",
    "print(device)\n",
    "\n",
    "print(os.environ.get('CUDA_VISIBLE_DEVICES'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "20ff3de5-0d0e-497b-ac75-d5179a3f65d3",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "838e1d83-8670-407b-82f6-bf9652f58639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RLE 인코딩 함수\n",
    "def rle_encode(mask):\n",
    "    pixels = mask.flatten()\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)\n",
    "\n",
    "# 클래스별 IoU를 계산하기 위한 함수\n",
    "def calculate_iou_per_class(y_true, y_pred, class_id):\n",
    "    intersection = np.sum((y_true == class_id) & (y_pred == class_id))\n",
    "    union = np.sum((y_true == class_id) | (y_pred == class_id))\n",
    "    iou = intersection / union if union > 0 else 0\n",
    "    return iou"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "be76a29e-e9c2-411a-a569-04166f074184",
   "metadata": {},
   "source": [
    "## Dataset, Data Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5acf65a",
   "metadata": {},
   "source": [
    "출력이미지 크기 키우기->ex) resnet 2048->1024->512->256 conv 256->512->1024->2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8496767-2f64-4285-bec4-c6f53a1fd9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None, infer=False):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "        self.infer = infer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        directory_path = \"/mnt/nas27/Dataset/Samsung_DM\"\n",
    "        img_path = self.data.iloc[idx, 1]\n",
    "        img_path = os.path.join(directory_path, img_path[2:])\n",
    "        image = cv2.imread(img_path)\n",
    "        if image is None:\n",
    "            raise ValueError(f\"Failed to load image at {img_path}\")\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        if self.infer:\n",
    "            if self.transform:\n",
    "                image = self.transform(image=image)['image']\n",
    "            return image\n",
    "        \n",
    "        mask_path = self.data.iloc[idx, 2]\n",
    "        mask_path = os.path.join(directory_path, mask_path[2:])\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if mask is None:\n",
    "            raise ValueError(f\"Failed to load mask at {mask_path}\")\n",
    "        mask[mask == 255] = 12\n",
    "\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "    \n",
    "    # 이미지 전처리 클래스\n",
    "class ImageTransform():\n",
    "  \"\"\"\n",
    "  훈련, 검증 동작 다르게 설정\n",
    "  이미지 크기 resize, 색상 표준화\n",
    "  훈련시 RandomResizedCrop, RandomHorizontalFilp으로 데이터 확장\n",
    "  \"\"\"\n",
    "  def __init__(self, resize, mean, std):\n",
    "    self.data_transform = {\n",
    "        'train' : transforms.Compose([\n",
    "            #transforms.RandomResizedCrop(\n",
    "            #    resize, scale = (0.5, 1.0)), # 데이터 확장\n",
    "            transforms.RandomHorizontalFlip(), # 데이터 확장\n",
    "            transforms.ToTensor(), # Tensor로 변환\n",
    "            transforms.Normalize(mean = mean, std = std) #표준화\n",
    "        ]),\n",
    "        'test': transforms.Compose([\n",
    "            #transforms.Resize(resize), # Resize\n",
    "            #transforms.CenterCrop(resize), # 중앙을 resize*resize로 crop\n",
    "            transforms.ToTensor(), # Tensor로 변환\n",
    "            transforms.Normalize(mean = mean, std = std) # 표준화\n",
    "        ])\n",
    "\n",
    "    }\n",
    "  def __call__(self, img, phase = 'train'):\n",
    "    \"\"\"\n",
    "    phase : 'train' or 'test'\n",
    "    전처리 모드 지정\n",
    "    \"\"\"\n",
    "    return self.data_transform[phase](img)\n",
    "\n",
    "\n",
    "transform = A.Compose(\n",
    "    [   \n",
    "        A.Resize(224, 224),\n",
    "        #A.Resize(128, 128),\n",
    "        A.Normalize(),\n",
    "        \n",
    "        # 변형\n",
    "        #A.VerticalFlip(p=0.5),\n",
    "        # A.RandomRotate90(p=0.5),\n",
    "        #A.HueSaturationValue(p=0.2),\n",
    "        \n",
    "        ToTensorV2()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a644a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function CustomDataset.__len__ at 0x7faa94e11320>\n"
     ]
    }
   ],
   "source": [
    "print(CustomDataset.__len__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a91d6b1",
   "metadata": {},
   "source": [
    "Warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9e62c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class WarmUpLR(_LRScheduler):\n",
    "#     def __init__(self, optimizer, total_iters, last_epoch=-1):\n",
    "#         self.total_iters = total_iters\n",
    "#         super(WarmUpLR, self).__init__(optimizer, last_epoch)\n",
    "\n",
    "#     def get_lr(self):\n",
    "#         return [base_lr * self.last_epoch / self.total_iters for base_lr in self.base_lrs]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f42501fc-b573-4893-a7c4-5e280dfdaf09",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "867c4b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeadBlock(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_channels,channels, stride=1):\n",
    "        super(HeadBlock, self).__init__()\n",
    "\n",
    "        # 1x1 convolution\n",
    "        self.conv1 = nn.Conv2d(in_channels, channels, kernel_size=1, stride=stride, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(channels)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "\n",
    "        # 3x3 convolution\n",
    "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(channels)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "\n",
    "        # 1x1 convolution\n",
    "        self.conv3 = nn.Conv2d(channels, channels * self.expansion, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(channels * self.expansion)\n",
    "\n",
    "\n",
    "        self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(channels, channels * self.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(channels * self.expansion)\n",
    "            )\n",
    "        self.relu3 = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu1(out)\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu2(out)\n",
    "         \n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "        \n",
    "        \n",
    "        # Check the size of identity and out\n",
    "        if identity.size() != out.size():\n",
    "            identity = F.interpolate(identity, size=out.size()[2:])\n",
    "        identity = self.shortcut(identity)\n",
    "        out += identity\n",
    "        out = self.relu3(out)\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3574888b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unet의 기본이 되는 conv블럭\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels,kernel_size = 3):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size, padding=1)  # 여기서 in_channels는 out_channels와 동일해야 합니다.\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "        return x\n",
    "\n",
    "class IdentityBlock(nn.Module):\n",
    "    def __init__(self, in_channels, mid_channels, out_channels, stride=1):\n",
    "        super(IdentityBlock, self).__init__()\n",
    "        \n",
    "        # 1x1 convolution\n",
    "        self.conv1 = nn.Conv2d(in_channels, mid_channels, kernel_size=1, stride=stride, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(mid_channels)\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "        # 3x3 convolution\n",
    "        self.conv2 = nn.Conv2d(mid_channels, mid_channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(mid_channels)\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        # 1x1 convolution\n",
    "        self.conv3 = nn.Conv2d(mid_channels, out_channels, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu1(out)\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu2(out)\n",
    "         \n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "        out = self.relu3(out)\n",
    "        \n",
    "        return out\n",
    "class HeadBlock(IdentityBlock):\n",
    "    def __init__(self, in_channels, mid_channels, out_channels, stride=1):\n",
    "        super(HeadBlock, self).__init__(in_channels, mid_channels, out_channels, stride)\n",
    "        \n",
    "        self.shortcut = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = super().forward(x)\n",
    "        \n",
    "        if identity.size() != out.size():\n",
    "            identity = F.interpolate(identity, size=out.size()[2:])\n",
    "        identity = self.shortcut(identity)\n",
    "        \n",
    "        out += identity\n",
    "        out = self.relu3(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "#인코더 블럭\n",
    "class Conv2(nn.Module):\n",
    "    def __init__(self,in_channels, mid_channels, out_channels):\n",
    "        super(Conv2,self).__init__() \n",
    "        self.headblock = HeadBlock(in_channels,mid_channels,out_channels)\n",
    "        self.identityblock1 = IdentityBlock(out_channels,mid_channels,out_channels)\n",
    "        self.identityblock2 = IdentityBlock(out_channels,mid_channels,out_channels)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "    def forward(self,x):\n",
    "        x = self.headblock(x)\n",
    "        x = self.identityblock1(x)\n",
    "        x = self.identityblock2(x)\n",
    "        p = self.maxpool(x)\n",
    "        return x , p\n",
    "class Conv3(nn.Module):\n",
    "    def __init__(self,in_channels, mid_channels, out_channels):\n",
    "        super(Conv3,self).__init__() \n",
    "        self.headblock = HeadBlock(in_channels,mid_channels,out_channels)\n",
    "        self.identityblock1 = IdentityBlock(out_channels,mid_channels,out_channels)\n",
    "        self.identityblock2 = IdentityBlock(out_channels,mid_channels,out_channels)\n",
    "        self.identityblock3 = IdentityBlock(out_channels,mid_channels,out_channels)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "    def forward(self,x):\n",
    "        x = self.headblock(x)\n",
    "        x = self.identityblock1(x)\n",
    "        x = self.identityblock2(x)\n",
    "        x = self.identityblock3(x)\n",
    "        p = self.maxpool(x)\n",
    "        return x , p\n",
    "class Conv4(nn.Module):\n",
    "    def __init__(self,in_channels, mid_channels, out_channels):\n",
    "        super(Conv4,self).__init__() \n",
    "        self.headblock = HeadBlock(in_channels,mid_channels,out_channels)\n",
    "        self.identityblock1 = IdentityBlock(out_channels,mid_channels,out_channels)\n",
    "        self.identityblock2 = IdentityBlock(out_channels,mid_channels,out_channels)\n",
    "        self.identityblock3 = IdentityBlock(out_channels,mid_channels,out_channels)\n",
    "        self.identityblock4 = IdentityBlock(out_channels,mid_channels,out_channels)\n",
    "        self.identityblock5 = IdentityBlock(out_channels,mid_channels,out_channels)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "    def forward(self,x):\n",
    "        x = self.headblock(x)\n",
    "        x = self.identityblock1(x)\n",
    "        x = self.identityblock2(x)\n",
    "        x = self.identityblock3(x)\n",
    "        x = self.identityblock4(x)\n",
    "        x = self.identityblock5(x)\n",
    "        p = self.maxpool(x)\n",
    "        return x , p\n",
    "class Conv5(nn.Module):\n",
    "    def __init__(self,in_channels, mid_channels, out_channels):\n",
    "        super(Conv5,self).__init__() \n",
    "        self.headblock = HeadBlock(in_channels,mid_channels,out_channels)\n",
    "        self.identityblock1 = IdentityBlock(out_channels,mid_channels,out_channels)\n",
    "        self.identityblock2 = IdentityBlock(out_channels,mid_channels,out_channels)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "    def forward(self,x):\n",
    "        x = self.headblock(x)\n",
    "        x = self.identityblock1(x)\n",
    "        x = self.identityblock2(x)\n",
    "        p = self.maxpool(x)\n",
    "        return x , p\n",
    "#디코더 블럭\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.upsample = nn.ConvTranspose2d(channels*2, channels, kernel_size=4, stride=2, padding=1, output_padding=1) # output_padding 추가\n",
    "        self.convblock1 = ConvBlock(channels*2, channels)\n",
    "\n",
    "    def forward(self, x, skip):\n",
    "        x = self.upsample(x)\n",
    "        if x.size(2) != skip.size(2) or x.size(3) != skip.size(3):\n",
    "            x = F.interpolate(x, size=(skip.size(2), skip.size(3)))\n",
    "        x = torch.cat([x, skip], dim=1)\n",
    "        x = self.convblock1(x)\n",
    "        return x\n",
    "\n",
    "#Unet구조 middle의 xm값의 움직임에 주의\n",
    "class Unet(nn.Module):\n",
    "    def __init__(self,n_classes):\n",
    "        super(Unet,self).__init__()\n",
    "        self.fconv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2)\n",
    "        self.fbn1 = nn.BatchNorm2d(64)\n",
    "        self.frelu1 = nn.ReLU()\n",
    "        self.fconv2 = nn.Conv2d(64, 128, kernel_size=1, stride=1)\n",
    "        self.fbn2 = nn.BatchNorm2d(128)\n",
    "        self.frelu2 = nn.ReLU()\n",
    "        self.fmaxpooling = nn.MaxPool2d(kernel_size=3,stride=2)\n",
    "        \n",
    "        self.conv2 = Conv2(128,64,256)\n",
    "        self.conv3 = Conv3(256,128,512)\n",
    "        self.conv4 = Conv4(512,256,1024)\n",
    "        self.conv5 = Conv5(1024,512,2048)\n",
    "        \n",
    "        self.middleconv = ConvBlock(2048,4096)\n",
    "        self.dropout = nn.Dropout2d(0.4) #\n",
    "           \n",
    "        self.decoder5 = DecoderBlock(2048)\n",
    "        self.decoder4 = DecoderBlock(1024)\n",
    "        self.decoder3 = DecoderBlock(512)\n",
    "        self.decoder2 = DecoderBlock(256)\n",
    "        self.decoder1 = DecoderBlock(128)\n",
    "        \n",
    "        self.segmap = nn.Conv2d(128,n_classes, kernel_size=1)\n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.fconv1(x)#3->64\n",
    "        x = self.fbn1(x)\n",
    "        x = self.frelu1(x)\n",
    "        x = self.fconv2(x)#3->64\n",
    "        x = self.fbn2(x)\n",
    "        x1 = self.frelu2(x)\n",
    "        p = self.fmaxpooling(x)#첫 conv: x0([8, 64, 109, 109]) p([8, 64, 54, 54])\n",
    "        x2,p = self.conv2(p)#conv2:  x1:([8, 256, 54, 54]) p([8, 256, 26, 26])\n",
    "        x3,p = self.conv3(p)#conv3:  x2([8, 512, 26, 26]) p([8, 512, 12, 12])\n",
    "        x4,p = self.conv4(p)#conv4:  x3([8, 1024, 12, 12]) p([8, 1024, 5, 5])\n",
    "        x5,p = self.conv5(p)#conv5:  x4([8, 2048, 5, 5]) p([8, 2048, 2, 2])\n",
    "        \n",
    "        xm = self.middleconv(p)#xm([8, 4096, 2, 2])\n",
    "        xm = self.dropout(xm)\n",
    "        \n",
    "        x = self.decoder5(xm,x5)#뉴런:2048*2->2048 1\n",
    "        x = self.decoder4(x,x4)#뉴런:1024*2->1024 \n",
    "        x = self.decoder3(x,x3) #14\n",
    "        x = self.decoder2(x,x2)#28\n",
    "        x = self.decoder1(x,x1)#55\n",
    "        \n",
    "        x = F.interpolate(x, size=(224, 224))\n",
    "        x = self.segmap(x)\n",
    "        #print(x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4773eff0",
   "metadata": {},
   "source": [
    "## 파라미터 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd34483e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BASE_LR = 0.001\n",
    "END_LR = 0.01\n",
    "EP = 30\n",
    "BATCH_SIZE = 4\n",
    "ACCMULATION_STEP = 3 \n",
    "N_CLASSES = 13 #IoU 점수측정하기 위한 클래스의 개수\n",
    "WUP_ITERS = 10  # 웜업을 위한 반복 횟수\n",
    "# model 초기화\n",
    "#model = Unet_resnet18(n_classes = N_CLASSES).to(device)\n",
    "#model = ResNet50(num_classes=N_CLASSES).to(device)\n",
    "model = Unet(n_classes = N_CLASSES).to(device)\n",
    "\n",
    "\n",
    "# loss function과 optimizer 정의\n",
    "criterion =nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=BASE_LR)\n",
    "optimizer.zero_grad() \n",
    "# Warmup을 위한 스케줄러 설정\n",
    "#scheduler_warmup = WarmUpLR(optimizer, WUP_ITERS)\n",
    "\n",
    "dataset = CustomDataset(csv_file=os.path.join(\"/mnt/nas27/Dataset/Samsung_DM\",'./train_source.csv'), transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=8,pin_memory=True)\n",
    "valid_dataset = CustomDataset(csv_file=os.path.join(\"/mnt/nas27/Dataset/Samsung_DM\",'./val_source.csv'), transform=transform)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=8,pin_memory=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a0895765-fba0-4fd9-b955-a6c0e43012e9",
   "metadata": {},
   "source": [
    "## Model Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dec3e516",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 549/549 [03:24<00:00,  2.69it/s]\n",
      "100%|██████████| 117/117 [00:14<00:00,  7.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--IoU Scores Valid--\n",
      "Class0: 0.3499 Class1: 0.3300 Class2: 0.3478 Class3: 0.3582 Class4: 0.3506 Class5: 0.3467 \n",
      "Class6: 0.3592 Class7: 0.3317 Class8: 0.3293 Class9: 0.3479 Class10: 0.3340 Class11: 0.3259 \n",
      "Class12: 0.3459 \n",
      "Epoch1\n",
      "Train Loss: 0.5832888638712669\n",
      "Validation Loss: 0.593081710430292, Validation mIoU Score: 0.3429\n",
      "___________________________________________________________________________________________\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 549/549 [03:24<00:00,  2.68it/s]\n",
      "100%|██████████| 117/117 [00:15<00:00,  7.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--IoU Scores Valid--\n",
      "Class0: 0.3408 Class1: 0.3380 Class2: 0.3385 Class3: 0.3670 Class4: 0.3543 Class5: 0.3566 \n",
      "Class6: 0.3518 Class7: 0.3483 Class8: 0.3292 Class9: 0.3598 Class10: 0.3417 Class11: 0.3413 \n",
      "Class12: 0.3385 \n",
      "Epoch2\n",
      "Train Loss: 0.41920589922990087\n",
      "Validation Loss: 0.6650938720275195, Validation mIoU Score: 0.3466\n",
      "___________________________________________________________________________________________\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 549/549 [03:24<00:00,  2.68it/s]\n",
      "100%|██████████| 117/117 [00:15<00:00,  7.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--IoU Scores Valid--\n",
      "Class0: 0.3105 Class1: 0.3043 Class2: 0.3073 Class3: 0.3233 Class4: 0.3223 Class5: 0.3128 \n",
      "Class6: 0.3107 Class7: 0.3136 Class8: 0.3033 Class9: 0.3240 Class10: 0.3127 Class11: 0.3089 \n",
      "Class12: 0.3144 \n",
      "Epoch3\n",
      "Train Loss: 0.35932408788925096\n",
      "Validation Loss: 0.8231237400800754, Validation mIoU Score: 0.3129\n",
      "___________________________________________________________________________________________\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 549/549 [03:24<00:00,  2.68it/s]\n",
      "100%|██████████| 117/117 [00:15<00:00,  7.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--IoU Scores Valid--\n",
      "Class0: 0.3829 Class1: 0.3738 Class2: 0.3774 Class3: 0.3908 Class4: 0.3766 Class5: 0.3776 \n",
      "Class6: 0.3805 Class7: 0.3686 Class8: 0.3598 Class9: 0.3785 Class10: 0.3685 Class11: 0.3601 \n",
      "Class12: 0.3696 \n",
      "Epoch4\n",
      "Train Loss: 0.3328845322620673\n",
      "Validation Loss: 0.49464141634794384, Validation mIoU Score: 0.3742\n",
      "___________________________________________________________________________________________\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 549/549 [03:24<00:00,  2.69it/s]\n",
      "100%|██████████| 117/117 [00:15<00:00,  7.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--IoU Scores Valid--\n",
      "Class0: 0.3782 Class1: 0.3684 Class2: 0.3810 Class3: 0.4032 Class4: 0.3847 Class5: 0.3813 \n",
      "Class6: 0.3876 Class7: 0.3789 Class8: 0.3526 Class9: 0.3801 Class10: 0.3848 Class11: 0.3560 \n",
      "Class12: 0.3689 \n",
      "Epoch5\n",
      "Train Loss: 0.300196266673736\n",
      "Validation Loss: 0.485473684520803, Validation mIoU Score: 0.3774\n",
      "___________________________________________________________________________________________\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 11/549 [00:05<03:25,  2.62it/s]"
     ]
    }
   ],
   "source": [
    "import random\n",
    "torch.cuda.empty_cache()\n",
    "for epoch in range(EP):\n",
    "    # 클래스별 IoU를 누적할 리스트 초기화\n",
    "    train_class_ious = []\n",
    "    # 학습\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for images, masks in tqdm(dataloader):\n",
    "        images = images.float().to(device)\n",
    "        masks = masks.long().to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        # if (epoch+1) % ACCMULATION_STEP == 0:\n",
    "        #     optimizer.step()\n",
    "        #     optimizer.zero_grad()\n",
    "        #     # Warmup 스케줄러 업데이트\n",
    "            # if epoch < WUP_ITERS:\n",
    "            #     scheduler_warmup.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "    '''\n",
    "    #    # train 클래스별 IoU 계산\n",
    "    #    outputs = torch.softmax(outputs, dim=1).cpu()\n",
    "    #    outputs = torch.argmax(outputs, dim=1).numpy()\n",
    "\n",
    "    #     for class_id in range(N_CLASSES):\n",
    "    #         iou = calculate_iou_per_class(np.array(masks.cpu()), np.array(outputs), class_id)\n",
    "    #         train_class_ious.append(iou)\n",
    "\n",
    "    # train_class_ious = np.array(train_class_ious).reshape(N_CLASSES, -1)\n",
    "    # train_class_ious = np.mean(train_class_ious, axis=1)\n",
    "    # print(\"--IoU Scores Train--\")\n",
    "    # for class_id, iou in enumerate(train_class_ious):\n",
    "    #     print(f'Class{class_id}: {iou:.4f}', end=\" \")\n",
    "    #     if (class_id+1) % 6 == 0:\n",
    "    #         print()\n",
    "\n",
    "    ## mIoU 계산\n",
    "    #train_mIoU = np.mean(train_class_ious)\n",
    "    '''\n",
    "    # ___________________validation_____________________\n",
    "    val_loss = 0\n",
    "    val_class_ious = []  # 클래스별 IoU를 누적할 리스트 초기화\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for images, masks in tqdm(valid_dataloader):\n",
    "            images = images.float().to(device)\n",
    "            masks = masks.long().to(device)\n",
    "            outputs = model(images)\n",
    "            # validation loss 계산\n",
    "            val_loss += criterion(outputs, masks).item()\n",
    "            # validation 클래스별 IoU 계산\n",
    "            outputs = torch.softmax(outputs, dim=1).cpu()\n",
    "            outputs = torch.argmax(outputs, dim=1).numpy()\n",
    "            for class_id in range(N_CLASSES):\n",
    "                iou = calculate_iou_per_class(np.array(masks.cpu()), np.array(outputs), class_id)\n",
    "                val_class_ious.append(iou)\n",
    "    val_class_ious = np.array(val_class_ious).reshape(N_CLASSES, -1)\n",
    "    val_class_ious = np.mean(val_class_ious, axis=1)\n",
    "    print(\"--IoU Scores Valid--\")\n",
    "    for class_id, iou in enumerate(val_class_ious):\n",
    "        print(f'Class{class_id}: {iou:.4f}', end=\" \")\n",
    "        if (class_id+1) % 6 == 0:\n",
    "            print(\"\")\n",
    "    # mIoU 계산\n",
    "    val_mIoU = np.mean(val_class_ious)\n",
    "    # 에폭마다 결과 출력 \n",
    "    print(f\"\\nEpoch{epoch+1}\")\n",
    "    print(f\"Train Loss: {(epoch_loss/len(dataloader))}\")\n",
    "    print(f\"Validation Loss: {val_loss/len(valid_dataloader)}, Validation mIoU Score: {val_mIoU:.4f}\")\n",
    "    print(\"___________________________________________________________________________________________\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c32eb51c-a3fe-4e11-a616-3a717ba16f7e",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12371c8b-0c78-47df-89ec-2d8b55c8ea94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dataset = CustomDataset(csv_file='./test.csv', transform=transform, infer=True)\n",
    "# test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355b431c-ac8e-4c40-9046-4d53e4bab14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#     model.eval()\n",
    "#     result = []\n",
    "#     for images in tqdm(test_dataloader):\n",
    "#         images = images.float().to(device)\n",
    "#         outputs = model(images)\n",
    "#         outputs = torch.softmax(outputs, dim=1).cpu()\n",
    "#         outputs = torch.argmax(outputs, dim=1).numpy()\n",
    "#         # batch에 존재하는 각 이미지에 대해서 반복\n",
    "#         for pred in outputs:\n",
    "#             pred = pred.astype(np.uint8)\n",
    "#             pred = Image.fromarray(pred) # 이미지로 변환\n",
    "#             pred = pred.resize((960, 540), Image.NEAREST) # 960 x 540 사이즈로 변환\n",
    "#             pred = np.array(pred) # 다시 수치로 변환\n",
    "#             # class 0 ~ 11에 해당하는 경우에 마스크 형성 / 12(배경)는 제외하고 진행\n",
    "#             for class_id in range(12):\n",
    "#                 class_mask = (pred == class_id).astype(np.uint8)\n",
    "#                 if np.sum(class_mask) > 0: # 마스크가 존재하는 경우 encode\n",
    "#                     mask_rle = rle_encode(class_mask)\n",
    "#                     result.append(mask_rle)\n",
    "#                 else: # 마스크가 존재하지 않는 경우 -1\n",
    "#                     result.append(-1)\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "36c2cbbb-04f1-4f9c-b4df-4b744dfce046",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ac2a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# submit = pd.read_csv('./sample_submission.csv')\n",
    "# submit['mask_rle'] = result\n",
    "# submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da10cb6f-0826-4755-a376-97b695ae8f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# submit.to_csv('./baseline_submit.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seungyoon2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
