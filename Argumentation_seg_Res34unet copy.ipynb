{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/MMI24limbyungwan/.conda/envs/byungwan_resn/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "#from torch.optim.lr_scheduler import _LRScheduler\n",
    "\n",
    "from tqdm import tqdm\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "from torchvision import transforms ,models\n",
    "from torchsummary import summary\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# GPU 사용이 가능할 경우, GPU를 사용할 수 있게 함.'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = torch.device(device)\n",
    "print(device)\n",
    "\n",
    "print(os.environ.get('CUDA_VISIBLE_DEVICES'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RLE 인코딩 함수\n",
    "def rle_encode(mask):\n",
    "    pixels = mask.flatten()\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)\n",
    "\n",
    "# 클래스별 IoU를 계산하기 위한 함수\n",
    "def calculate_iou_per_class(y_true, y_pred, class_id):\n",
    "    intersection = np.sum((y_true == class_id) & (y_pred == class_id))\n",
    "    union = np.sum((y_true == class_id) | (y_pred == class_id))\n",
    "    iou = intersection / union if union > 0 else 1\n",
    "    return iou\n",
    "\n",
    "def calculate_ious(y_true, y_pred, class_num):\n",
    "    ious = []\n",
    "    for class_id in range(class_num):\n",
    "        intersection = np.sum((y_true == class_id) & (y_pred == class_id))\n",
    "        union = np.sum((y_true == class_id) | (y_pred == class_id))\n",
    "        iou = intersection / union if union > 0 else 0\n",
    "        ious.append(iou)\n",
    "    return ious\n",
    "\n",
    "def apply_fisheye_distortion(images, masks, label):\n",
    "    # 이미지 크기 가져오기\n",
    "    batch, channel, height, width = images.shape\n",
    "\n",
    "    # 카메라 매트릭스 생성\n",
    "    focal_length = width / 4\n",
    "    center_x = width / 2\n",
    "    center_y = height / 2\n",
    "    camera_matrix = np.array([[focal_length, 0, center_x],\n",
    "                              [0, focal_length, center_y],\n",
    "                              [0, 0, 1]], dtype=np.float32)\n",
    "\n",
    "    # 왜곡 계수 생성\n",
    "    dist_coeffs = np.array([0, 0.02 * label, 0, 0], dtype=np.float32)\n",
    "\n",
    "    # 왜곡 보정\n",
    "    undistorted_images = []\n",
    "    undistorted_masks = []\n",
    "\n",
    "    for i in range(batch):\n",
    "        image = images[i].permute(1, 2, 0).cpu().numpy()  # 텐서를 NumPy 배열로 변환\n",
    "        mask = masks[i].cpu().numpy()\n",
    "        undistorted_image = cv2.undistort(image, camera_matrix, dist_coeffs)\n",
    "        undistorted_mask = cv2.undistort(mask, camera_matrix, dist_coeffs)\n",
    "        undistorted_mask = np.round(undistorted_mask).astype(np.uint8)\n",
    "        undistorted_mask[undistorted_mask > 12] = 12\n",
    "\n",
    "        # 다시 텐서로 변환\n",
    "        undistorted_image = torch.from_numpy(undistorted_image).permute(2, 0, 1).float().to(device)\n",
    "        undistorted_mask = torch.from_numpy(undistorted_mask).long().to(device)\n",
    "\n",
    "        undistorted_images.append(undistorted_image)\n",
    "        undistorted_masks.append(undistorted_mask)\n",
    "\n",
    "    undistorted_images = torch.stack(undistorted_images, dim=0)\n",
    "    undistorted_masks = torch.stack(undistorted_masks, dim=0)\n",
    "\n",
    "    return undistorted_images, undistorted_masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None, infer=False):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "        self.infer = infer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #directory_path = \"/mnt/nas27/Dataset/Samsung_DM\"\n",
    "        directory_path = './data/224'\n",
    "        img_path = self.data.iloc[idx, 1]\n",
    "        img_path = os.path.join(directory_path, img_path)\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        #image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        if self.infer:\n",
    "            if self.transform:\n",
    "                image = self.transform(image=image)['image']\n",
    "            return image\n",
    "        \n",
    "        mask_path = self.data.iloc[idx, 2]\n",
    "        mask_path = os.path.join(directory_path, mask_path)\n",
    "        mask = cv2.imread(mask_path)\n",
    "        #mask = cv2.cvtColor(mask, cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)\n",
    "        mask = np.round(mask).astype(np.uint8)\n",
    "        mask[mask > 12] = 12 #배경을 픽셀값 12로 간주\n",
    "        mask += 1\n",
    "        mask[mask == 13] = 0\n",
    "\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "transform = A.Compose(\n",
    "    [   \n",
    "        A.Normalize(),\n",
    "        A.HorizontalFlip(p=0.3),\n",
    "        A.GaussNoise(var_limit=(10.0, 30.0), p=0.5),\n",
    "        # 변형\n",
    "        # A.VerticalFlip(p=0.5),\n",
    "        # A.RandomRotate90(p=0.5),\n",
    "        # A.HueSaturationValue(p=0.2),\n",
    "        \n",
    "        ToTensorV2()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradReverse(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(self, x):\n",
    "        return x.view_as(x)\n",
    "    @staticmethod\n",
    "    def backward(self, grad_output): # 역전파 시에 gradient에 음수를 취함\n",
    "        return grad_output * (-1)\n",
    "\n",
    "class domain_classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(domain_classifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(224*224*64, 10)\n",
    "        self.fc2 = nn.Linear(10, 4) # source = 0, target = 1 회귀 가정\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 224*224*64)\n",
    "        x = GradReverse.apply(x) # gradient reverse\n",
    "        x = F.leaky_relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        #return torch.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "class domain_linear(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(domain_linear, self).__init__()\n",
    "        self.fc1 = nn.Linear(224*224*64, 10)\n",
    "        self.fc2 = nn.Linear(10, 1) # source = 0, target = 1 회귀 가정\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 224*224*64)\n",
    "        x = GradReverse.apply(x) # gradient reverse\n",
    "        x = F.leaky_relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        #return torch.sigmoid(x)\n",
    "        return x\n",
    "class IdentityBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(IdentityBlock, self).__init__()\n",
    "        \n",
    "        # 3x3 convolution\n",
    "        self.conv1 = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1, stride=stride, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(in_channels)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        # 3x3 convolution\n",
    "        self.conv2 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        # Skip connection\n",
    "        self.skip = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.skip = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        \n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu1(out)\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        \n",
    "        # Adding the skip connection\n",
    "        out += self.skip(identity)\n",
    "        out = self.relu2(out)\n",
    "        \n",
    "        return out\n",
    "#인코더 블럭\n",
    "class Conv2(nn.Module):\n",
    "    def __init__(self,in_channels, out_channels):\n",
    "        super(Conv2,self).__init__() \n",
    "        self.identityblock1 = IdentityBlock(in_channels,in_channels)\n",
    "        self.identityblock2 = IdentityBlock(in_channels,in_channels)\n",
    "        self.identityblock3 = IdentityBlock(in_channels,out_channels)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2,padding=1)\n",
    "    def forward(self,x):\n",
    "        x = self.identityblock1(x)\n",
    "        x = self.identityblock2(x)\n",
    "        x = self.identityblock3(x)\n",
    "        p = self.maxpool(x)\n",
    "        \n",
    "        return x , p\n",
    "class Conv3(nn.Module):\n",
    "    def __init__(self,in_channels, out_channels):\n",
    "        super(Conv3,self).__init__()         \n",
    "        self.identityblock1 = IdentityBlock(in_channels,in_channels)\n",
    "        self.identityblock2 = IdentityBlock(in_channels,in_channels)\n",
    "        self.identityblock3 = IdentityBlock(in_channels,in_channels)\n",
    "        self.identityblock4 = IdentityBlock(in_channels,out_channels)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2,padding=1)\n",
    "    def forward(self,x):\n",
    "        x = self.identityblock1(x)\n",
    "        x = self.identityblock2(x)\n",
    "        x = self.identityblock3(x)\n",
    "        x = self.identityblock4(x)\n",
    "        p = self.maxpool(x)\n",
    "        \n",
    "        return x , p\n",
    "class Conv4(nn.Module):\n",
    "    def __init__(self,in_channels, out_channels):\n",
    "        super(Conv4,self).__init__()         \n",
    "        self.identityblock1 = IdentityBlock(in_channels,in_channels)\n",
    "        self.identityblock2 = IdentityBlock(in_channels,in_channels)\n",
    "        self.identityblock3 = IdentityBlock(in_channels,in_channels)\n",
    "        self.identityblock4 = IdentityBlock(in_channels,in_channels)\n",
    "        self.identityblock5 = IdentityBlock(in_channels,in_channels)\n",
    "        self.identityblock6 = IdentityBlock(in_channels,out_channels)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2,padding=1)\n",
    "    def forward(self,x):\n",
    "        x = self.identityblock1(x)\n",
    "        x = self.identityblock2(x)\n",
    "        x = self.identityblock3(x)\n",
    "        x = self.identityblock4(x)\n",
    "        x = self.identityblock5(x)\n",
    "        x = self.identityblock6(x)\n",
    "        p = self.maxpool(x)\n",
    "        \n",
    "        return x , p\n",
    "class Conv5(nn.Module):\n",
    "    def __init__(self,in_channels, out_channels):\n",
    "        super(Conv5,self).__init__() \n",
    "        self.identityblock1 = IdentityBlock(in_channels,in_channels)\n",
    "        self.identityblock2 = IdentityBlock(in_channels,in_channels)\n",
    "        self.identityblock3 = IdentityBlock(in_channels,out_channels)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2,padding=1)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.identityblock1(x)\n",
    "        x = self.identityblock2(x)\n",
    "        x = self.identityblock3(x)\n",
    "        p = self.maxpool(x)\n",
    "        \n",
    "        return x , p\n",
    "#디코더 블럭\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.upsample = nn.ConvTranspose2d(channels*2, channels, kernel_size=4, stride=2, padding=1) # output_padding 추가\n",
    "        self.convblock1 = IdentityBlock(channels*2, channels)\n",
    "\n",
    "    def forward(self, x, skip):\n",
    "        x = self.upsample(x)\n",
    "        if x.size(2) != skip.size(2) or x.size(3) != skip.size(3):\n",
    "            x = F.interpolate(x, size=(skip.size(2), skip.size(3)))\n",
    "        x = torch.cat([x, skip], dim=1)\n",
    "        x = self.convblock1(x)\n",
    "        #print(\"x\",x.shape,\"skip: \",skip.shape)\n",
    "        return x\n",
    "\n",
    "#Unet구조 middle의 xm값의 움직임에 주의\n",
    "class Resnet34_Unet(nn.Module):\n",
    "    def __init__(self,n_classes):\n",
    "        super(Resnet34_Unet,self).__init__()\n",
    "        self.fconv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3)\n",
    "        self.fbn1 = nn.BatchNorm2d(64)\n",
    "        self.frelu1 = nn.ReLU()\n",
    "        self.fmaxpooling = nn.MaxPool2d(kernel_size=3,stride=2,padding=1)\n",
    "        \n",
    "        self.conv2 = Conv2(64,128)\n",
    "        self.conv3 = Conv3(128,256)\n",
    "        self.conv4 = Conv4(256,512)\n",
    "        self.conv5 = Conv5(512,1024)\n",
    "        \n",
    "        self.middleconv = IdentityBlock(1024,2048)\n",
    "        self.dropout = nn.Dropout2d(0.1) #\n",
    "           \n",
    "        self.decoder5 = DecoderBlock(1024)\n",
    "        self.decoder4 = DecoderBlock(512)\n",
    "        self.decoder3 = DecoderBlock(256)\n",
    "        self.decoder2 = DecoderBlock(128)\n",
    "        self.decoder1 = DecoderBlock(64)\n",
    "        self.transpose = nn.ConvTranspose2d(64, 64, kernel_size=4, stride=2, padding=1) # output_padding 추가\n",
    "        \n",
    "        self.segmap = nn.Conv2d(64,n_classes, kernel_size=1)\n",
    "        self.domain_classifier = domain_classifier()\n",
    "        self.domain_linear = domain_linear()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.fconv1(x)#3->64\n",
    "        x0 = self.fbn1(x)\n",
    "        x1 = self.frelu1(x)\n",
    "        p = self.fmaxpooling(x1)#첫 conv: x0([8, 64, 109, 109]) p([8, 64, 54, 54])\n",
    "        #print(\"conv1: \",x1.shape, \"maxpooling: \",p.shape)\n",
    "        x2,p = self.conv2(p)\n",
    "        #print(\"conv2: \",x2.shape, \"maxpooling: \",p.shape)\n",
    "        x3,p = self.conv3(p)\n",
    "        #print(\"conv3: \",x3.shape, \"maxpooling: \",p.shape)\n",
    "        x4,p = self.conv4(p)\n",
    "        #print(\"conv4: \",x4.shape, \"maxpooling: \",p.shape)\n",
    "        x5,p = self.conv5(p)\n",
    "        #print(\"conv5: \",x5.shape, \"maxpooling: \",p.shape)\n",
    "        \n",
    "        xm = self.middleconv(p)#xm([8, 4096, 2, 2])\n",
    "        #print(\"xm: \",xm.shape, \"maxpooling: \",p.shape)\n",
    "        xm = self.dropout(xm)\n",
    "        \n",
    "        x = self.decoder5(xm,x5)#뉴런:2048*2->2048 1\n",
    "        x = self.decoder4(x,x4)#뉴런:1024*2->1024 \n",
    "        x = self.decoder3(x,x3) #14\n",
    "        x = self.decoder2(x,x2)#28\n",
    "        x = self.decoder1(x,x1)#55\n",
    "        x = self.transpose(x)\n",
    "        \n",
    "        #print(x.shape)\n",
    "        #x = F.interpolate(x, size=(224, 224))\n",
    "        x_c = self.segmap(x)\n",
    "        x_d = self.domain_classifier(x)\n",
    "        \n",
    "        return x_c,x_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# # 저장된 class_weights를 불러옵니다.\n",
    "# class_weights_path = 'CLASS_WEIGHTS.pkl'\n",
    "\n",
    "# with open(class_weights_path, 'rb') as file:\n",
    "#     CLASS_WEIGHTS = pickle.load(file)\n",
    "\n",
    "# print(CLASS_WEIGHTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function과 optimizer 정의\n",
    "\n",
    "class DANN_Loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DANN_Loss, self).__init__()\n",
    "\n",
    "        #self.CE = nn.CrossEntropyLoss(weight=CLASS_WEIGHTS) # 0~9 class 분류용\n",
    "        self.CE = nn.CrossEntropyLoss()\n",
    "        \n",
    "        \n",
    "    # result : DANN_CNN에서 반환된 값\n",
    "    # label : 숫자 0 ~ 9에 대한 라벨\n",
    "    # domain_num : 0(source) or 1(target)\n",
    "    def forward(self, result, label, domain_num, alpha = 1):\n",
    "        label_logits, domain_logits = result # DANN_CNN의 결과\n",
    "\n",
    "        batch_size = domain_logits.shape[0]\n",
    "\n",
    "        segment_loss = self.CE(label_logits, label) # class 분류 loss\n",
    "        \n",
    "        domain_target = torch.LongTensor([domain_num] * batch_size).to(device)\n",
    "\n",
    "        domain_loss = self.CE(domain_logits, domain_target) # domain 분류 loss\n",
    "        \n",
    "        #domain_target = torch.FloatTensor([domain_num] * batch_size).unsqueeze(1).to(device)\n",
    "        #domain_loss = self.BCE(domain_logits, domain_target) # domain 분류 loss\n",
    "        \n",
    "        loss = segment_loss + alpha * domain_loss\n",
    "        # print(\"segment_mask : \", label.shape)\n",
    "        # print(\"domain_answer : \", domain_target.shape)\n",
    "        return (loss) ,segment_loss, domain_loss\n",
    "    \n",
    "class DANN_Loss_mse(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DANN_Loss_mse, self).__init__()\n",
    "        self.CE = nn.CrossEntropyLoss()\n",
    "        self.MSE = nn.MSELoss()\n",
    "\n",
    "    def forward(self, result, label, domain_num, alpha = 1):\n",
    "        label_logits, domain_logits = result # DANN_CNN의 결과\n",
    "\n",
    "        batch_size = domain_logits.shape[0]\n",
    "\n",
    "        segment_loss = self.CE(label_logits, label) # class 분류 loss\n",
    "        \n",
    "        domain_target = torch.LongTensor([domain_num] * batch_size).unsqueeze(1).to(device).float()      \n",
    "        domain_loss = self.MSE(domain_logits, domain_target) # domain 분류 loss\n",
    "        domain_loss = torch.sqrt(domain_loss)\n",
    "        loss = segment_loss + alpha * domain_loss\n",
    "        # print(\"segment_mask : \", label.shape)\n",
    "        # print(\"domain_answer : \", domain_target.shape)\n",
    "        return loss ,segment_loss, domain_loss\n",
    "    \n",
    "\n",
    "loss_fn = DANN_Loss().to(device)\n",
    "#loss_fn = DANN_Loss_mse().to(device)\n",
    "\n",
    "#criterion =nn.CrossEntropyLoss()\n",
    "#domain_criterion = nn.BCELoss()\n",
    "#criterion = nn.CrossEntropyLoss(weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.001\n",
    "EP = 10\n",
    "BATCH_SIZE = 16\n",
    "ACCMULATION_STEP = 1 \n",
    "N_CLASSES = 13\n",
    "alpha = 0\n",
    "N_LABELS = 1\n",
    "# model 초기화\n",
    "model = Resnet34_Unet(n_classes = N_CLASSES).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "optimizer.zero_grad() \n",
    "\n",
    "source_dataset = CustomDataset(csv_file='./data/896_csv/train_source.csv', transform=transform)\n",
    "source_dataloader = DataLoader(source_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_target_dataset = CustomDataset(csv_file='./data/896_csv/val_source.csv', transform=transform)\n",
    "val_target_dataloader = DataLoader(val_target_dataset, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "miou test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparamerters:  LR = 0.001 | EP = 10, BATCH_SIZE = 16, N_CLASSES = 13, init_alpha = 0.0045, N_LABELS = 1\n",
      "0.0045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1: 100%|██████████| 138/138 [01:42<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label_0: IoU Scores Train\n",
      "Class00: 0.9584 Class01: 0.8350 Class02: 0.3525 Class03: 0.6855 Class04: 0.4949 Class05: 0.1098 Class06: 0.2195 \n",
      "Class07: 0.3382 Class08: 0.6858 Class09: 0.9084 Class10: 0.0064 Class11: 0.0047 Class12: 0.7551 \n",
      "Train seg Loss: 0.376272927807725 Train dom Loss: 1.1558621249337127\n",
      "Train Loss: 0.38147430661795795\n",
      "Train mIoU: 0.48878312830927556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:10<00:00,  2.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--IoU Scores Fish val--\n",
      "Class0: 0.9518 Class1: 0.7113 Class2: 0.0898 Class3: 0.3500 Class4: 0.1160 Class5: 0.0024 Class6: 0.0213 \n",
      "Class7: 0.1086 Class8: 0.4958 Class9: 0.5338 Class10: 0.0006 Class11: 0.0000 Class12: 0.3929 \n",
      "Epoch1\n",
      "Valid_Seg Loss: 1.071051416794459 Valid_dom Loss: 1.0630225578943888\n",
      "Valid Loss: 1.0758350133895873\n",
      "Valid mIoU: 0.2903328998613308\n",
      "___________________________________________________________________________________________\n",
      "\n",
      "0.0045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 2: 100%|██████████| 138/138 [01:43<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label_0: IoU Scores Train\n",
      "Class00: 0.9587 Class01: 0.8355 Class02: 0.3766 Class03: 0.6853 Class04: 0.4895 Class05: 0.1346 Class06: 0.2215 \n",
      "Class07: 0.3486 Class08: 0.6879 Class09: 0.9079 Class10: 0.0055 Class11: 0.0077 Class12: 0.7561 \n",
      "Train seg Loss: 0.37349283403676486 Train dom Loss: 1.1322558677714805\n",
      "Train Loss: 0.3785879866800446\n",
      "Train mIoU: 0.4934995091760115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:11<00:00,  2.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--IoU Scores Fish val--\n",
      "Class0: 0.9559 Class1: 0.7713 Class2: 0.1271 Class3: 0.5572 Class4: 0.2016 Class5: 0.0019 Class6: 0.0237 \n",
      "Class7: 0.1334 Class8: 0.6143 Class9: 0.9183 Class10: 0.0005 Class11: 0.0000 Class12: 0.5614 \n",
      "Epoch2\n",
      "Valid_Seg Loss: 0.4824731568495432 Valid_dom Loss: 1.1767364144325256\n",
      "Valid Loss: 0.48776846726735434\n",
      "Valid mIoU: 0.37436277791069855\n",
      "___________________________________________________________________________________________\n",
      "\n",
      "0.0045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 3: 100%|██████████| 138/138 [01:44<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label_0: IoU Scores Train\n",
      "Class00: 0.9593 Class01: 0.8387 Class02: 0.3872 Class03: 0.7037 Class04: 0.4939 Class05: 0.1519 Class06: 0.2401 \n",
      "Class07: 0.3622 Class08: 0.7033 Class09: 0.9155 Class10: 0.0100 Class11: 0.0245 Class12: 0.7758 \n",
      "Train seg Loss: 0.35472060891165247 Train dom Loss: 1.1247963844865994\n",
      "Train Loss: 0.3597821921542071\n",
      "Train mIoU: 0.5050839289710168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:10<00:00,  2.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--IoU Scores Fish val--\n",
      "Class0: 0.9574 Class1: 0.7566 Class2: 0.1492 Class3: 0.5407 Class4: 0.1558 Class5: 0.0094 Class6: 0.0423 \n",
      "Class7: 0.1344 Class8: 0.6083 Class9: 0.9154 Class10: 0.0017 Class11: 0.0000 Class12: 0.5278 \n",
      "Epoch3\n",
      "Valid_Seg Loss: 0.5162389536698659 Valid_dom Loss: 1.0727981448173523\n",
      "Valid Loss: 0.5210665434598922\n",
      "Valid mIoU: 0.36913969456202417\n",
      "___________________________________________________________________________________________\n",
      "\n",
      "0.0045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 4: 100%|██████████| 138/138 [01:44<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label_0: IoU Scores Train\n",
      "Class00: 0.9599 Class01: 0.8443 Class02: 0.4004 Class03: 0.7077 Class04: 0.5031 Class05: 0.1658 Class06: 0.2438 \n",
      "Class07: 0.3604 Class08: 0.6981 Class09: 0.9145 Class10: 0.0180 Class11: 0.0416 Class12: 0.7739 \n",
      "Train seg Loss: 0.3513351553592129 Train dom Loss: 1.1501471685326619\n",
      "Train Loss: 0.3565108171407727\n",
      "Train mIoU: 0.5101160059135834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:11<00:00,  2.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--IoU Scores Fish val--\n",
      "Class0: 0.9521 Class1: 0.7631 Class2: 0.1188 Class3: 0.5335 Class4: 0.1247 Class5: 0.0056 Class6: 0.0409 \n",
      "Class7: 0.1451 Class8: 0.6136 Class9: 0.9003 Class10: 0.0000 Class11: 0.0000 Class12: 0.4591 \n",
      "Epoch4\n",
      "Valid_Seg Loss: 0.53533127506574 Valid_dom Loss: 1.1514951229095458\n",
      "Valid Loss: 0.5405130038658778\n",
      "Valid mIoU: 0.35821600229730627\n",
      "___________________________________________________________________________________________\n",
      "\n",
      "0.0045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 5: 100%|██████████| 138/138 [01:45<00:00,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label_0: IoU Scores Train\n",
      "Class00: 0.9596 Class01: 0.8440 Class02: 0.3899 Class03: 0.7050 Class04: 0.5138 Class05: 0.1836 Class06: 0.2438 \n",
      "Class07: 0.3762 Class08: 0.7042 Class09: 0.9121 Class10: 0.0126 Class11: 0.0563 Class12: 0.7759 \n",
      "Train seg Loss: 0.3532544661691223 Train dom Loss: 1.1332984445751577\n",
      "Train Loss: 0.35835430889889813\n",
      "Train mIoU: 0.5136186312344696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:10<00:00,  2.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--IoU Scores Fish val--\n",
      "Class0: 0.9570 Class1: 0.7579 Class2: 0.1542 Class3: 0.5211 Class4: 0.1442 Class5: 0.0015 Class6: 0.0435 \n",
      "Class7: 0.1383 Class8: 0.5804 Class9: 0.8964 Class10: 0.0010 Class11: 0.0045 Class12: 0.5298 \n",
      "Epoch5\n",
      "Valid_Seg Loss: 0.5748144298791885 Valid_dom Loss: 1.126249090830485\n",
      "Valid Loss: 0.5798825492461522\n",
      "Valid mIoU: 0.3638201898059054\n",
      "___________________________________________________________________________________________\n",
      "\n",
      "0.0045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 6: 100%|██████████| 138/138 [01:46<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label_0: IoU Scores Train\n",
      "Class00: 0.9594 Class01: 0.8380 Class02: 0.3724 Class03: 0.6896 Class04: 0.5108 Class05: 0.1741 Class06: 0.2358 \n",
      "Class07: 0.3593 Class08: 0.6876 Class09: 0.9109 Class10: 0.0173 Class11: 0.0464 Class12: 0.7523 \n",
      "Train seg Loss: 0.3722201654876488 Train dom Loss: 1.156964016997296\n",
      "Train Loss: 0.377426502281341\n",
      "Train mIoU: 0.5041479456789261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:11<00:00,  2.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--IoU Scores Fish val--\n",
      "Class0: 0.9559 Class1: 0.7727 Class2: 0.1212 Class3: 0.5441 Class4: 0.1675 Class5: 0.0067 Class6: 0.0409 \n",
      "Class7: 0.1732 Class8: 0.5855 Class9: 0.9158 Class10: 0.0010 Class11: 0.0000 Class12: 0.5427 \n",
      "Epoch6\n",
      "Valid_Seg Loss: 0.5011503001054128 Valid_dom Loss: 1.131394080320994\n",
      "Valid Loss: 0.5062415689229965\n",
      "Valid mIoU: 0.3713099358926856\n",
      "___________________________________________________________________________________________\n",
      "\n",
      "0.0045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 7: 100%|██████████| 138/138 [01:47<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label_0: IoU Scores Train\n",
      "Class00: 0.9601 Class01: 0.8453 Class02: 0.4013 Class03: 0.7094 Class04: 0.5128 Class05: 0.2088 Class06: 0.2488 \n",
      "Class07: 0.3714 Class08: 0.7074 Class09: 0.9136 Class10: 0.0251 Class11: 0.0530 Class12: 0.7702 \n",
      "Train seg Loss: 0.3486455834430197 Train dom Loss: 1.1524149516354436\n",
      "Train Loss: 0.35383145187212073\n",
      "Train mIoU: 0.5174769135029149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:11<00:00,  2.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--IoU Scores Fish val--\n",
      "Class0: 0.9456 Class1: 0.7683 Class2: 0.1731 Class3: 0.5496 Class4: 0.1048 Class5: 0.0115 Class6: 0.0580 \n",
      "Class7: 0.1626 Class8: 0.5516 Class9: 0.9112 Class10: 0.0013 Class11: 0.0000 Class12: 0.5356 \n",
      "Epoch7\n",
      "Valid_Seg Loss: 0.5385770410299301 Valid_dom Loss: 1.17492915391922\n",
      "Valid Loss: 0.5438642183939616\n",
      "Valid mIoU: 0.3671625390797223\n",
      "___________________________________________________________________________________________\n",
      "\n",
      "0.0045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 8: 100%|██████████| 138/138 [01:47<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label_0: IoU Scores Train\n",
      "Class00: 0.9610 Class01: 0.8479 Class02: 0.4181 Class03: 0.7186 Class04: 0.5166 Class05: 0.2187 Class06: 0.2653 \n",
      "Class07: 0.3859 Class08: 0.7135 Class09: 0.9182 Class10: 0.0367 Class11: 0.0783 Class12: 0.7801 \n",
      "Train seg Loss: 0.33713440013968426 Train dom Loss: 1.1639140351958897\n",
      "Train Loss: 0.3423720130669898\n",
      "Train mIoU: 0.5276153760350876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:12<00:00,  2.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--IoU Scores Fish val--\n",
      "Class0: 0.9469 Class1: 0.7820 Class2: 0.1497 Class3: 0.5699 Class4: 0.1782 Class5: 0.0078 Class6: 0.0411 \n",
      "Class7: 0.1593 Class8: 0.6183 Class9: 0.9217 Class10: 0.0008 Class11: 0.0000 Class12: 0.5305 \n",
      "Epoch8\n",
      "Valid_Seg Loss: 0.4995466579993566 Valid_dom Loss: 1.1445781429608664\n",
      "Valid Loss: 0.5046972572803498\n",
      "Valid mIoU: 0.3773891080903128\n",
      "___________________________________________________________________________________________\n",
      "\n",
      "0.0045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 9: 100%|██████████| 138/138 [01:49<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label_0: IoU Scores Train\n",
      "Class00: 0.9613 Class01: 0.8535 Class02: 0.4292 Class03: 0.7251 Class04: 0.5281 Class05: 0.2355 Class06: 0.2742 \n",
      "Class07: 0.3995 Class08: 0.7224 Class09: 0.9182 Class10: 0.0342 Class11: 0.0892 Class12: 0.7859 \n",
      "Train seg Loss: 0.3279056063164835 Train dom Loss: 1.143286783626114\n",
      "Train Loss: 0.3330503965633503\n",
      "Train mIoU: 0.5350932384597422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:11<00:00,  2.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--IoU Scores Fish val--\n",
      "Class0: 0.9468 Class1: 0.7744 Class2: 0.1849 Class3: 0.5560 Class4: 0.2211 Class5: 0.0107 Class6: 0.0395 \n",
      "Class7: 0.1348 Class8: 0.5817 Class9: 0.9178 Class10: 0.0023 Class11: 0.0000 Class12: 0.5638 \n",
      "Epoch9\n",
      "Valid_Seg Loss: 0.4978291044632594 Valid_dom Loss: 1.1716388583183288\n",
      "Valid Loss: 0.5031014750401179\n",
      "Valid mIoU: 0.3795224284572861\n",
      "___________________________________________________________________________________________\n",
      "\n",
      "0.0045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 10: 100%|██████████| 138/138 [01:47<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label_0: IoU Scores Train\n",
      "Class00: 0.9610 Class01: 0.8545 Class02: 0.4346 Class03: 0.7308 Class04: 0.5467 Class05: 0.2567 Class06: 0.2753 \n",
      "Class07: 0.4076 Class08: 0.7251 Class09: 0.9217 Class10: 0.0460 Class11: 0.0927 Class12: 0.7870 \n",
      "Train seg Loss: 0.3225995279524637 Train dom Loss: 1.149828989436661\n",
      "Train Loss: 0.3277737587906312\n",
      "Train mIoU: 0.5415165944185591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:11<00:00,  2.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--IoU Scores Fish val--\n",
      "Class0: 0.9524 Class1: 0.7695 Class2: 0.1599 Class3: 0.5564 Class4: 0.1991 Class5: 0.0037 Class6: 0.0404 \n",
      "Class7: 0.1435 Class8: 0.6364 Class9: 0.9144 Class10: 0.0003 Class11: 0.0000 Class12: 0.5395 \n",
      "Epoch10\n",
      "Valid_Seg Loss: 0.4987408479054769 Valid_dom Loss: 1.1529700040817261\n",
      "Valid Loss: 0.5039292126893997\n",
      "Valid mIoU: 0.3781112633231887\n",
      "___________________________________________________________________________________________\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "EP = 0.0001\n",
    "print(\"Hyperparamerters: \",f\"LR = {LR} | EP = {EP}, BATCH_SIZE = {BATCH_SIZE}, N_CLASSES = {N_CLASSES}, init_alpha = {alpha}, N_LABELS = {N_LABELS}\")\n",
    "\n",
    "for epoch in range(EP):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    seg_losses = 0\n",
    "    domain_losses = 0\n",
    "    alpha = 0.0045\n",
    "    train_class_ious = [[]]\n",
    "    \n",
    "    # label_dfs = {label: None for label in range(N_LABELS)}\n",
    "    print(alpha)\n",
    "    for source_image, source_mask in tqdm(source_dataloader,desc=f\"Epoch: {epoch+1}\"):\n",
    "        for label in range(N_LABELS):\n",
    "            #source_image, source_mask = apply_fisheye_distortion(source_images, source_masks, label)\n",
    "            source_image = source_image.float().to(device)\n",
    "            source_mask = source_mask.long().to(device)\n",
    "            source_outputs = model(source_image)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            target_loss, seg_loss, domain_loss = loss_fn(source_outputs, source_mask, label, alpha = alpha)\n",
    "            epoch_loss += target_loss.item()\n",
    "            seg_losses +=  seg_loss.item()\n",
    "            domain_losses += domain_loss.item()\n",
    "            seg_loss.backward()\n",
    "            optimizer.step()\n",
    "            # miou측정\n",
    "            source_outputs = model(source_image)\n",
    "            source_outputs = torch.softmax(source_outputs[0], dim=1).cpu()\n",
    "            source_outputs = torch.argmax(source_outputs, dim=1).numpy()\n",
    "            \n",
    "            for class_id in range(N_CLASSES):\n",
    "                iou = calculate_iou_per_class(np.array(source_mask.cpu()), np.array(source_outputs), class_id)\n",
    "                train_class_ious[label].append(iou)\n",
    "            \n",
    "    for i in range(N_LABELS):\n",
    "        buff = np.array(train_class_ious[i]).reshape(-1,N_CLASSES)\n",
    "        buff = np.mean(buff, axis=0)\n",
    "        print(f\"\\nLabel_{i}: IoU Scores Train\")\n",
    "        for class_id, iou in enumerate(buff):\n",
    "            print(f'Class{class_id:02d}: {iou:.4f}', end=\" \")\n",
    "            if (class_id+1) % 7 == 0:\n",
    "                print()   \n",
    "    print()    \n",
    "    print(f\"Train seg Loss: {(seg_losses/(N_LABELS*len(source_dataloader)))}\",f\"Train dom Loss: {(domain_losses/(N_LABELS*len(source_dataloader)))}\")\n",
    "    print(f\"Train Loss: {(epoch_loss/(N_LABELS*len(source_dataloader)))}\")\n",
    "    print(f\"Train mIoU: {np.mean(train_class_ious)}\" )\n",
    "    ################################################################\n",
    "    # 클래스별 IoU를 누적할 리스트 초기화\n",
    "    val_class_ious = []\n",
    "    fish_val_class_ious = []\n",
    "    val_epoch_loss = 0\n",
    "    val_seg_loss = 0\n",
    "    val_domain_loss = 0\n",
    "    # valid\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for target_images, target_masks in tqdm(val_target_dataloader):\n",
    "            label = 0\n",
    "            #target_images, target_masks = apply_fisheye_distortion(target_images, target_masks, label)\n",
    "            target_images = target_images.float().to(device)\n",
    "            target_masks = target_masks.long().to(device)\n",
    "\n",
    "            target_outputs = model(target_images)\n",
    "\n",
    "            target_loss, target_seg_loss, target_domain_loss = loss_fn(target_outputs, target_masks, label, alpha = alpha)\n",
    "\n",
    "            val_seg_loss +=  target_seg_loss.item()\n",
    "            val_domain_loss += target_domain_loss.item()\n",
    "            \n",
    "            loss = target_loss\n",
    "\n",
    "            val_epoch_loss += loss.item()\n",
    "\n",
    "            # train 클래스별 IoU 계산\n",
    "            target_outputs = torch.softmax(target_outputs[0], dim=1).cpu()\n",
    "            target_outputs = torch.argmax(target_outputs, dim=1).numpy()\n",
    "\n",
    "            for class_id in range(N_CLASSES):\n",
    "                iou = calculate_iou_per_class(np.array(target_masks.cpu()), np.array(target_outputs), class_id)\n",
    "                fish_val_class_ious.append(iou)\n",
    "\n",
    "    fish_val_class_ious = np.array(fish_val_class_ious).reshape(-1,N_CLASSES)\n",
    "    fish_val_class_ious = np.mean(fish_val_class_ious, axis=0)\n",
    "    print()\n",
    "    print(\"--IoU Scores Fish val--\")\n",
    "    for class_id, iou in enumerate(fish_val_class_ious):\n",
    "        print(f'Class{class_id}: {iou:.4f}', end=\" \")\n",
    "        if (class_id+1) % 7 == 0:\n",
    "            print()\n",
    "\n",
    "    # mIoU 계산\n",
    "    fish_val_mIoU = np.mean(fish_val_class_ious)\n",
    "\n",
    "    # 에폭마다 결과 출력 \n",
    "    print(f\"\\nEpoch{epoch+1}\")\n",
    "    print(f\"Valid_Seg Loss: {(val_seg_loss/len(val_target_dataloader))}\",f\"Valid_dom Loss: {(val_domain_loss/len(val_target_dataloader))}\")\n",
    "    print(f\"Valid Loss: {(val_epoch_loss/len(val_target_dataloader))}\")\n",
    "    print(f\"Valid mIoU: {fish_val_mIoU}\" )\n",
    "    print(\"___________________________________________________________________________________________\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './data/Res34Unet_SEG_11-09.pth')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "byungwan_resn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
